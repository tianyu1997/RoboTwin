# RL Training Configuration for F1-VLA
# This config is shared across all three training phases

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  # Console log level for main process: DEBUG, INFO, WARNING, ERROR, CRITICAL
  # Non-main processes (other GPUs in DDP) always use WARNING to reduce noise
  console_level: "INFO"
  # File log level - set to WARNING to reduce file size
  file_level: "CRITICAL"
  # Enable/disable file logging
  enable_file_logging: false

# =============================================================================
# Model Configuration
# =============================================================================
model:
  config_file: /mnt/data2/ty/F1-VLA/f1_vla/config/debug_test.yaml
  
  # LoRA configuration
  lora:
    r: 8
    lora_alpha: 32
    # Only target gemma_expert and gemma_wm_expert, excluding paligemma (vision encoder + base LLM)
    target_modules: '.*(gemma_expert|gemma_wm_expert).*(q_proj|v_proj|k_proj|o_proj|gate_proj|up_proj|down_proj)$'
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"
    # Modules to save (train fully) alongside LoRA adapters
    modules_to_save: ["memory_gru", "memory_to_gemma", "memory_to_gemma_wm", "memory_gate_act", "memory_gate_wm"]

# =============================================================================
# Environment Configuration
# =============================================================================
environment:
  # Embodiment - single arm
  embodiment: ["franka-panda"]
  # Single arm mode: only use left arm, right arm is disabled
  single_arm: true
  
  scene_reset_interval: 1
  randomize_robot_init: true
  
  # Motion planner settings (for RL training, disable to save time and GPU memory)
  # need_planner: false = skip CuRobo motion planner initialization (saves ~10GB VRAM)
  # need_topp: false = skip MplibPlanner initialization
  # These are only needed for trajectory planning, not for delta action control
  need_planner: false
  need_topp: false
  
  # Task settings
  task_name: "random_exploration"
  control_mode: "delta_qpos"
  delta_qpos_scale: 0.05
  render_mode: "rasterize"  # "rt" or "rasterize"
  num_objects: 0
  
  # Camera configuration - collect both cameras
  camera:
    head_camera_type: "D435"
    wrist_camera_type: "D435"
    collect_head_camera: true
    collect_wrist_camera: true
  
  # Domain randomization
  domain_randomization:
    random_appearance: false
    random_background: false
    random_light: false
    cluttered_table: true # clear means background only, table is always cluttered with objects
  
  # Data type
  data_type:
    collect_rgb: true
    collect_depth: false
    collect_qpos: true
    collect_endpose: true

# =============================================================================
# Training Configuration (shared across phases)
# =============================================================================
training:
  # Dimensions
  action_dim: 32
  state_dim: 32
  n_pred_img_steps: 1
  
  # Training parameters - adjusted for better convergence
  learning_rate: 1.0e-5  # Reduced for stability
  weight_decay: 1.0e-5
  gradient_accumulation_steps: 2
  max_grad_norm: 0.5  # Reduced from 1.0 for tighter clipping
  
  # Loss Schedule (Warmup within episode)
  loss_warmup_steps: 10
  loss_warmup_start_weight: 0.1
  
  # Scheduler
  scheduler:
    type: "cosine"
    T_max: 10000
    eta_min: 1.0e-6
  
  num_epochs: 100

  # Episode settings
  num_episodes: 2000
  steps_per_episode: 50
  batch_size: 1
  # Mini-batch size for PPO updates
  mini_batch_size: 2
  
  # Logging & checkpointing
  log_every: 1
  save_every: 10
  video_save_every: 5
  
  # BPTT (Backpropagation Through Time) sequence length for memory training
  # Longer sequences = better memory learning, but higher memory cost and gradient issues 4-8 for stability, up to 16 if memory is sufficient
  bptt_length: 7
  memory_backprop: true
  sequential_training: true
  
  # Action bounds
  action_scale: 0.3  # Scale factor for action bounds (0.01-1.0)
  action_bounds:
    low: -1.0
    high: 1.0
 
# =============================================================================
# Phase 1: Teacher Training Configuration
# =============================================================================
teacher:
  output_dir: "./outputs/teacher_rl"
  
  # Mixed Dataset Configuration (Optional)
  # If defined, 'datasets' overrides the command line --data_dir argument
  datasets:
    - path: "./rl/data/clean"
      weight: 1.0
    - path: "./rl/data/complex_texture"
      weight: 0.5
  
  # Teacher uses random exploration
  exploration_type: "random"
  
  # Train world model only
  train_world_model: true
  train_action_head: false
  freeze_vision_encoder: true

# =============================================================================
# Phase 2: Student/Explorer Training Configuration
# =============================================================================
student:
  output_dir: "./outputs/student_rl"
  
  # Student uses learned policy for exploration
  exploration_type: "policy"
  
  # Freeze world model, train action head
  train_world_model: false
  train_action_head: true
  freeze_vision_encoder: true
  
  # Reward weights - adjusted based on training observations
  rewards:
    memory_divergence_weight: 1.0
    wm_uncertainty_weight: 1.0
    task_success_weight: 1.0
  
  # PPO parameters - adjusted for stability and exploration
  ppo:
    clip_epsilon: 0.2  # Standard PPO value
    entropy_coef: 0.01  # Reduced from 0.05 for stability
    value_loss_coef: 0.5
    gamma: 0.99
    gae_lambda: 0.95
    # Additional stability controls
    normalize_advantages: true
    clip_value_loss: true
    # Max gradient norm for PPO optimizer
    max_grad_norm: 1.0

# =============================================================================
# Phase 3: Adversarial Training Configuration
# =============================================================================
adversarial:
  output_dir: "./outputs/adversarial_rl"
  total_iterations: 100000
  
  # Adversarial training parameters
  wm_updates_per_iter: 5
  explorer_updates_per_iter: 1
  warmup_steps: 1000
  buffer_size: 100000
  
  # Adversarial weights
  adversarial_weight: 0.5
  entropy_weight: 0.01
  
  # Separate learning rates
  wm_lr: 1.0e-4
  explorer_lr: 3.0e-4
  
  # # Explorer network
  # explorer:
  #   hidden_dim: 256
  #   image_encoder_dim: 512

# =============================================================================
# Device Configuration
# =============================================================================
device: "cuda"
debug: false
