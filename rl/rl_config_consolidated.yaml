# RL Training Configuration for F1-VLA
# This config is shared across all three training phases

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  # Console log level for main process: DEBUG, INFO, WARNING, ERROR, CRITICAL
  # Non-main processes (other GPUs in DDP) always use WARNING to reduce noise
  console_level: "INFO"
  # File log level - set to WARNING to reduce file size
  file_level: "CRITICAL"
  # Enable/disable file logging
  enable_file_logging: false

# =============================================================================
# Model Configuration
# =============================================================================
model:
  # If config_file is null, we look for 'f1_vla' section in this file
  config_file: null
  
  # LoRA configuration
  lora:
    r: 8
    lora_alpha: 32
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

# =============================================================================
# Environment Configuration
# =============================================================================
environment:
  # Embodiment - single arm
  embodiment: ["franka-panda"]
  
  # Single arm mode: only use left arm, right arm is disabled
  single_arm: true
  
  # Scene reset interval: only regenerate scene every N episodes
  # This significantly speeds up training by reusing the same scene
  # Set to 1 to regenerate scene every episode (slow)
  scene_reset_interval: 50
  randomize_robot_init: false
  
  # Motion planner settings (for RL training, disable to save time and GPU memory)
  # need_planner: false = skip CuRobo motion planner initialization (saves ~10GB VRAM)
  # need_topp: false = skip MplibPlanner initialization
  # These are only needed for trajectory planning, not for delta action control
  need_planner: false
  need_topp: false
  
  # Task settings
  task_name: "random_exploration"
  control_mode: "delta_qpos"
  delta_qpos_scale: 0.05
  render_mode: "rasterize"  # "rt" or "rasterize"
  num_objects: 0
  
  # Camera configuration - collect both cameras
  camera:
    head_camera_type: "D435"
    wrist_camera_type: "D435"
    collect_head_camera: true
    collect_wrist_camera: true
  
  # Domain randomization
  domain_randomization:
    random_appearance: false
    random_background: true
    random_light: true
    cluttered_table: true
  
  # Data type
  data_type:
    collect_rgb: true
    collect_depth: false
    collect_qpos: true
    collect_endpose: true

# =============================================================================
# Training Configuration (shared across phases)
# =============================================================================
training:
  # Dimensions
  action_dim: 32
  state_dim: 32
  
  # World model configuration (read from debug_test.yaml)
  # history_length will be auto-calculated as: n_obs_img_steps + (n_obs_img_steps - 1) * obs_img_stride
  n_pred_img_steps: 1
  
  # Training parameters - adjusted for better convergence
  learning_rate: 1.0e-5  # Reduced for stability
  weight_decay: 1.0e-5
  gradient_accumulation_steps: 2
  max_grad_norm: 0.5  # Reduced from 1.0 for tighter clipping
  
  # Scheduler
  scheduler:
    type: "cosine"
    T_max: 10000
    eta_min: 1.0e-6
  
  # Episode settings
  num_episodes: 10000
  steps_per_episode: 50
  batch_size: 1
  # Mini-batch size for PPO updates
  mini_batch_size: 16
  
  # Logging & checkpointing
  log_every: 1
  save_every: 200
  
  # Memory/Sequential processing
  # F1-VLA has memory_rnn that needs sequential state propagation
  # Set to false when memory_enabled=false in f1_config.json
  sequential_training: true
  # BPTT (Backpropagation Through Time) sequence length for memory training
  # Longer sequences = better memory learning, but higher memory cost and gradient issues
  # Recommended: 4-8 for stability, up to 16 if memory is sufficient
  bptt_length: 4
  # Enable gradient flow through memory states (requires sequential_training=true)
  # When true, memory GRU can learn better encodings through backprop
  # When false, memory state is detached each step (faster but memory doesn't improve)
  # Note: Gradients only flow WITHIN each sequence (truncated BPTT), not across sequences
  memory_backprop: true
  
  # Action bounds
  action_scale: 0.5  # Scale factor for action bounds (0.01-1.0)
  action_bounds:
    low: -1.0
    high: 1.0
  # How often to save videos (1 = every episode)
  video_save_every: 20

# =============================================================================
# Phase 1: Teacher Training Configuration
# =============================================================================
teacher:
  output_dir: "./outputs/teacher_rl"
  
  # Teacher uses random exploration
  exploration_type: "random"
  
  # Train world model only
  train_world_model: true
  train_action_head: false
  freeze_vision_encoder: true

# =============================================================================
# Phase 2: Student/Explorer Training Configuration
# =============================================================================
student:
  output_dir: "./outputs/student_rl"
  
  # Student uses learned policy for exploration
  exploration_type: "policy"
  
  # Freeze world model, train action head
  train_world_model: false
  train_action_head: true
  freeze_vision_encoder: true
  
  # Reward weights - adjusted based on training observations
  rewards:
    # Memory divergence: NEW DESIGN - directly reward closeness to teacher
    # Normalized divergence typically ranges from 0.1 to 2.0
    # Higher weight to encourage strong memory alignment
    memory_divergence_weight: 1.0
    # World model uncertainty: encourage exploration that surprises WM
    # Typical range: 3.0 to 8.0
    wm_uncertainty_weight: 1.0
    # Task success reward weight
    task_success_weight: 1.0
  
  # PPO parameters - adjusted for stability and exploration
  ppo:
    clip_epsilon: 0.2  # Standard PPO value
    entropy_coef: 0.01  # Reduced from 0.05 for stability
    value_loss_coef: 0.5
    gamma: 0.99
    gae_lambda: 0.95
    # Additional stability controls
    normalize_advantages: true
    clip_value_loss: true
    # Max gradient norm for PPO optimizer
    max_grad_norm: 1.0

# =============================================================================
# Phase 3: Adversarial Training Configuration
# =============================================================================
adversarial:
  output_dir: "./outputs/adversarial_rl"
  total_iterations: 100000
  
  # Adversarial training parameters
  wm_updates_per_iter: 5
  explorer_updates_per_iter: 1
  warmup_steps: 1000
  buffer_size: 100000
  
  # Adversarial weights
  adversarial_weight: 0.5
  entropy_weight: 0.01
  
  # Separate learning rates
  wm_lr: 1.0e-4
  explorer_lr: 3.0e-4
  
  # # Explorer network
  # explorer:
  #   hidden_dim: 256
  #   image_encoder_dim: 512

# =============================================================================
# Device Configuration
# =============================================================================
device: "cuda"
debug: false

# =============================================================================
# F1-VLA Configuration (Consolidated)
# =============================================================================
f1_vla:
  dataset:
    train_dir:
      # ========== libero ==========
      libero_spatial_no_noops:
        path: /mnt/data2/ty/F1-VLA/libero_spatial_no_noops_1.0.0_lerobot
        camera_keys: 
          - observation.images.image
          - observation.images.wrist_image
        state_keys: 
          - observation.state
        action_keys:
          - action
        predict_img_keys:
          - observation.images.image
        type: libero
        weight: 5
        n_obs_img_steps: 4
        n_pred_img_steps: 1
        obs_img_stride: 1
        num_indices: 20
        norm_method: mean_std

    image_size:
      height: 224
      width: 224
    world_model_suffix: history
    transforms:
      consistent_random_crop_stages:
        - stage1_pretrain_wm
        - stage2_pretrain_vla
        - stage3_finetune_vla

  exp:
    stage: stage3_finetune_vla
    load_ckpt: /mnt/data2/ty/F1-VLA/wm

    training_args:
      output_dir: outputs/baseline
      run_name: baseline
      do_train: True
      do_eval: False
      num_train_epochs: 5
      num_eval_episodes: 2000
      per_device_train_batch_size: 5
      per_device_eval_batch_size: 1

      learning_rate: !!float 5e-5
      weight_decay: !!float 1e-10
      adam_beta1: 0.9
      adam_beta2: 0.95
      adam_epsilon: !!float 1e-8
      lr_scheduler_type: cosine
      gradient_accumulation_steps: 1

      max_steps: 52_000
      warmup_steps: 2000
      log_level: info
      log_level_replica: error
      seed: 42
      bf16: True
      logging_strategy: steps
      logging_steps: 200
      save_strategy: steps
      save_steps: 5_000
      eval_strategy: "no"
      eval_steps: 500
      eval_accumulation_steps: 1
      save_safetensors: True
      include_for_metrics: 
        - loss

      save_total_limit: 800
      overwrite_output_dir: True
      dataloader_num_workers: 12
      dataloader_pin_memory: False
      dataloader_persistent_workers: True
      dataloader_prefetch_factor: 4
      remove_unused_columns: False
      resume_from_checkpoint: null

      label_names: 
        - action
      metric_for_best_model: "eval_loss"
      greater_is_better: True

      report_to: 
        - tensorboard

      torch_compile: True

      gen_out_loss_ratio: 0.1
      freeze_vision_encoder: True
      freeze_gen_expert: False
      train_act_expert_only: False
      train_gen_expert_only: True
      train_state_proj: True

      image_transforms_enabled: True
      image_transforms_type:
        - brightness
        - contrast
        - saturation
        - random_crop
        - random_rotation
      image_transforms_max_num_transforms: 3
      image_transforms_random_order: True

      und_expert_lr: !!float 5e-5
      act_expert_lr: !!float 5e-5
      vision_encoder_lr: !!float 2.5e-5
      gen_expert_lr: !!float 5e-5

      persistent_memory: True
      persistent_memory_cache_size: 10000
      episode_sampler_enabled: True
      episode_sharding_strategy: contiguous
      episode_sampler_shuffle: False
      episode_sampler_debug: True
      episode_sampler_verify_disjoint: True
      episode_sampler_log_batches: True
      episode_sampler_log_batches_limit: 1

    max_eval_samples: 5000

  policy:
    model_name: f1
    path: /mnt/data2/ty/F1-VLA/pi0
    # ckpt_path is kept for backward compatibility but will be overridden by f1_config if present
    ckpt_path: /mnt/data2/ty/F1-VLA/f1_vla/config/f1_config.json
    language_tokenizer_path: /mnt/data2/ty/F1-VLA/paligemma-3b-pt-224
    mwm_pretrained_path: /mnt/data2/ty/F1-VLA/wm
    chunk_size: 4

    use_world_model: True
    pn: "1_2_3_4_5_6_8_10_13_16"
    vae_ckpt: /mnt/data2/ty/F1_VLA/var/var_d16.pth

    temporal_conv_kernel_size: 4
    temporal_conv_stride: 4
    vocab_size: 4096
    vae_dim: 32
    num_resolutions: 16

    resize_imgs_with_padding: (224, 224)
    attention_implementation: eager
    
    # Embedded F1 Config (formerly f1_config.json)
    f1_config:
      architectures:
        - F1ForConditionalGeneration
      auto_map:
        AutoConfig: configuration_f1.F1Config
        AutoModel: modeling_f1.F1ForConditionalGeneration
      und_expert_config:
        transformers_version: 4.48.1
        _vocab_size: 257152
        bos_token_id: 2
        eos_token_id: 1
        hidden_size: 2048
        image_token_index: 257152
        model_type: paligemma
        pad_token_id: 0
        projection_dim: 2048
        text_config:
          hidden_activation: gelu_pytorch_tanh
          hidden_size: 2048
          intermediate_size: 16384
          model_type: gemma
          num_attention_heads: 8
          num_hidden_layers: 18
          num_image_tokens: 256
          num_key_value_heads: 1
          torch_dtype: float32
          vocab_size: 257152
        vision_config:
          hidden_size: 1152
          intermediate_size: 4304
          model_type: siglip_vision_model
          num_attention_heads: 16
          num_hidden_layers: 27
          num_image_tokens: 256
          patch_size: 14
          projection_dim: 2048
          projector_hidden_act: gelu_fast
          torch_dtype: float32
          vision_use_head: false
      gen_expert_config:
        attention_bias: false
        attention_dropout: 0.0
        bos_token_id: 2
        eos_token_id: 1
        head_dim: 256
        hidden_act: gelu_pytorch_tanh
        hidden_activation: gelu_pytorch_tanh
        hidden_size: 1024
        initializer_range: 0.02
        intermediate_size: 4096
        max_position_embeddings: 8192
        model_type: gemma
        num_attention_heads: 8
        num_hidden_layers: 18
        num_key_value_heads: 1
        pad_token_id: 0
        rms_norm_eps: 1.0e-06
        rope_theta: 10000.0
        torch_dtype: float32
        transformers_version: 4.48.1
        use_cache: true
        vocab_size: 257152
        pn: "1_2_3_4_5_6_8_10_13_16"
        temporal_conv_kernel_size: 4
        temporal_conv_stride: 4
        num_resolutions: 4
        vae:
          vocab_size: 4096
          z_channels: 32
          ch: 160
          test_mode: true
          share_quant_resi: 4
          vae_ckpt: ""
      act_expert_config:
        attention_bias: false
        attention_dropout: 0.0
        bos_token_id: 2
        eos_token_id: 1
        head_dim: 256
        hidden_act: gelu_pytorch_tanh
        hidden_activation: gelu_pytorch_tanh
        hidden_size: 1024
        initializer_range: 0.02
        intermediate_size: 4096
        max_position_embeddings: 8192
        model_type: gemma
        num_attention_heads: 8
        num_hidden_layers: 18
        num_key_value_heads: 1
        pad_token_id: 0
        rms_norm_eps: 1.0e-06
        rope_theta: 10000.0
        torch_dtype: float32
        transformers_version: 4.48.1
        use_cache: true
        vocab_size: 257152
      use_world_model: true
      chunk_size: 50
      max_action_dim: 32
      max_state_dim: 32
      tokenizer_max_length: 48
      use_cache: true
      attention_implementation: eager
      resize_imgs_with_padding:
        - 224
        - 224
      language_tokenizer_path: ""
      proj_width: 1024
      num_steps: 10
      memory_enabled: true
      memory_hidden: 1024
      memory_num_layers: 2
      memory_project_to_vae_dim: true
