# RL Training Configuration for F1-VLA
# This config is shared across all three training phases

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  # Console log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  console_level: "WARNING"
  # File log level - set to WARNING to reduce file size
  file_level: "WARNING"
  # Enable/disable file logging
  enable_file_logging: true

# =============================================================================
# Model Configuration
# =============================================================================
model:
  config_file: /mnt/data2/ty/F1-VLA/f1_vla/config/debug_test.yaml
  
  # LoRA configuration
  lora:
    r: 8
    lora_alpha: 32
    target_modules: ["q_proj", "v_proj"]
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

# =============================================================================
# Environment Configuration
# =============================================================================
environment:
  # Embodiment - single arm
  embodiment: ["franka-panda"]
  
  # Single arm mode: only use left arm, right arm is disabled
  single_arm: true
  
  # Scene reset interval: only regenerate scene every N episodes
  # This significantly speeds up training by reusing the same scene
  # Set to 1 to regenerate scene every episode (slow)
  scene_reset_interval: 50
  
  # Task settings
  task_name: "random_exploration"
  control_mode: "delta_qpos"
  delta_qpos_scale: 0.05
  render_mode: "rasterize"  # "rt" or "rasterize"
  num_objects: 0
  
  # Camera configuration - collect both cameras
  camera:
    head_camera_type: "D435"
    wrist_camera_type: "D435"
    collect_head_camera: true
    collect_wrist_camera: true
  
  # Domain randomization
  domain_randomization:
    random_appearance: false
    random_background: true
    random_light: true
    cluttered_table: true
  
  # Data type
  data_type:
    collect_rgb: true
    collect_depth: false
    collect_qpos: true
    collect_endpose: true

# =============================================================================
# Training Configuration (shared across phases)
# =============================================================================
training:
  # Dimensions
  action_dim: 32
  state_dim: 32
  
  # World model configuration (read from debug_test.yaml)
  # history_length will be auto-calculated as: n_obs_img_steps + (n_obs_img_steps - 1) * obs_img_stride
  n_pred_img_steps: 1
  
  # Training parameters
  learning_rate: 1.0e-4
  weight_decay: 1.0e-4
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  
  # Scheduler
  scheduler:
    type: "cosine"
    T_max: 10000
    eta_min: 1.0e-6
  
  # Episode settings
  num_episodes: 10000
  steps_per_episode: 50
  batch_size: 8
  
  # Logging & checkpointing
  log_every: 10
  save_every: 1000
  
  # Memory/Sequential processing
  # F1-VLA has memory_rnn that needs sequential state propagation
  sequential_training: true
  
  # Action bounds
  action_scale: 0.05  # Scale factor for action bounds (0.01-1.0)
  action_bounds:
    low: -1.0
    high: 1.0

# =============================================================================
# Phase 1: Teacher Training Configuration
# =============================================================================
teacher:
  output_dir: "./outputs/teacher_rl"
  
  # Teacher uses random exploration
  exploration_type: "random"
  
  # Train world model only
  train_world_model: true
  train_action_head: false
  freeze_vision_encoder: true

# =============================================================================
# Phase 2: Student/Explorer Training Configuration
# =============================================================================
student:
  output_dir: "./outputs/student_rl"
  
  # Student uses learned policy for exploration
  exploration_type: "policy"
  
  # Freeze world model, train action head
  train_world_model: false
  train_action_head: true
  freeze_vision_encoder: true
  
  # Reward weights
  rewards:
    memory_divergence_weight: 0.5
    wm_uncertainty_weight: 0.5
  
  # PPO parameters
  ppo:
    clip_epsilon: 0.2
    entropy_coef: 0.01
    value_loss_coef: 0.5
    gamma: 0.99
    gae_lambda: 0.95

# =============================================================================
# Phase 3: Adversarial Training Configuration
# =============================================================================
adversarial:
  output_dir: "./outputs/adversarial_rl"
  total_iterations: 100000
  
  # Adversarial training parameters
  wm_updates_per_iter: 5
  explorer_updates_per_iter: 1
  warmup_steps: 1000
  buffer_size: 100000
  
  # Adversarial weights
  adversarial_weight: 0.5
  entropy_weight: 0.01
  
  # Separate learning rates
  wm_lr: 1.0e-4
  explorer_lr: 3.0e-4
  
  # Explorer network
  explorer:
    hidden_dim: 256
    image_encoder_dim: 512

# =============================================================================
# Device Configuration
# =============================================================================
device: "cuda"
debug: false
