# RL Training Configuration for F1-VLA
# This config is shared across all three training phases

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  # Console log level for main process: DEBUG, INFO, WARNING, ERROR, CRITICAL
  # Non-main processes (other GPUs in DDP) always use WARNING to reduce noise
  console_level: "INFO"
  # File log level - set to WARNING to reduce file size
  file_level: "CRITICAL"
  # Enable/disable file logging
  enable_file_logging: false

# =============================================================================
# Model Configuration
# =============================================================================
model:
  config_file: /mnt/data2/ty/F1-VLA/f1_vla/config/debug_test.yaml
  
  # LoRA configuration
  lora:
    r: 8
    lora_alpha: 32
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

# =============================================================================
# Environment Configuration
# =============================================================================
environment:
  # Embodiment - single arm
  embodiment: ["franka-panda"]
  
  # Single arm mode: only use left arm, right arm is disabled
  single_arm: true
  
  # Scene reset interval: only regenerate scene every N episodes
  # This significantly speeds up training by reusing the same scene
  # Set to 1 to regenerate scene every episode (slow)
  scene_reset_interval: 50
  randomize_robot_init: false
  
  # Motion planner settings (for RL training, disable to save time and GPU memory)
  # need_planner: false = skip CuRobo motion planner initialization (saves ~10GB VRAM)
  # need_topp: false = skip MplibPlanner initialization
  # These are only needed for trajectory planning, not for delta action control
  need_planner: false
  need_topp: false
  
  # Task settings
  task_name: "random_exploration"
  control_mode: "delta_qpos"
  delta_qpos_scale: 0.05
  render_mode: "rasterize"  # "rt" or "rasterize"
  num_objects: 0
  
  # Camera configuration - collect both cameras
  camera:
    head_camera_type: "D435"
    wrist_camera_type: "D435"
    collect_head_camera: true
    collect_wrist_camera: true
  
  # Domain randomization
  domain_randomization:
    random_appearance: false
    random_background: true
    random_light: true
    cluttered_table: true
  
  # Data type
  data_type:
    collect_rgb: true
    collect_depth: false
    collect_qpos: true
    collect_endpose: true

# =============================================================================
# Training Configuration (shared across phases)
# =============================================================================
training:
  # Dimensions
  action_dim: 32
  state_dim: 32
  
  # World model configuration (read from debug_test.yaml)
  # history_length will be auto-calculated as: n_obs_img_steps + (n_obs_img_steps - 1) * obs_img_stride
  n_pred_img_steps: 1
  
  # Training parameters - adjusted for better convergence
  learning_rate: 1.0e-5  # Reduced for stability
  weight_decay: 1.0e-5
  gradient_accumulation_steps: 2
  max_grad_norm: 0.5  # Reduced from 1.0 for tighter clipping
  
  # Scheduler
  scheduler:
    type: "cosine"
    T_max: 10000
    eta_min: 1.0e-6
  
  # Episode settings
  num_episodes: 10000
  steps_per_episode: 50
  batch_size: 1
  # Mini-batch size for PPO updates
  mini_batch_size: 16
  
  # Logging & checkpointing
  log_every: 1
  save_every: 200
  
  # Memory/Sequential processing
  # F1-VLA has memory_rnn that needs sequential state propagation
  # Set to false when memory_enabled=false in f1_config.json
  sequential_training: true
  # BPTT (Backpropagation Through Time) sequence length for memory training
  # Longer sequences = better memory learning, but higher memory cost and gradient issues
  # Recommended: 4-8 for stability, up to 16 if memory is sufficient
  bptt_length: 4
  # Enable gradient flow through memory states (requires sequential_training=true)
  # When true, memory GRU can learn better encodings through backprop
  # When false, memory state is detached each step (faster but memory doesn't improve)
  # Note: Gradients only flow WITHIN each sequence (truncated BPTT), not across sequences
  memory_backprop: true
  
  # Action bounds
  action_scale: 0.5  # Scale factor for action bounds (0.01-1.0)
  action_bounds:
    low: -1.0
    high: 1.0
  # How often to save videos (1 = every episode)
  video_save_every: 20

# =============================================================================
# Phase 1: Teacher Training Configuration
# =============================================================================
teacher:
  output_dir: "./outputs/teacher_rl"
  
  # Teacher uses random exploration
  exploration_type: "random"
  
  # Train world model only
  train_world_model: true
  train_action_head: false
  freeze_vision_encoder: true

# =============================================================================
# Phase 2: Student/Explorer Training Configuration
# =============================================================================
student:
  output_dir: "./outputs/student_rl"
  
  # Student uses learned policy for exploration
  exploration_type: "policy"
  
  # Freeze world model, train action head
  train_world_model: false
  train_action_head: true
  freeze_vision_encoder: true
  
  # Reward weights - adjusted based on training observations
  rewards:
    # Memory divergence: NEW DESIGN - directly reward closeness to teacher
    # Normalized divergence typically ranges from 0.1 to 2.0
    # Higher weight to encourage strong memory alignment
    memory_divergence_weight: 1.0
    # World model uncertainty: encourage exploration that surprises WM
    # Typical range: 3.0 to 8.0
    wm_uncertainty_weight: 1.0
    # Task success reward weight
    task_success_weight: 1.0
  
  # PPO parameters - adjusted for stability and exploration
  ppo:
    clip_epsilon: 0.2  # Standard PPO value
    entropy_coef: 0.01  # Reduced from 0.05 for stability
    value_loss_coef: 0.5
    gamma: 0.99
    gae_lambda: 0.95
    # Additional stability controls
    normalize_advantages: true
    clip_value_loss: true
    # Max gradient norm for PPO optimizer
    max_grad_norm: 1.0

# =============================================================================
# Phase 3: Adversarial Training Configuration
# =============================================================================
adversarial:
  output_dir: "./outputs/adversarial_rl"
  total_iterations: 100000
  
  # Adversarial training parameters
  wm_updates_per_iter: 5
  explorer_updates_per_iter: 1
  warmup_steps: 1000
  buffer_size: 100000
  
  # Adversarial weights
  adversarial_weight: 0.5
  entropy_weight: 0.01
  
  # Separate learning rates
  wm_lr: 1.0e-4
  explorer_lr: 3.0e-4
  
  # # Explorer network
  # explorer:
  #   hidden_dim: 256
  #   image_encoder_dim: 512

# =============================================================================
# Device Configuration
# =============================================================================
device: "cuda"
debug: false
