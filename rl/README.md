# F1-VLA RL Training Module

This directory contains the Reinforcement Learning training framework for F1-VLA.

## üìÅ Directory Structure

```
rl/
‚îú‚îÄ‚îÄ __init__.py                  # Module exports
‚îú‚îÄ‚îÄ f1_rl_env.py                 # F1-VLA RL environment (Gymnasium compatible)
‚îú‚îÄ‚îÄ gym_wrapper.py               # Core Gymnasium environment wrapper
‚îú‚îÄ‚îÄ vec_env.py                   # Vectorized environment utilities
‚îú‚îÄ‚îÄ normalizers.py               # Action/State normalization
‚îú‚îÄ‚îÄ suppress_logs.py             # Log suppression utilities
‚îú‚îÄ‚îÄ rl_config.yaml               # Training configuration
‚îú‚îÄ‚îÄ GYM_README.md                # Gymnasium wrapper documentation
‚îú‚îÄ‚îÄ README.md                    # This file
‚îÇ
‚îî‚îÄ‚îÄ training/                    # Training scripts
    ‚îú‚îÄ‚îÄ __init__.py              # Training module exports
    ‚îú‚îÄ‚îÄ rl_training_common.py    # Shared training utilities (config, model loading, etc.)
    ‚îú‚îÄ‚îÄ parallel_utils.py        # Multi-GPU/multi-env utilities (Accelerate + SyncVectorEnv)
    ‚îú‚îÄ‚îÄ train_teacher_rl.py      # Phase 1: World Model training
    ‚îú‚îÄ‚îÄ train_student_rl.py      # Phase 2: Explorer training
    ‚îú‚îÄ‚îÄ train_adversarial_rl.py  # Phase 3: Adversarial training
    ‚îú‚îÄ‚îÄ train.sh                 # Shell script entry point (recommended)
    ‚îî‚îÄ‚îÄ evaluate_rl.py           # Policy evaluation
```

## üéØ Three-Phase Training Architecture

F1-VLA uses a three-phase training approach:

### Phase 1: Teacher Training (World Model)
Train the World Model to predict next frame observations.

- **Input**: History images (head + wrist cameras), action history, states, **memory state**
- **Output**: Predicted next wrist camera frame (VQ-VAE tokens)
- **Loss**: Cross-entropy on predicted vs actual image tokens
- **Actions**: Random (for exploration/data collection)
- **Memory**: GRU memory state propagated through sequence (initialized to zeros for first frame)

### Phase 2: Student Training (Explorer)
Train the action policy using the frozen World Model from Phase 1.

- **Input**: Wrist camera only (no head camera), **memory state**
- **Output**: Actions from F1-VLA actor
- **Reward**: Memory divergence + WM uncertainty
- **World Model**: Frozen (from Phase 1)

### Phase 3: Adversarial Training
Jointly train World Model and Explorer in adversarial manner.

- **World Model**: Tries to accurately predict next frame
- **Explorer**: Tries to find actions that make WM's predictions fail
- **Result**: Both become more robust

## ‚öôÔ∏è Configuration

### Main Config: `rl_config.yaml`

```yaml
# Model configuration
model:
  config_file: "/path/to/f1_vla/config/debug_test.yaml"
  lora:
    r: 8
    lora_alpha: 32
    target_modules: ["q_proj", "v_proj"]

# Training parameters (shared across phases)
training:
  num_episodes: 10000
  steps_per_episode: 50
  batch_size: 8
  learning_rate: 1e-4
  action_dim: 32
  state_dim: 32
  n_pred_img_steps: 1
  save_every: 1000
  log_every: 10
  video_save_every: 100
  sample_save_every: 100

# Environment configuration
environment:
  task_name: "random_exploration"
  control_mode: "delta_qpos"
  single_arm: true
  scene_reset_interval: 10
  embodiment: ["franka-panda"]
  domain_randomization:
    random_appearance: false
    random_background: true
    random_light: true
    cluttered_table: true

# Logging configuration
logging:
  console_level: "WARNING"  # Only show warnings+ in console
  file_level: "DEBUG"       # Full debug in log file
  enable_file_logging: true

# Phase-specific configs
teacher:
  output_dir: "./outputs/teacher_rl"

student:
  output_dir: "./outputs/student_rl"
  rewards:
    memory_divergence_weight: 0.5
    wm_uncertainty_weight: 0.5

adversarial:
  output_dir: "./outputs/adversarial_rl"
  total_iterations: 100000
```

### Model Config: `f1_vla/config/debug_test.yaml`

This config specifies:
- `n_obs_img_steps`: Number of history frames (default: 4)
- `n_pred_img_steps`: Number of prediction frames (default: 1)
- `obs_img_stride`: Stride for observation sampling

### Memory Configuration: `f1_vla/config/f1_config.json`

```json
{
  "memory_enabled": true,
  "memory_hidden": 2048,
  "memory_num_layers": 4,
  "memory_project_to_vae_dim": true
}
```

## üöÄ Training Entry Point

### Recommended: Shell Script

```bash
cd /path/to/F1-VLA

# Single GPU
bash ./RoboTwin/rl/training/train.sh teacher 1 1 --num_episodes 1000

# Multi-GPU (4 GPUs, 2 envs per GPU)
CUDA_VISIBLE_DEVICES=0,1,2,3 bash ./RoboTwin/rl/training/train.sh teacher 4 2 --num_episodes 10000

# Arguments:
#   $1: phase (teacher|student|adversarial)
#   $2: num_gpus (default: 1)
#   $3: num_envs_per_gpu (default: 1)
#   $4+: additional arguments passed to training script
```

### Direct Python

```bash
cd /path/to/F1-VLA/RoboTwin

# Single GPU
python -m rl.training.train_teacher_rl --num_episodes 1000

# Multi-GPU with Accelerate
accelerate launch --num_processes=4 \
    -m rl.training.train_teacher_rl \
    --num_envs 2 \
    --num_episodes 10000
```

## üîß Key Components

### 1. Memory State Management (‚ö†Ô∏è CRITICAL)

F1-VLA has a GRU-based memory that requires proper state propagation:

```python
# Memory state shape: (num_layers, batch_size, hidden_dim)
# Default: (4, batch_size, 2048)

# First frame: MUST initialize to zeros
memory_state = torch.zeros(4, batch_size, 2048, device=device)

# Subsequent frames: use output from previous step
loss_dict = policy.forward_with_world_model(batch, ...)
memory_state = loss_dict["memory_state"]  # Use for next step

# NEVER let memory_state be None!
if memory_state is None:
    raise ValueError("memory_state cannot be None!")
```

**‚ö†Ô∏è IMPORTANT**: 
- Memory state **MUST** be initialized to zeros for the first frame
- Memory state **MUST NOT** be None during training
- The training script will raise an error if memory_state becomes None

### 2. Multi-GPU Distribution

SAPIEN uses Vulkan rendering which requires explicit GPU assignment:

```python
# At script start (BEFORE importing SAPIEN):
local_rank = int(os.environ.get("LOCAL_RANK", "0"))
cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES", "")
visible_gpus = [int(x) for x in cuda_visible.split(",") if x]
physical_gpu_id = visible_gpus[local_rank] if local_rank < len(visible_gpus) else local_rank

os.environ["VK_DEVICE_INDEX"] = str(physical_gpu_id)
os.environ["SAPIEN_DEVICE_INDEX"] = str(physical_gpu_id)
os.environ["EGL_DEVICE_ID"] = str(physical_gpu_id)
```

### 3. Camera Configuration

- **Teacher Phase**: Uses both head and wrist cameras
  - `head_rgb` ‚Üí `image0` (VLM input)
  - `wrist_rgb` ‚Üí `image1` (VLM input) + `image0_history` (WM input)
  
- **Student Phase**: Uses only wrist camera
  - `wrist_rgb` ‚Üí `image0` (VLM input) + `image0_history` (WM input)

### 4. Image History ‰∏é Action History ÁöÑÂØπÂ∫îÂÖ≥Á≥ª

**‚ö†Ô∏è ÂÖ≥ÈîÆËÆæËÆ°ÔºöÊó∂Â∫èÂØπÈΩê**

```
Êó∂Èó¥Ê≠•:     t-3    t-2    t-1    t     t+1 (È¢ÑÊµãÁõÆÊ†á)
           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ÂõæÂÉèÂéÜÂè≤:  [img‚ÇÄ, img‚ÇÅ, img‚ÇÇ, img‚ÇÉ] ‚Üí È¢ÑÊµã img‚ÇÑ
Âä®‰ΩúÂéÜÂè≤:  [a‚Çã‚ÇÉ,  a‚Çã‚ÇÇ,  a‚Çã‚ÇÅ,  a‚ÇÄ  ]
Áä∂ÊÄÅ:                           s_t
```

**ÂØπÂ∫îÂÖ≥Á≥ª**:
- `image_history[i]` ÂØπÂ∫î `action_history[i]` **ÊâßË°åÂâç**ÁöÑËßÇÊµã
- `image_history[i+1]` ÊòØÊâßË°å `action_history[i]` **Âêé**ÁöÑËßÇÊµã
- World Model ÁöÑÁõÆÊ†áÊòØÔºöÁªôÂÆö `[img‚ÇÄ...img‚ÇÉ]` + `[a‚ÇÄ...a‚ÇÉ]` ‚Üí È¢ÑÊµã `img‚ÇÑ`

**ÈÖçÁΩÆÂèÇÊï∞**:
- `n_obs_img_steps`: ÂõæÂÉèÂéÜÂè≤ÈïøÂ∫¶ (ÈªòËÆ§: 4)
- `obs_img_stride`: ÈááÊ†∑Ê≠•Èïø (ÈªòËÆ§: 1)
- ÂÆûÈôÖÂéÜÂè≤Â∏ßÊï∞ = `n_obs_img_steps * obs_img_stride`

**‰ª£Á†ÅÂÆûÁé∞**:
```python
# Âú® BatchBuilder.build_batch() ‰∏≠:
# image_history ÂåÖÂê´ [img_t-3, img_t-2, img_t-1, img_t, img_t+1]
# ÂÖ∂‰∏≠ img_t+1 ÊòØ next_obsÔºàÈ¢ÑÊµãÁõÆÊ†áÔºâÔºåÂú®ËÆ≠ÁªÉÊó∂ÊãºÊé•

# Âú® f1_rl_env.py ‰∏≠:
# action_history Áª¥Êä§ÊúÄËøëÁöÑÂä®‰ΩúÂ∫èÂàó
# ÊØèÊ¨° step() ÂêéÊõ¥Êñ∞: action_history.append(action); action_history.pop(0)

# ‚ö†Ô∏è ÈáçË¶Å: ‰∏§‰∏™ÂéÜÂè≤ÂøÖÈ°ªÂêåÊ≠•Áª¥Êä§ÔºÅ
# Â¶ÇÊûú image_history Êúâ 4 Â∏ßÔºåaction_history ‰πüÂøÖÈ°ªÊúâÂØπÂ∫îÁöÑ 4 ‰∏™Âä®‰Ωú
```

**È™åËØÅÊ£ÄÊü•**:
```python
assert len(image_history) == n_obs_img_steps, "Image history length mismatch"
assert len(action_history) == n_obs_img_steps, "Action history length mismatch"
```

### 5. Action Space

Default action dimension: 32 (padded)
- For single arm (Franka Panda): 8 actual DOFs (7 joints + 1 gripper)
- Control mode: `delta_qpos` (joint position deltas)

## üìä Outputs

Each training phase produces:

```
outputs/teacher_rl/
‚îú‚îÄ‚îÄ checkpoint-{episode}/
‚îÇ   ‚îú‚îÄ‚îÄ model.pt           # Model weights
‚îÇ   ‚îî‚îÄ‚îÄ training_state.pt  # Optimizer, scheduler, metrics
‚îú‚îÄ‚îÄ samples/
‚îÇ   ‚îî‚îÄ‚îÄ episode_{n}.png    # Prediction comparison images
‚îú‚îÄ‚îÄ videos/
‚îÇ   ‚îî‚îÄ‚îÄ episode_{n}.mp4    # Episode recordings (Head | GT Wrist | Predicted)
‚îú‚îÄ‚îÄ episode_metrics.jsonl  # Training metrics log
‚îî‚îÄ‚îÄ tensorboard/           # Tensorboard logs
```

## ‚ö†Ô∏è Important Notes

### 1. Memory State is Critical
- Memory state must be initialized to zeros for the first frame
- Must be propagated through the sequence (not None)
- The training script will raise an error if memory_state becomes None
- Logger records memory state info at DEBUG level for debugging

### 2. SAPIEN GPU Assignment
- SAPIEN uses Vulkan which doesn't respect `CUDA_VISIBLE_DEVICES`
- Must set `VK_DEVICE_INDEX` **before** importing SAPIEN
- This is done automatically in training scripts

### 3. collect_steps vs steps_per_episode
- `collect_steps(num_steps)` collects `num_steps` total across all envs
- For `num_envs=2` and `steps_per_episode=50`, use `num_steps = 50 * 2 = 100`

### 4. Logging Levels
- Console shows only WARNING+ by default (for clean output)
- Full DEBUG logs are written to `logs/rl_training_*.log`
- Set `console_level: "INFO"` in config for more verbose output
- Memory state logging is at DEBUG level

### 5. Domain Randomization
- `cluttered_table: true` adds random objects to the scene
- `random_background: true` randomizes wall/table textures
- `random_light: true` randomizes lighting conditions

## üêõ Troubleshooting

### GPU Distribution Issues
```bash
# Check GPU assignment
nvidia-smi  # Should show processes on different GPUs

# If all processes on GPU 0:
# Ensure VK_DEVICE_INDEX is set BEFORE importing SAPIEN
```

### Memory State Errors
```
ValueError: CRITICAL: initial_memory_state is None
```
This means memory state wasn't properly initialized. Check:
1. `memory_enabled: true` in model config
2. `_init_memory_state()` is being called
3. Memory state is not being set to None anywhere in the pipeline

### SAPIEN Rendering Issues
```
SIGBUS or Vulkan errors
```
Try:
1. Use `PhysxCpuSystem` instead of `PhysxGpuSystem`
2. Check GPU memory availability
3. Verify Vulkan drivers are installed

### Video Only Has 21 Frames
This was a bug where `collect_steps(steps_per_episode)` was used instead of `collect_steps(steps_per_episode * num_envs)`. This has been fixed.

## üìö API Reference

### rl_training_common.py

```python
# GPU assignment utilities
get_physical_gpu_id(accelerator=None) -> int
setup_sapien_gpu(gpu_id: Optional[int] = None)

# Config loading
load_rl_config(config_path: str) -> DictConfig
get_training_config(config: DictConfig) -> TrainingConfig
get_environment_config(config: DictConfig) -> Dict[str, Any]

# Model loading
load_f1_policy(config_file, device, debug, lora_config, checkpoint_path) -> Tuple[policy, config, full_config]

# Training utilities
set_policy_requires_grad(policy, freeze_vision_encoder, freeze_gen_expert, train_act_expert_only, train_gen_expert_only)
setup_optimizer(policy, lr, weight_decay) -> Optimizer
setup_scheduler(optimizer, scheduler_type, T_max, eta_min) -> Scheduler

# Batch building
class BatchBuilder:
    def build_batch(transitions, include_memory_states=True) -> Dict[str, Tensor]

# Memory management
class MemoryStateManager:
    def reset()
    def update(memory_state)
    def get_current() -> Optional[Tensor]
```

### f1_rl_env.py

```python
class F1RLEnv(gymnasium.Env):
    def __init__(task_config, phase, teacher_policy, history_length, max_steps, device, action_scale, single_arm)
    def reset(seed=None, options=None) -> Tuple[obs, info]
    def step(action) -> Tuple[obs, reward, terminated, truncated, info]
    def close()

class TeacherEnv(F1RLEnv):  # Convenience wrapper for phase="teacher"
class StudentEnv(F1RLEnv):  # Convenience wrapper for phase="student"
```

## üìö References

- [F1-VLA Paper](https://arxiv.org/abs/...)
- [SAPIEN Documentation](https://sapien.ucsd.edu/docs/)
- [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/)
- [Gymnasium](https://gymnasium.farama.org/)
