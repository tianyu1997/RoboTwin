17:57:51 [INFO] Using external F1-VLA configuration file: /mnt/data2/ty/F1-VLA/f1_vla/config/debug_test.yaml
17:57:51 [INFO] Loading teacher policy from: /mnt/data2/ty/F1-VLA/RoboTwin/outputs/teacher_rl/checkpoint-400
17:57:53 [INFO] PyTorch version 2.7.1 available.
17:59:07 [INFO] Missing keys (324): ['base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.down_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.self_attn.k_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.self_attn.o_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.gate_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.up_proj.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.down_proj.weight']
17:59:07 [INFO] Unexpected keys (918): ['base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.base_layer.bias', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.paligemma.language_model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_expert.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.gate_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.up_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.paligemma_with_expert.gemma_wm_expert.model.layers.17.mlp.down_proj.lora_B.default.weight']
18:00:24 [INFO] Models loaded successfully
18:00:24 [INFO] Student trainable parameters: 33,056,802 / 4,238,560,181
18:00:24 [INFO] Loading domain_randomization from config: {'random_appearance': False, 'random_background': True, 'random_light': True, 'cluttered_table': True}
18:00:24 [INFO] Memory config: enabled=True, hidden=1024, layers=2
18:00:24 [INFO] Starting student training for 2000 episodes
18:00:24 [INFO] Starting from episode: 0
18:00:24 [INFO] Student environment setup complete
18:00:24 [INFO] [Rank 0] Starting training loop

============================================================
Student Policy Training (Phase 2)
============================================================
Device: cuda:0
Using DDP: False
Number of episodes: 2000
============================================================
  Loading config file...
  Loading F1Config from: /mnt/data2/ty/F1-VLA/f1_vla/config/f1_config.json
  Loading pretrained model from: /mnt/data2/ty/F1-VLA/pi0
[93mFreeze vision encoder: True[0m
[93mFreeze gen expert: False[0m
[93mTrain act expert only: False[0m
[93mTrain gen expert only: False[0m
Loading weights from local directory: /mnt/data2/ty/F1-VLA/pi0
  Loading additional checkpoint...
  Pretrained weights loaded successfully
  Applying LoRA configuration...
  Loading checkpoint from: /mnt/data2/ty/F1-VLA/RoboTwin/outputs/teacher_rl/checkpoint-400
  Loading RL checkpoint (model.pt)
  RL checkpoint loaded (missing_keys=324, unexpected_keys=918)
  Unfrozen 73 memory parameters
  Moving model to cuda:0...
  Model ready!
  Loading config file...
  Loading F1Config from: /mnt/data2/ty/F1-VLA/f1_vla/config/f1_config.json
  Loading pretrained model from: /mnt/data2/ty/F1-VLA/pi0
[93mFreeze vision encoder: True[0m
[93mFreeze gen expert: False[0m
[93mTrain act expert only: False[0m
[93mTrain gen expert only: False[0m
Loading weights from local directory: /mnt/data2/ty/F1-VLA/pi0
  Loading additional checkpoint...
  Pretrained weights loaded successfully
  Applying LoRA configuration...
  Unfrozen 73 memory parameters
  Moving model to cuda:0...
  Model ready!
Student policy config from /mnt/data2/ty/F1-VLA/f1_vla/config/debug_test.yaml:
  n_obs_img_steps: 4
  n_pred_img_steps: 1
  history_length: 4

Configuring student training: Action Expert only
[93mFreeze vision encoder: True[0m
[93mFreeze gen expert: True[0m
[93mTrain act expert only: True[0m
[93mTrain gen expert only: False[0m
Training Episodes:   0%|          | 0/2000 [00:00<?, ?ep/s][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
18:00:40 [INFO] [Step 0] First memory divergence measurement: 50.9646, setting reward=0
18:03:26 [INFO] [Episode 0] Completed - Detailed Statistics:
18:03:26 [INFO]   Total steps: 50
18:03:26 [INFO]   Episode reward: 104.1955
18:03:26 [INFO]   Avg memory div reward: -2.492457
18:03:26 [INFO]   Avg memory div abs: 50.917026
18:03:26 [INFO]   Avg WM uncertainty: 4.576368
18:03:26 [INFO]   Min/Max reward: 1.7533 / 4.6245
18:04:46 [INFO] [Episode 0] PPO Epoch 0 - Training Losses:
18:04:46 [INFO]   Policy loss: 0.288154
18:04:46 [INFO]   Value loss: 23.346480
18:04:46 [INFO]   Entropy: -18.593869
18:04:46 [INFO]   Total loss: 12.147332
18:06:01 [INFO] Memory Gate Act: -0.000011
18:08:21 [INFO] ================================================================================
18:08:21 [INFO] Episode 0 - Aggregated Metrics Summary:
18:08:21 [INFO]   Reward: 104.1955 (recent avg: 104.1955, std: 0.0000)
18:08:21 [INFO]   Policy Loss: 0.263712 (recent avg: 0.263712)
18:08:21 [INFO]   Value Loss: 23.337799
18:08:21 [INFO]   Entropy: -18.593715
18:08:21 [INFO]   Memory Div Reward: -2.492457 (recent avg: -2.492457)
18:08:21 [INFO]   WM Uncertainty: 4.576368
18:08:21 [INFO]   Learning Rate: 1.00e-06
18:08:21 [INFO]   Total Steps: 50
18:08:21 [INFO]   Log Std: mean=-2.0000, std=0.0000
18:08:21 [INFO] ================================================================================
18:08:21 [INFO] Episode 0: reward=104.20, p_loss=0.2637, v_loss=23.3378, entropy=-18.5937, mem_div=-2.4925, wm_unc=4.5764, lr=1.00e-06
Training Episodes:   0%|          | 1/2000 [07:56<264:50:21, 476.95s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
18:08:26 [INFO] [Step 0] First memory divergence measurement: 50.8380, setting reward=0
18:11:02 [INFO] [Episode 1] Completed - Detailed Statistics:
18:11:02 [INFO]   Total steps: 50
18:11:02 [INFO]   Episode reward: 103.8692
18:11:02 [INFO]   Avg memory div reward: -2.485584
18:11:02 [INFO]   Avg memory div abs: 50.763519
18:11:02 [INFO]   Avg WM uncertainty: 4.562967
18:11:02 [INFO]   Min/Max reward: 1.7469 / 4.6886
18:12:14 [INFO] [Episode 1] PPO Epoch 0 - Training Losses:
18:12:14 [INFO]   Policy loss: 0.341640
18:12:14 [INFO]   Value loss: 23.185577
18:12:14 [INFO]   Entropy: -18.593424
18:12:14 [INFO]   Total loss: 12.120363
18:15:51 [INFO] ================================================================================
18:15:51 [INFO] Episode 1 - Aggregated Metrics Summary:
18:15:51 [INFO]   Reward: 104.0324 (recent avg: 104.0324, std: 0.1632)
18:15:51 [INFO]   Policy Loss: 0.326405 (recent avg: 0.326405)
18:15:51 [INFO]   Value Loss: 23.257897
18:15:51 [INFO]   Entropy: -18.593486
18:15:51 [INFO]   Memory Div Reward: -2.489020 (recent avg: -2.489020)
18:15:51 [INFO]   WM Uncertainty: 4.569667
18:15:51 [INFO]   Learning Rate: 1.00e-06
18:15:51 [INFO]   Total Steps: 100
18:15:51 [INFO]   Log Std: mean=-2.0000, std=0.0000
18:15:51 [INFO] ================================================================================
18:15:51 [INFO] Episode 1: reward=104.03, p_loss=0.3264, v_loss=23.2579, entropy=-18.5935, mem_div=-2.4890, wm_unc=4.5697, lr=1.00e-06
Training Episodes:   0%|          | 2/2000 [15:27<255:55:37, 461.13s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
18:15:56 [INFO] [Step 0] First memory divergence measurement: 50.8486, setting reward=0
18:17:33 [INFO] Memory Gate WM: 0.000019
18:18:46 [INFO] [Episode 2] Completed - Detailed Statistics:
18:18:46 [INFO]   Total steps: 50
18:18:46 [INFO]   Episode reward: 100.8839
18:18:46 [INFO]   Avg memory div reward: -2.483496
18:18:46 [INFO]   Avg memory div abs: 50.753314
18:18:46 [INFO]   Avg WM uncertainty: 4.501174
18:18:46 [INFO]   Min/Max reward: 1.5847 / 4.5015
18:19:18 [INFO] Memory Gate Act: -0.000011
18:19:57 [INFO] [Episode 2] PPO Epoch 0 - Training Losses:
18:19:57 [INFO]   Policy loss: 0.367013
18:19:57 [INFO]   Value loss: 22.792405
18:19:57 [INFO]   Entropy: -18.593067
18:19:57 [INFO]   Total loss: 11.949146
18:21:36 [INFO] Memory Gate WM: 0.000023
18:23:28 [INFO] ================================================================================
18:23:28 [INFO] Episode 2 - Aggregated Metrics Summary:
18:23:28 [INFO]   Reward: 102.9829 (recent avg: 102.9829, std: 1.4902)
18:23:28 [INFO]   Policy Loss: 0.314067 (recent avg: 0.323115)
18:23:28 [INFO]   Value Loss: 23.100149
18:23:28 [INFO]   Entropy: -18.593334
18:23:28 [INFO]   Memory Div Reward: -2.487179 (recent avg: -2.487179)
18:23:28 [INFO]   WM Uncertainty: 4.546836
18:23:28 [INFO]   Learning Rate: 1.00e-06
18:23:28 [INFO]   Total Steps: 150
18:23:28 [INFO]   Log Std: mean=-2.0000, std=0.0000
18:23:28 [INFO] ================================================================================
18:23:28 [INFO] Episode 2: reward=102.98, p_loss=0.3141, v_loss=23.1001, entropy=-18.5933, mem_div=-2.4872, wm_unc=4.5468, lr=1.00e-06
Training Episodes:   0%|          | 3/2000 [23:04<254:50:56, 459.42s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
18:23:33 [INFO] [Step 0] First memory divergence measurement: 50.7252, setting reward=0
18:24:28 [INFO] Memory Gate Act: -0.000025
18:26:22 [INFO] [Episode 3] Completed - Detailed Statistics:
18:26:22 [INFO]   Total steps: 50
18:26:22 [INFO]   Episode reward: 107.2403
18:26:22 [INFO]   Avg memory div reward: -2.489349
18:26:22 [INFO]   Avg memory div abs: 50.796945
18:26:22 [INFO]   Avg WM uncertainty: 4.634155
18:26:22 [INFO]   Min/Max reward: 1.7325 / 4.7408
18:27:23 [INFO] Memory Gate WM: 0.000029
18:27:34 [INFO] [Episode 3] PPO Epoch 0 - Training Losses:
18:27:34 [INFO]   Policy loss: 0.257678
18:27:34 [INFO]   Value loss: 23.881437
18:27:34 [INFO]   Entropy: -18.592980
18:27:34 [INFO]   Total loss: 12.384326
18:31:07 [INFO] ================================================================================
18:31:07 [INFO] Episode 3 - Aggregated Metrics Summary:
18:31:07 [INFO]   Reward: 104.0472 (recent avg: 104.0472, std: 2.2503)
18:31:07 [INFO]   Policy Loss: 0.295189 (recent avg: 0.306628)
18:31:07 [INFO]   Value Loss: 23.293668
18:31:07 [INFO]   Entropy: -18.593241
18:31:07 [INFO]   Memory Div Reward: -2.487721 (recent avg: -2.487721)
18:31:07 [INFO]   WM Uncertainty: 4.568666
18:31:07 [INFO]   Learning Rate: 1.00e-06
18:31:07 [INFO]   Total Steps: 200
18:31:07 [INFO]   Log Std: mean=-2.0000, std=0.0000
18:31:07 [INFO] ================================================================================
18:31:07 [INFO] Episode 3: reward=104.05, p_loss=0.2952, v_loss=23.2937, entropy=-18.5932, mem_div=-2.4877, wm_unc=4.5687, lr=1.00e-06
Training Episodes:   0%|          | 4/2000 [30:42<254:28:16, 458.97s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
18:31:12 [INFO] [Step 0] First memory divergence measurement: 50.9122, setting reward=0
18:33:59 [INFO] [Episode 4] Completed - Detailed Statistics:
18:33:59 [INFO]   Total steps: 50
18:33:59 [INFO]   Episode reward: 107.1402
18:33:59 [INFO]   Avg memory div reward: -2.488332
18:33:59 [INFO]   Avg memory div abs: 50.842838
18:33:59 [INFO]   Avg WM uncertainty: 4.631136
18:33:59 [INFO]   Min/Max reward: 1.8212 / 4.6005
18:35:13 [INFO] [Episode 4] PPO Epoch 0 - Training Losses:
18:35:13 [INFO]   Policy loss: 0.244566
18:35:13 [INFO]   Value loss: 24.201248
18:35:13 [INFO]   Entropy: -18.592921
18:35:13 [INFO]   Total loss: 12.531119
18:38:53 [INFO] ================================================================================
18:38:53 [INFO] Episode 4 - Aggregated Metrics Summary:
18:38:53 [INFO]   Reward: 104.6658 (recent avg: 104.6658, std: 2.3626)
18:38:53 [INFO]   Policy Loss: 0.290699 (recent avg: 0.255431)
18:38:53 [INFO]   Value Loss: 23.473876
18:38:53 [INFO]   Entropy: -18.593169
18:38:53 [INFO]   Memory Div Reward: -2.487844 (recent avg: -2.487844)
18:38:53 [INFO]   WM Uncertainty: 4.581160
18:38:53 [INFO]   Learning Rate: 1.00e-06
18:38:53 [INFO]   Total Steps: 250
18:38:53 [INFO]   Log Std: mean=-2.0000, std=0.0000
18:38:53 [INFO] ================================================================================
18:38:53 [INFO] Episode 4: reward=104.67, p_loss=0.2907, v_loss=23.4739, entropy=-18.5932, mem_div=-2.4878, wm_unc=4.5812, lr=1.00e-06
Training Episodes:   0%|          | 5/2000 [38:29<255:50:42, 461.68s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
18:38:57 [INFO] [Step 0] First memory divergence measurement: 50.9374, setting reward=0
18:40:50 [INFO] Memory Gate Act: -0.000057
18:41:49 [INFO] [Episode 5] Completed - Detailed Statistics:
18:41:49 [INFO]   Total steps: 50
18:41:49 [INFO]   Episode reward: 105.7178
18:41:49 [INFO]   Avg memory div reward: -2.486823
18:41:49 [INFO]   Avg memory div abs: 50.851389
18:41:49 [INFO]   Avg WM uncertainty: 4.601180
18:41:49 [INFO]   Min/Max reward: 1.7198 / 4.6355
18:43:03 [INFO] [Episode 5] PPO Epoch 0 - Training Losses:
18:43:03 [INFO]   Policy loss: 0.260077
18:43:03 [INFO]   Value loss: 23.695439
18:43:03 [INFO]   Entropy: -18.592836
18:43:03 [INFO]   Total loss: 12.293725
18:45:48 [INFO] Memory Gate Act: -0.000067
18:46:42 [INFO] ================================================================================
18:46:42 [INFO] Episode 5 - Aggregated Metrics Summary:
18:46:42 [INFO]   Reward: 104.8412 (recent avg: 104.8412, std: 2.1921)
18:46:42 [INFO]   Policy Loss: 0.312685 (recent avg: 0.323553)
18:46:42 [INFO]   Value Loss: 23.509384
18:46:42 [INFO]   Entropy: -18.593095
18:46:42 [INFO]   Memory Div Reward: -2.487674 (recent avg: -2.487674)
18:46:42 [INFO]   WM Uncertainty: 4.584497
18:46:42 [INFO]   Learning Rate: 1.00e-06
18:46:42 [INFO]   Total Steps: 300
18:46:42 [INFO]   Log Std: mean=-2.0000, std=0.0000
18:46:42 [INFO] ================================================================================
18:46:42 [INFO] Episode 5: reward=104.84, p_loss=0.3127, v_loss=23.5094, entropy=-18.5931, mem_div=-2.4877, wm_unc=4.5845, lr=1.00e-06
Training Episodes:   0%|          | 6/2000 [46:18<257:07:52, 464.23s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
18:46:47 [INFO] [Step 0] First memory divergence measurement: 51.0277, setting reward=0
18:47:24 [INFO] Memory Gate WM: 0.000053
18:49:15 [INFO] Memory Gate WM: 0.000053
18:49:36 [INFO] [Episode 6] Completed - Detailed Statistics:
18:49:36 [INFO]   Total steps: 50
18:49:36 [INFO]   Episode reward: 101.1102
18:49:36 [INFO]   Avg memory div reward: -2.502685
18:49:36 [INFO]   Avg memory div abs: 51.006170
18:49:36 [INFO]   Avg WM uncertainty: 4.524888
18:49:36 [INFO]   Min/Max reward: 1.7196 / 4.5927
18:50:11 [INFO] Memory Gate Act: -0.000077
18:50:47 [INFO] [Episode 6] PPO Epoch 0 - Training Losses:
18:50:47 [INFO]   Policy loss: 0.252388
18:50:47 [INFO]   Value loss: 22.639645
18:50:47 [INFO]   Entropy: -18.592650
18:50:47 [INFO]   Total loss: 11.758137
18:52:36 [INFO] Memory Gate Act: -0.000081
18:54:24 [INFO] ================================================================================
18:54:24 [INFO] Episode 6 - Aggregated Metrics Summary:
18:54:24 [INFO]   Reward: 104.3082 (recent avg: 104.3082, std: 2.4132)
18:54:24 [INFO]   Policy Loss: 0.305082 (recent avg: 0.323333)
18:54:24 [INFO]   Value Loss: 23.384101
18:54:24 [INFO]   Entropy: -18.593030
18:54:24 [INFO]   Memory Div Reward: -2.489818 (recent avg: -2.489818)
18:54:24 [INFO]   WM Uncertainty: 4.575981
18:54:24 [INFO]   Learning Rate: 1.00e-06
18:54:24 [INFO]   Total Steps: 350
18:54:24 [INFO]   Log Std: mean=-2.0000, std=0.0000
18:54:24 [INFO] ================================================================================
18:54:24 [INFO] Episode 6: reward=104.31, p_loss=0.3051, v_loss=23.3841, entropy=-18.5930, mem_div=-2.4898, wm_unc=4.5760, lr=1.00e-06
Training Episodes:   0%|          | 7/2000 [54:00<256:39:02, 463.59s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
18:54:31 [INFO] [Step 0] First memory divergence measurement: 51.0234, setting reward=0
18:54:36 [INFO] Memory Gate Act: -0.000086
18:55:07 [INFO] Memory Gate WM: 0.000072
18:56:49 [INFO] Memory Gate WM: 0.000072
18:57:20 [INFO] [Episode 7] Completed - Detailed Statistics:
18:57:20 [INFO]   Total steps: 50
18:57:20 [INFO]   Episode reward: 107.6056
18:57:20 [INFO]   Avg memory div reward: -2.493746
18:57:20 [INFO]   Avg memory div abs: 50.942896
18:57:20 [INFO]   Avg WM uncertainty: 4.645858
18:57:20 [INFO]   Min/Max reward: 1.7723 / 4.5102
18:57:23 [INFO] Memory Gate Act: -0.000086
18:57:24 [INFO] Memory Gate Act: -0.000086
18:58:34 [INFO] [Episode 7] PPO Epoch 0 - Training Losses:
18:58:34 [INFO]   Policy loss: 0.253226
18:58:34 [INFO]   Value loss: 24.244497
18:58:34 [INFO]   Entropy: -18.592632
18:58:34 [INFO]   Total loss: 12.561401
19:02:09 [INFO] ================================================================================
19:02:09 [INFO] Episode 7 - Aggregated Metrics Summary:
19:02:09 [INFO]   Reward: 104.7203 (recent avg: 104.7203, std: 2.5069)
19:02:09 [INFO]   Policy Loss: 0.297828 (recent avg: 0.263409)
19:02:09 [INFO]   Value Loss: 23.490697
19:02:09 [INFO]   Entropy: -18.592977
19:02:09 [INFO]   Memory Div Reward: -2.490309 (recent avg: -2.490309)
19:02:09 [INFO]   WM Uncertainty: 4.584716
19:02:09 [INFO]   Learning Rate: 1.00e-06
19:02:09 [INFO]   Total Steps: 400
19:02:09 [INFO]   Log Std: mean=-2.0000, std=0.0000
19:02:09 [INFO] ================================================================================
19:02:09 [INFO] Episode 7: reward=104.72, p_loss=0.2978, v_loss=23.4907, entropy=-18.5930, mem_div=-2.4903, wm_unc=4.5847, lr=1.00e-06
Training Episodes:   0%|          | 8/2000 [1:01:45<256:44:19, 463.99s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
19:02:14 [INFO] [Step 0] First memory divergence measurement: 50.9269, setting reward=0
19:03:26 [INFO] Memory Gate WM: 0.000113
19:05:01 [INFO] [Episode 8] Completed - Detailed Statistics:
19:05:01 [INFO]   Total steps: 50
19:05:01 [INFO]   Episode reward: 99.9248
19:05:01 [INFO]   Avg memory div reward: -2.492150
19:05:01 [INFO]   Avg memory div abs: 50.894877
19:05:01 [INFO]   Avg WM uncertainty: 4.490645
19:05:01 [INFO]   Min/Max reward: 1.7456 / 4.5170
19:06:14 [INFO] [Episode 8] PPO Epoch 0 - Training Losses:
19:06:14 [INFO]   Policy loss: 0.245481
19:06:14 [INFO]   Value loss: 22.425356
19:06:14 [INFO]   Entropy: -18.592569
19:06:14 [INFO]   Total loss: 11.644084
19:09:31 [INFO] Memory Gate WM: 0.000113
19:09:51 [INFO] ================================================================================
19:09:51 [INFO] Episode 8 - Aggregated Metrics Summary:
19:09:51 [INFO]   Reward: 104.1875 (recent avg: 104.1875, std: 2.8032)
19:09:51 [INFO]   Policy Loss: 0.290353 (recent avg: 0.244725)
19:09:51 [INFO]   Value Loss: 23.371566
19:09:51 [INFO]   Entropy: -18.592930
19:09:51 [INFO]   Memory Div Reward: -2.490514 (recent avg: -2.490514)
19:09:51 [INFO]   WM Uncertainty: 4.574264
19:09:51 [INFO]   Learning Rate: 1.00e-06
19:09:51 [INFO]   Total Steps: 450
19:09:51 [INFO]   Log Std: mean=-2.0000, std=0.0000
19:09:51 [INFO] ================================================================================
19:09:51 [INFO] Episode 8: reward=104.19, p_loss=0.2904, v_loss=23.3716, entropy=-18.5929, mem_div=-2.4905, wm_unc=4.5743, lr=1.00e-06
Training Episodes:   0%|          | 9/2000 [1:09:27<256:14:37, 463.32s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
19:09:58 [INFO] [Step 0] First memory divergence measurement: 51.1523, setting reward=0
19:10:45 [INFO] Memory Gate WM: 0.000113
19:12:51 [INFO] [Episode 9] Completed - Detailed Statistics:
19:12:51 [INFO]   Total steps: 50
19:12:51 [INFO]   Episode reward: 97.4169
19:12:51 [INFO]   Avg memory div reward: -2.498364
19:12:51 [INFO]   Avg memory div abs: 51.122932
19:12:51 [INFO]   Avg WM uncertainty: 4.446702
19:12:51 [INFO]   Min/Max reward: 1.5966 / 4.6386
19:14:04 [INFO] [Episode 9] PPO Epoch 0 - Training Losses:
19:14:04 [INFO]   Policy loss: 0.270805
19:14:04 [INFO]   Value loss: 21.482126
19:14:04 [INFO]   Entropy: -18.592518
19:14:04 [INFO]   Total loss: 11.197793
19:17:08 [INFO] Memory Gate WM: 0.000109
19:17:40 [INFO] ================================================================================
19:17:40 [INFO] Episode 9 - Aggregated Metrics Summary:
19:17:40 [INFO]   Reward: 103.5104 (recent avg: 103.5104, std: 3.3463)
19:17:40 [INFO]   Policy Loss: 0.295665 (recent avg: 0.279843)
19:17:40 [INFO]   Value Loss: 23.181699
19:17:40 [INFO]   Entropy: -18.592885
19:17:40 [INFO]   Memory Div Reward: -2.491299 (recent avg: -2.491299)
19:17:40 [INFO]   WM Uncertainty: 4.561507
19:17:40 [INFO]   Learning Rate: 1.00e-06
19:17:40 [INFO]   Total Steps: 500
19:17:40 [INFO]   Log Std: mean=-1.9999, std=0.0000
19:17:40 [INFO] ================================================================================
19:17:40 [INFO] Episode 9: reward=103.51, p_loss=0.2957, v_loss=23.1817, entropy=-18.5929, mem_div=-2.4913, wm_unc=4.5615, lr=1.00e-06
19:17:41 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000010.mp4
Training Episodes:   0%|          | 10/2000 [1:17:17<257:17:27, 465.45s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
19:17:46 [INFO] [Step 0] First memory divergence measurement: 50.7426, setting reward=0
19:18:45 [INFO] Memory Gate Act: -0.000095
19:19:33 [INFO] Memory Gate WM: 0.000111
19:20:35 [INFO] [Episode 10] Completed - Detailed Statistics:
19:20:35 [INFO]   Total steps: 50
19:20:35 [INFO]   Episode reward: 106.9799
19:20:35 [INFO]   Avg memory div reward: -2.508198
19:20:35 [INFO]   Avg memory div abs: 51.006466
19:20:35 [INFO]   Avg WM uncertainty: 4.647797
19:20:35 [INFO]   Min/Max reward: 1.7057 / 4.7819
19:21:46 [INFO] [Episode 10] PPO Epoch 0 - Training Losses:
19:21:46 [INFO]   Policy loss: 0.368305
19:21:46 [INFO]   Value loss: 23.947841
19:21:46 [INFO]   Entropy: -18.592278
19:21:46 [INFO]   Total loss: 12.528149
19:22:19 [INFO] Memory Gate WM: 0.000105
19:25:24 [INFO] ================================================================================
19:25:24 [INFO] Episode 10 - Aggregated Metrics Summary:
19:25:24 [INFO]   Reward: 103.8258 (recent avg: 103.7889, std: 3.5038)
19:25:24 [INFO]   Policy Loss: 0.299258 (recent avg: 0.315390)
19:25:24 [INFO]   Value Loss: 23.250738
19:25:24 [INFO]   Entropy: -18.592827
19:25:24 [INFO]   Memory Div Reward: -2.492835 (recent avg: -2.492873)
19:25:24 [INFO]   WM Uncertainty: 4.569352
19:25:24 [INFO]   Learning Rate: 1.00e-06
19:25:24 [INFO]   Total Steps: 550
19:25:24 [INFO]   Log Std: mean=-1.9999, std=0.0000
19:25:24 [INFO] ================================================================================
19:25:24 [INFO] Episode 10: reward=103.83, p_loss=0.2993, v_loss=23.2507, entropy=-18.5928, mem_div=-2.4928, wm_unc=4.5694, lr=1.00e-06
Training Episodes:   1%|          | 11/2000 [1:24:59<256:37:54, 464.49s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
19:25:30 [INFO] [Step 0] First memory divergence measurement: 50.9636, setting reward=0
19:27:41 [INFO] Memory Gate Act: -0.000089
19:28:21 [INFO] Memory Gate Act: -0.000089
19:28:23 [INFO] [Episode 11] Completed - Detailed Statistics:
19:28:23 [INFO]   Total steps: 50
19:28:23 [INFO]   Episode reward: 105.6489
19:28:23 [INFO]   Avg memory div reward: -2.477712
19:28:23 [INFO]   Avg memory div abs: 50.768446
19:28:23 [INFO]   Avg WM uncertainty: 4.590689
19:28:23 [INFO]   Min/Max reward: 1.6943 / 4.7457
19:29:34 [INFO] [Episode 11] PPO Epoch 0 - Training Losses:
19:29:34 [INFO]   Policy loss: 0.264204
19:29:34 [INFO]   Value loss: 23.661702
19:29:34 [INFO]   Entropy: -18.592184
19:29:34 [INFO]   Total loss: 12.280976
19:33:11 [INFO] ================================================================================
19:33:11 [INFO] Episode 11 - Aggregated Metrics Summary:
19:33:11 [INFO]   Reward: 103.9778 (recent avg: 103.9669, std: 3.5483)
19:33:11 [INFO]   Policy Loss: 0.295851 (recent avg: 0.320360)
19:33:11 [INFO]   Value Loss: 23.284335
19:33:11 [INFO]   Entropy: -18.592768
19:33:11 [INFO]   Memory Div Reward: -2.491575 (recent avg: -2.492085)
19:33:11 [INFO]   WM Uncertainty: 4.571130
19:33:11 [INFO]   Learning Rate: 1.00e-06
19:33:11 [INFO]   Total Steps: 600
19:33:11 [INFO]   Log Std: mean=-1.9999, std=0.0000
19:33:11 [INFO] ================================================================================
19:33:11 [INFO] Episode 11: reward=103.98, p_loss=0.2959, v_loss=23.2843, entropy=-18.5928, mem_div=-2.4916, wm_unc=4.5711, lr=1.00e-06
Training Episodes:   1%|          | 12/2000 [1:32:47<257:00:13, 465.40s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
19:33:15 [INFO] [Step 0] First memory divergence measurement: 51.0364, setting reward=0
19:36:07 [INFO] [Episode 12] Completed - Detailed Statistics:
19:36:07 [INFO]   Total steps: 50
19:36:07 [INFO]   Episode reward: 104.5620
19:36:07 [INFO]   Avg memory div reward: -2.490962
19:36:07 [INFO]   Avg memory div abs: 50.895868
19:36:07 [INFO]   Avg WM uncertainty: 4.582201
19:36:07 [INFO]   Min/Max reward: 1.6731 / 4.5385
19:37:17 [INFO] [Episode 12] PPO Epoch 0 - Training Losses:
19:37:17 [INFO]   Policy loss: 0.250743
19:37:17 [INFO]   Value loss: 23.377331
19:37:17 [INFO]   Entropy: -18.592050
19:37:17 [INFO]   Total loss: 12.125329
19:39:31 [INFO] Memory Gate Act: -0.000084
19:40:50 [INFO] ================================================================================
19:40:50 [INFO] Episode 12 - Aggregated Metrics Summary:
19:40:50 [INFO]   Reward: 104.0227 (recent avg: 104.3347, std: 3.3971)
19:40:50 [INFO]   Policy Loss: 0.292682 (recent avg: 0.278077)
19:40:50 [INFO]   Value Loss: 23.290922
19:40:50 [INFO]   Entropy: -18.592710
19:40:50 [INFO]   Memory Div Reward: -2.491527 (recent avg: -2.492832)
19:40:50 [INFO]   WM Uncertainty: 4.571982
19:40:50 [INFO]   Learning Rate: 1.00e-06
19:40:50 [INFO]   Total Steps: 650
19:40:50 [INFO]   Log Std: mean=-1.9999, std=0.0000
19:40:50 [INFO] ================================================================================
19:40:50 [INFO] Episode 12: reward=104.02, p_loss=0.2927, v_loss=23.2909, entropy=-18.5927, mem_div=-2.4915, wm_unc=4.5720, lr=1.00e-06
Training Episodes:   1%|          | 13/2000 [1:40:25<255:42:02, 463.27s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
19:40:56 [INFO] [Step 0] First memory divergence measurement: 50.7530, setting reward=0
19:43:11 [INFO] Memory Gate Act: -0.000095
19:43:30 [INFO] Memory Gate WM: 0.000124
19:43:47 [INFO] [Episode 13] Completed - Detailed Statistics:
19:43:47 [INFO]   Total steps: 50
19:43:47 [INFO]   Episode reward: 100.4198
19:43:47 [INFO]   Avg memory div reward: -2.486353
19:43:47 [INFO]   Avg memory div abs: 50.715292
19:43:47 [INFO]   Avg WM uncertainty: 4.494748
19:43:47 [INFO]   Min/Max reward: 1.6224 / 4.4975
19:44:59 [INFO] [Episode 13] PPO Epoch 0 - Training Losses:
19:44:59 [INFO]   Policy loss: 0.330089
19:44:59 [INFO]   Value loss: 22.290732
19:44:59 [INFO]   Entropy: -18.591958
19:44:59 [INFO]   Total loss: 11.661375
19:45:12 [INFO] Memory Gate Act: -0.000099
19:45:14 [INFO] Memory Gate Act: -0.000099
19:46:05 [INFO] Memory Gate Act: -0.000104
19:46:28 [INFO] Memory Gate WM: 0.000135
19:48:33 [INFO] ================================================================================
19:48:33 [INFO] Episode 13 - Aggregated Metrics Summary:
19:48:33 [INFO]   Reward: 103.7654 (recent avg: 103.6526, std: 3.4298)
19:48:33 [INFO]   Policy Loss: 0.292757 (recent avg: 0.264360)
19:48:33 [INFO]   Value Loss: 23.219007
19:48:33 [INFO]   Entropy: -18.592656
19:48:33 [INFO]   Memory Div Reward: -2.491158 (recent avg: -2.492532)
19:48:33 [INFO]   WM Uncertainty: 4.566465
19:48:33 [INFO]   Learning Rate: 1.00e-06
19:48:33 [INFO]   Total Steps: 700
19:48:33 [INFO]   Log Std: mean=-1.9999, std=0.0000
19:48:33 [INFO] ================================================================================
19:48:33 [INFO] Episode 13: reward=103.77, p_loss=0.2928, v_loss=23.2190, entropy=-18.5927, mem_div=-2.4912, wm_unc=4.5665, lr=1.00e-06
Training Episodes:   1%|          | 14/2000 [1:48:09<255:35:10, 463.30s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
19:48:39 [INFO] [Step 0] First memory divergence measurement: 50.7320, setting reward=0
19:48:57 [INFO] Memory Gate Act: -0.000102
19:49:47 [INFO] Memory Gate Act: -0.000102
19:51:31 [INFO] [Episode 14] Completed - Detailed Statistics:
19:51:31 [INFO]   Total steps: 50
19:51:31 [INFO]   Episode reward: 96.9016
19:51:31 [INFO]   Avg memory div reward: -2.497274
19:51:31 [INFO]   Avg memory div abs: 50.908466
19:51:31 [INFO]   Avg WM uncertainty: 4.435307
19:51:31 [INFO]   Min/Max reward: 1.5589 / 4.4967
19:51:52 [INFO] Memory Gate Act: -0.000101
19:52:42 [INFO] [Episode 14] PPO Epoch 0 - Training Losses:
19:52:42 [INFO]   Policy loss: 0.275605
19:52:42 [INFO]   Value loss: 21.664020
19:52:42 [INFO]   Entropy: -18.591936
19:52:42 [INFO]   Total loss: 11.293534
19:54:54 [INFO] Memory Gate WM: 0.000171
19:55:02 [INFO] Memory Gate WM: 0.000173
19:56:14 [INFO] ================================================================================
19:56:14 [INFO] Episode 14 - Aggregated Metrics Summary:
19:56:14 [INFO]   Reward: 103.3078 (recent avg: 102.6288, std: 3.7492)
19:56:14 [INFO]   Policy Loss: 0.291844 (recent avg: 0.279409)
19:56:14 [INFO]   Value Loss: 23.114900
19:56:14 [INFO]   Entropy: -18.592607
19:56:14 [INFO]   Memory Div Reward: -2.491566 (recent avg: -2.493427)
19:56:14 [INFO]   WM Uncertainty: 4.557721
19:56:14 [INFO]   Learning Rate: 1.00e-06
19:56:14 [INFO]   Total Steps: 750
19:56:14 [INFO]   Log Std: mean=-1.9999, std=0.0000
19:56:14 [INFO] ================================================================================
19:56:14 [INFO] Episode 14: reward=103.31, p_loss=0.2918, v_loss=23.1149, entropy=-18.5926, mem_div=-2.4916, wm_unc=4.5577, lr=1.00e-06
Training Episodes:   1%|          | 15/2000 [1:55:50<255:06:36, 462.67s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
19:56:21 [INFO] [Step 0] First memory divergence measurement: 50.8939, setting reward=0
19:58:42 [INFO] Memory Gate Act: -0.000089
19:58:52 [INFO] Memory Gate Act: -0.000089
19:59:10 [INFO] [Episode 15] Completed - Detailed Statistics:
19:59:10 [INFO]   Total steps: 50
19:59:10 [INFO]   Episode reward: 105.7574
19:59:10 [INFO]   Avg memory div reward: -2.494980
19:59:10 [INFO]   Avg memory div abs: 50.913564
19:59:10 [INFO]   Avg WM uncertainty: 4.610127
19:59:10 [INFO]   Min/Max reward: 1.8104 / 4.5631
20:00:20 [INFO] [Episode 15] PPO Epoch 0 - Training Losses:
20:00:20 [INFO]   Policy loss: 0.249303
20:00:20 [INFO]   Value loss: 23.533187
20:00:20 [INFO]   Entropy: -18.591889
20:00:20 [INFO]   Total loss: 12.201815
20:00:51 [INFO] Memory Gate WM: 0.000212
20:01:08 [INFO] Memory Gate WM: 0.000218
20:03:51 [INFO] ================================================================================
20:03:51 [INFO] Episode 15 - Aggregated Metrics Summary:
20:03:51 [INFO]   Reward: 103.4609 (recent avg: 102.6327, std: 3.7525)
20:03:51 [INFO]   Policy Loss: 0.295506 (recent avg: 0.308415)
20:03:51 [INFO]   Value Loss: 23.140625
20:03:51 [INFO]   Entropy: -18.592555
20:03:51 [INFO]   Memory Div Reward: -2.491779 (recent avg: -2.494242)
20:03:51 [INFO]   WM Uncertainty: 4.560996
20:03:51 [INFO]   Learning Rate: 1.00e-06
20:03:51 [INFO]   Total Steps: 800
20:03:51 [INFO]   Log Std: mean=-1.9999, std=0.0001
20:03:51 [INFO] ================================================================================
20:03:51 [INFO] Episode 15: reward=103.46, p_loss=0.2955, v_loss=23.1406, entropy=-18.5926, mem_div=-2.4918, wm_unc=4.5610, lr=1.00e-06
Training Episodes:   1%|          | 16/2000 [2:03:27<254:00:06, 460.89s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
20:03:57 [INFO] [Step 0] First memory divergence measurement: 51.0426, setting reward=0
20:06:48 [INFO] [Episode 16] Completed - Detailed Statistics:
20:06:48 [INFO]   Total steps: 50
20:06:48 [INFO]   Episode reward: 97.5037
20:06:48 [INFO]   Avg memory div reward: -2.500052
20:06:48 [INFO]   Avg memory div abs: 51.030421
20:06:48 [INFO]   Avg WM uncertainty: 4.450127
20:06:48 [INFO]   Min/Max reward: 1.6277 / 4.4916
20:07:58 [INFO] [Episode 16] PPO Epoch 0 - Training Losses:
20:07:58 [INFO]   Policy loss: 0.276714
20:07:58 [INFO]   Value loss: 21.740230
20:07:58 [INFO]   Entropy: -18.591657
20:07:58 [INFO]   Total loss: 11.332746
20:08:13 [INFO] Memory Gate Act: -0.000145
20:11:18 [INFO] Memory Gate WM: 0.000332
20:11:29 [INFO] ================================================================================
20:11:29 [INFO] Episode 16 - Aggregated Metrics Summary:
20:11:29 [INFO]   Reward: 103.1105 (recent avg: 102.2721, std: 4.0435)
20:11:29 [INFO]   Policy Loss: 0.293557 (recent avg: 0.303234)
20:11:29 [INFO]   Value Loss: 23.057768
20:11:29 [INFO]   Entropy: -18.592501
20:11:29 [INFO]   Memory Div Reward: -2.492266 (recent avg: -2.493979)
20:11:29 [INFO]   WM Uncertainty: 4.554475
20:11:29 [INFO]   Learning Rate: 1.00e-06
20:11:29 [INFO]   Total Steps: 850
20:11:29 [INFO]   Log Std: mean=-1.9999, std=0.0001
20:11:29 [INFO] ================================================================================
20:11:29 [INFO] Episode 16: reward=103.11, p_loss=0.2936, v_loss=23.0578, entropy=-18.5925, mem_div=-2.4923, wm_unc=4.5545, lr=1.00e-06
Training Episodes:   1%|          | 17/2000 [2:11:05<253:24:13, 460.04s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
20:11:34 [INFO] [Step 0] First memory divergence measurement: 50.9432, setting reward=0
20:14:19 [INFO] [Episode 17] Completed - Detailed Statistics:
20:14:19 [INFO]   Total steps: 50
20:14:19 [INFO]   Episode reward: 102.3946
20:14:19 [INFO]   Avg memory div reward: -2.506563
20:14:19 [INFO]   Avg memory div abs: 51.084665
20:14:19 [INFO]   Avg WM uncertainty: 4.554455
20:14:19 [INFO]   Min/Max reward: 1.5665 / 4.4794
20:14:51 [INFO] Memory Gate Act: -0.000147
20:15:29 [INFO] [Episode 17] PPO Epoch 0 - Training Losses:
20:15:29 [INFO]   Policy loss: 0.604704
20:15:29 [INFO]   Value loss: 22.963210
20:15:29 [INFO]   Entropy: -18.591596
20:15:29 [INFO]   Total loss: 12.272224
20:19:03 [INFO] ================================================================================
20:19:03 [INFO] Episode 17 - Aggregated Metrics Summary:
20:19:03 [INFO]   Reward: 103.0707 (recent avg: 101.7510, std: 3.6380)
20:19:03 [INFO]   Policy Loss: 0.314327 (recent avg: 0.448168)
20:19:03 [INFO]   Value Loss: 23.051986
20:19:03 [INFO]   Entropy: -18.592445
20:19:03 [INFO]   Memory Div Reward: -2.493060 (recent avg: -2.495261)
20:19:03 [INFO]   WM Uncertainty: 4.554474
20:19:03 [INFO]   Learning Rate: 1.00e-06
20:19:03 [INFO]   Total Steps: 900
20:19:03 [INFO]   Log Std: mean=-1.9999, std=0.0001
20:19:03 [INFO] ================================================================================
20:19:03 [INFO] Episode 17: reward=103.07, p_loss=0.3143, v_loss=23.0520, entropy=-18.5924, mem_div=-2.4931, wm_unc=4.5545, lr=1.00e-06
Training Episodes:   1%|          | 18/2000 [2:18:38<252:14:07, 458.15s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
20:19:08 [INFO] [Step 0] First memory divergence measurement: 51.1303, setting reward=0
20:20:47 [INFO] Memory Gate WM: 0.000330
20:21:48 [INFO] Memory Gate WM: 0.000330
20:22:02 [INFO] [Episode 18] Completed - Detailed Statistics:
20:22:02 [INFO]   Total steps: 50
20:22:02 [INFO]   Episode reward: 104.4940
20:22:02 [INFO]   Avg memory div reward: -2.501254
20:22:02 [INFO]   Avg memory div abs: 51.064042
20:22:02 [INFO]   Avg WM uncertainty: 4.591135
20:22:02 [INFO]   Min/Max reward: 1.7146 / 4.6113
20:23:12 [INFO] [Episode 18] PPO Epoch 0 - Training Losses:
20:23:12 [INFO]   Policy loss: 0.314500
20:23:12 [INFO]   Value loss: 23.190805
20:23:12 [INFO]   Entropy: -18.591327
20:23:12 [INFO]   Total loss: 12.095816
20:23:36 [INFO] Memory Gate Act: -0.000149
20:23:58 [INFO] Memory Gate Act: -0.000149
20:25:14 [INFO] Memory Gate WM: 0.000340
20:26:10 [INFO] Memory Gate WM: 0.000350
20:26:44 [INFO] ================================================================================
20:26:44 [INFO] Episode 18 - Aggregated Metrics Summary:
20:26:44 [INFO]   Reward: 103.1456 (recent avg: 102.2079, std: 3.6668)
20:26:44 [INFO]   Policy Loss: 0.316977 (recent avg: 0.464452)
20:26:44 [INFO]   Value Loss: 23.058904
20:26:44 [INFO]   Entropy: -18.592383
20:26:44 [INFO]   Memory Div Reward: -2.493491 (recent avg: -2.496171)
20:26:44 [INFO]   WM Uncertainty: 4.556403
20:26:44 [INFO]   Learning Rate: 1.00e-06
20:26:44 [INFO]   Total Steps: 950
20:26:44 [INFO]   Log Std: mean=-1.9999, std=0.0001
20:26:44 [INFO] ================================================================================
20:26:44 [INFO] Episode 18: reward=103.15, p_loss=0.3170, v_loss=23.0589, entropy=-18.5924, mem_div=-2.4935, wm_unc=4.5564, lr=1.00e-06
Training Episodes:   1%|          | 19/2000 [2:26:20<252:38:03, 459.10s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
20:26:51 [INFO] [Step 0] First memory divergence measurement: 51.2851, setting reward=0
20:29:40 [INFO] [Episode 19] Completed - Detailed Statistics:
20:29:40 [INFO]   Total steps: 50
20:29:40 [INFO]   Episode reward: 103.7390
20:29:40 [INFO]   Avg memory div reward: -2.505274
20:29:40 [INFO]   Avg memory div abs: 51.193823
20:29:40 [INFO]   Avg WM uncertainty: 4.580054
20:29:40 [INFO]   Min/Max reward: 1.6320 / 4.6827
20:29:41 [INFO] Memory Gate Act: -0.000158
20:30:51 [INFO] [Episode 19] PPO Epoch 0 - Training Losses:
20:30:51 [INFO]   Policy loss: 0.289910
20:30:51 [INFO]   Value loss: 23.216078
20:30:51 [INFO]   Entropy: -18.591213
20:30:51 [INFO]   Total loss: 12.083861
20:32:29 [INFO] Memory Gate Act: -0.000156
20:34:28 [INFO] ================================================================================
20:34:28 [INFO] Episode 19 - Aggregated Metrics Summary:
20:34:28 [INFO]   Reward: 103.1753 (recent avg: 102.8401, std: 3.3143)
20:34:28 [INFO]   Policy Loss: 0.322798 (recent avg: 0.498733)
20:34:28 [INFO]   Value Loss: 23.066425
20:34:28 [INFO]   Entropy: -18.592319
20:34:28 [INFO]   Memory Div Reward: -2.494080 (recent avg: -2.496862)
20:34:28 [INFO]   WM Uncertainty: 4.557586
20:34:28 [INFO]   Learning Rate: 1.00e-06
20:34:28 [INFO]   Total Steps: 1000
20:34:28 [INFO]   Log Std: mean=-1.9999, std=0.0001
20:34:28 [INFO] ================================================================================
20:34:28 [INFO] Episode 19: reward=103.18, p_loss=0.3228, v_loss=23.0664, entropy=-18.5923, mem_div=-2.4941, wm_unc=4.5576, lr=1.00e-06
20:34:30 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000020.mp4
Training Episodes:   1%|          | 20/2000 [2:34:06<253:38:41, 461.17s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
20:34:36 [INFO] [Step 0] First memory divergence measurement: 50.9891, setting reward=0
20:37:29 [INFO] [Episode 20] Completed - Detailed Statistics:
20:37:29 [INFO]   Total steps: 50
20:37:29 [INFO]   Episode reward: 104.7403
20:37:29 [INFO]   Avg memory div reward: -2.504292
20:37:29 [INFO]   Avg memory div abs: 51.077002
20:37:29 [INFO]   Avg WM uncertainty: 4.599097
20:37:29 [INFO]   Min/Max reward: 1.8354 / 4.5077
20:37:53 [INFO] Memory Gate Act: -0.000145
20:38:39 [INFO] [Episode 20] PPO Epoch 0 - Training Losses:
20:38:39 [INFO]   Policy loss: 0.266057
20:38:39 [INFO]   Value loss: 23.293864
20:38:39 [INFO]   Entropy: -18.590927
20:38:39 [INFO]   Total loss: 12.098898
20:41:53 [INFO] Memory Gate Act: -0.000137
20:42:12 [INFO] ================================================================================
20:42:12 [INFO] Episode 20 - Aggregated Metrics Summary:
20:42:12 [INFO]   Reward: 103.2498 (recent avg: 102.6161, std: 3.0954)
20:42:12 [INFO]   Policy Loss: 0.326666 (recent avg: 0.396618)
20:42:12 [INFO]   Value Loss: 23.076973
20:42:12 [INFO]   Entropy: -18.592250
20:42:12 [INFO]   Memory Div Reward: -2.494567 (recent avg: -2.496472)
20:42:12 [INFO]   WM Uncertainty: 4.559562
20:42:12 [INFO]   Learning Rate: 1.00e-06
20:42:12 [INFO]   Total Steps: 1050
20:42:12 [INFO]   Log Std: mean=-1.9999, std=0.0001
20:42:12 [INFO] ================================================================================
20:42:12 [INFO] Episode 20: reward=103.25, p_loss=0.3267, v_loss=23.0770, entropy=-18.5923, mem_div=-2.4946, wm_unc=4.5596, lr=1.00e-06
Training Episodes:   1%|          | 21/2000 [2:41:48<253:39:35, 461.43s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
20:42:18 [INFO] [Step 0] First memory divergence measurement: 51.0830, setting reward=0
20:43:21 [INFO] Memory Gate WM: 0.000379
20:43:38 [INFO] Memory Gate Act: -0.000136
20:45:06 [INFO] [Episode 21] Completed - Detailed Statistics:
20:45:06 [INFO]   Total steps: 50
20:45:06 [INFO]   Episode reward: 113.7998
20:45:06 [INFO]   Avg memory div reward: -2.510471
20:45:06 [INFO]   Avg memory div abs: 51.184292
20:45:06 [INFO]   Avg WM uncertainty: 4.786467
20:45:06 [INFO]   Min/Max reward: 1.9352 / 4.8707
20:46:18 [INFO] [Episode 21] PPO Epoch 0 - Training Losses:
20:46:18 [INFO]   Policy loss: 0.244340
20:46:18 [INFO]   Value loss: 25.297402
20:46:18 [INFO]   Entropy: -18.590840
20:46:18 [INFO]   Total loss: 13.078949
20:49:51 [INFO] ================================================================================
20:49:51 [INFO] Episode 21 - Aggregated Metrics Summary:
20:49:51 [INFO]   Reward: 103.7293 (recent avg: 103.4312, std: 4.5282)
20:49:51 [INFO]   Policy Loss: 0.322810 (recent avg: 0.357523)
20:49:51 [INFO]   Value Loss: 23.177610
20:49:51 [INFO]   Entropy: -18.592186
20:49:51 [INFO]   Memory Div Reward: -2.495290 (recent avg: -2.499747)
20:49:51 [INFO]   WM Uncertainty: 4.569876
20:49:51 [INFO]   Learning Rate: 1.00e-06
20:49:51 [INFO]   Total Steps: 1100
20:49:51 [INFO]   Log Std: mean=-1.9999, std=0.0001
20:49:51 [INFO] ================================================================================
20:49:51 [INFO] Episode 21: reward=103.73, p_loss=0.3228, v_loss=23.1776, entropy=-18.5922, mem_div=-2.4953, wm_unc=4.5699, lr=1.00e-06
Training Episodes:   1%|          | 22/2000 [2:49:27<253:09:59, 460.77s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
20:50:00 [INFO] [Step 0] First memory divergence measurement: 51.3105, setting reward=0
20:51:22 [INFO] Memory Gate Act: -0.000146
20:52:49 [INFO] [Episode 22] Completed - Detailed Statistics:
20:52:49 [INFO]   Total steps: 50
20:52:49 [INFO]   Episode reward: 104.0625
20:52:49 [INFO]   Avg memory div reward: -2.510569
20:52:49 [INFO]   Avg memory div abs: 51.260738
20:52:49 [INFO]   Avg WM uncertainty: 4.591819
20:52:49 [INFO]   Min/Max reward: 1.5442 / 4.6076
20:52:58 [INFO] Memory Gate WM: 0.000385
20:53:32 [INFO] Memory Gate Act: -0.000148
20:53:59 [INFO] [Episode 22] PPO Epoch 0 - Training Losses:
20:53:59 [INFO]   Policy loss: 0.248916
20:53:59 [INFO]   Value loss: 22.658345
20:53:59 [INFO]   Entropy: -18.590830
20:53:59 [INFO]   Total loss: 11.763996
20:56:20 [INFO] Memory Gate Act: -0.000146
20:57:32 [INFO] ================================================================================
20:57:32 [INFO] Episode 22 - Aggregated Metrics Summary:
20:57:32 [INFO]   Reward: 103.7438 (recent avg: 103.3813, std: 4.5182)
20:57:32 [INFO]   Policy Loss: 0.319040 (recent avg: 0.294423)
20:57:32 [INFO]   Value Loss: 23.154736
20:57:32 [INFO]   Entropy: -18.592127
20:57:32 [INFO]   Memory Div Reward: -2.495954 (recent avg: -2.501708)
20:57:32 [INFO]   WM Uncertainty: 4.570830
20:57:32 [INFO]   Learning Rate: 1.00e-06
20:57:32 [INFO]   Total Steps: 1150
20:57:32 [INFO]   Log Std: mean=-1.9999, std=0.0001
20:57:32 [INFO] ================================================================================
20:57:32 [INFO] Episode 22: reward=103.74, p_loss=0.3190, v_loss=23.1547, entropy=-18.5921, mem_div=-2.4960, wm_unc=4.5708, lr=1.00e-06
Training Episodes:   1%|          | 23/2000 [2:57:07<252:59:50, 460.69s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
20:57:36 [INFO] [Step 0] First memory divergence measurement: 51.3759, setting reward=0
21:00:26 [INFO] [Episode 23] Completed - Detailed Statistics:
21:00:26 [INFO]   Total steps: 50
21:00:26 [INFO]   Episode reward: 98.1809
21:00:26 [INFO]   Avg memory div reward: -2.515110
21:00:26 [INFO]   Avg memory div abs: 51.373479
21:00:26 [INFO]   Avg WM uncertainty: 4.478729
21:00:26 [INFO]   Min/Max reward: 1.5841 / 4.5839
21:01:37 [INFO] [Episode 23] PPO Epoch 0 - Training Losses:
21:01:37 [INFO]   Policy loss: 0.452285
21:01:37 [INFO]   Value loss: 21.661799
21:01:37 [INFO]   Entropy: -18.590802
21:01:37 [INFO]   Total loss: 11.469093
21:03:54 [INFO] Memory Gate WM: 0.000402
21:05:07 [INFO] ================================================================================
21:05:07 [INFO] Episode 23 - Aggregated Metrics Summary:
21:05:07 [INFO]   Reward: 103.5120 (recent avg: 103.1574, std: 4.7108)
21:05:07 [INFO]   Policy Loss: 0.321077 (recent avg: 0.292150)
21:05:07 [INFO]   Value Loss: 23.092248
21:05:07 [INFO]   Entropy: -18.592069
21:05:07 [INFO]   Memory Div Reward: -2.496752 (recent avg: -2.504584)
21:05:07 [INFO]   WM Uncertainty: 4.566993
21:05:07 [INFO]   Learning Rate: 1.00e-06
21:05:07 [INFO]   Total Steps: 1200
21:05:07 [INFO]   Log Std: mean=-1.9999, std=0.0001
21:05:07 [INFO] ================================================================================
21:05:07 [INFO] Episode 23: reward=103.51, p_loss=0.3211, v_loss=23.0922, entropy=-18.5921, mem_div=-2.4968, wm_unc=4.5670, lr=1.00e-06
Training Episodes:   1%|          | 24/2000 [3:04:42<251:56:26, 459.00s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
21:05:12 [INFO] [Step 0] First memory divergence measurement: 51.3655, setting reward=0
21:07:39 [INFO] Memory Gate Act: -0.000151
21:07:46 [INFO] Memory Gate WM: 0.000406
21:07:59 [INFO] [Episode 24] Completed - Detailed Statistics:
21:07:59 [INFO]   Total steps: 50
21:07:59 [INFO]   Episode reward: 106.9083
21:07:59 [INFO]   Avg memory div reward: -2.514374
21:07:59 [INFO]   Avg memory div abs: 51.307156
21:07:59 [INFO]   Avg WM uncertainty: 4.652540
21:07:59 [INFO]   Min/Max reward: 1.8421 / 4.7021
21:08:19 [INFO] Memory Gate Act: -0.000151
21:09:11 [INFO] [Episode 24] PPO Epoch 0 - Training Losses:
21:09:11 [INFO]   Policy loss: 0.303541
21:09:11 [INFO]   Value loss: 23.735978
21:09:11 [INFO]   Entropy: -18.590683
21:09:11 [INFO]   Total loss: 12.357437
21:10:36 [INFO] Memory Gate WM: 0.000423
21:10:44 [INFO] Memory Gate WM: 0.000424
21:12:43 [INFO] ================================================================================
21:12:43 [INFO] Episode 24 - Aggregated Metrics Summary:
21:12:43 [INFO]   Reward: 103.6479 (recent avg: 104.1580, std: 4.3225)
21:12:43 [INFO]   Policy Loss: 0.323338 (recent avg: 0.344556)
21:12:43 [INFO]   Value Loss: 23.117736
21:12:43 [INFO]   Entropy: -18.592013
21:12:43 [INFO]   Memory Div Reward: -2.497457 (recent avg: -2.506294)
21:12:43 [INFO]   WM Uncertainty: 4.570415
21:12:43 [INFO]   Learning Rate: 1.00e-06
21:12:43 [INFO]   Total Steps: 1250
21:12:43 [INFO]   Log Std: mean=-1.9999, std=0.0001
21:12:43 [INFO] ================================================================================
21:12:43 [INFO] Episode 24: reward=103.65, p_loss=0.3233, v_loss=23.1177, entropy=-18.5920, mem_div=-2.4975, wm_unc=4.5704, lr=1.00e-06
Training Episodes:   1%|         | 25/2000 [3:12:19<251:23:59, 458.25s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
21:12:49 [INFO] [Step 0] First memory divergence measurement: 51.3155, setting reward=0
21:14:30 [INFO] Memory Gate Act: -0.000157
21:15:41 [INFO] [Episode 25] Completed - Detailed Statistics:
21:15:41 [INFO]   Total steps: 50
21:15:41 [INFO]   Episode reward: 104.5992
21:15:41 [INFO]   Avg memory div reward: -2.512337
21:15:41 [INFO]   Avg memory div abs: 51.288088
21:15:41 [INFO]   Avg WM uncertainty: 4.604321
21:15:41 [INFO]   Min/Max reward: 1.7475 / 4.7102
21:15:50 [INFO] Memory Gate WM: 0.000438
21:16:51 [INFO] [Episode 25] PPO Epoch 0 - Training Losses:
21:16:51 [INFO]   Policy loss: 0.345457
21:16:51 [INFO]   Value loss: 23.067980
21:16:51 [INFO]   Entropy: -18.590662
21:16:51 [INFO]   Total loss: 12.065354
21:16:58 [INFO] Memory Gate Act: -0.000157
21:20:25 [INFO] ================================================================================
21:20:25 [INFO] Episode 25 - Aggregated Metrics Summary:
21:20:25 [INFO]   Reward: 103.6845 (recent avg: 104.0422, std: 4.2935)
21:20:25 [INFO]   Policy Loss: 0.325366 (recent avg: 0.332491)
21:20:25 [INFO]   Value Loss: 23.106680
21:20:25 [INFO]   Entropy: -18.591890
21:20:25 [INFO]   Memory Div Reward: -2.498029 (recent avg: -2.508030)
21:20:25 [INFO]   WM Uncertainty: 4.571719
21:20:25 [INFO]   Learning Rate: 1.00e-06
21:20:25 [INFO]   Total Steps: 1300
21:20:25 [INFO]   Log Std: mean=-1.9999, std=0.0001
21:20:25 [INFO] ================================================================================
21:20:25 [INFO] Episode 25: reward=103.68, p_loss=0.3254, v_loss=23.1067, entropy=-18.5919, mem_div=-2.4980, wm_unc=4.5717, lr=1.00e-06
Training Episodes:   1%|         | 26/2000 [3:20:00<251:46:04, 459.15s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
21:20:31 [INFO] [Step 0] First memory divergence measurement: 51.4632, setting reward=0
21:20:32 [INFO] Memory Gate Act: -0.000161
21:21:12 [INFO] Memory Gate WM: 0.000439
21:22:55 [INFO] [Episode 26] Completed - Detailed Statistics:
21:22:55 [INFO]   Total steps: 50
21:22:55 [INFO]   Episode reward: 104.8248
21:22:55 [INFO]   Avg memory div reward: -2.513558
21:22:55 [INFO]   Avg memory div abs: 51.391244
21:22:55 [INFO]   Avg WM uncertainty: 4.610053
21:22:55 [INFO]   Min/Max reward: 1.7600 / 4.7932
21:24:12 [INFO] [Episode 26] PPO Epoch 0 - Training Losses:
21:24:12 [INFO]   Policy loss: 0.312457
21:24:12 [INFO]   Value loss: 23.190536
21:24:12 [INFO]   Entropy: -18.590630
21:24:12 [INFO]   Total loss: 12.093631
21:26:37 [INFO] Memory Gate Act: -0.000188
21:27:14 [INFO] Memory Gate Act: -0.000198
21:27:58 [INFO] ================================================================================
21:27:58 [INFO] Episode 26 - Aggregated Metrics Summary:
21:27:58 [INFO]   Reward: 103.7267 (recent avg: 104.7743, std: 3.6992)
21:27:58 [INFO]   Policy Loss: 0.320718 (recent avg: 0.316789)
21:27:58 [INFO]   Value Loss: 23.106835
21:27:58 [INFO]   Entropy: -18.591784
21:27:58 [INFO]   Memory Div Reward: -2.498604 (recent avg: -2.509380)
21:27:58 [INFO]   WM Uncertainty: 4.573139
21:27:58 [INFO]   Learning Rate: 1.00e-06
21:27:58 [INFO]   Total Steps: 1350
21:27:58 [INFO]   Log Std: mean=-1.9999, std=0.0001
21:27:58 [INFO] ================================================================================
21:27:58 [INFO] Episode 26: reward=103.73, p_loss=0.3207, v_loss=23.1068, entropy=-18.5918, mem_div=-2.4986, wm_unc=4.5731, lr=1.00e-06
Training Episodes:   1%|         | 27/2000 [3:27:33<250:37:06, 457.29s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
21:28:06 [INFO] [Step 0] First memory divergence measurement: 51.5430, setting reward=0
21:29:14 [INFO] Memory Gate WM: 0.000486
21:30:29 [INFO] Memory Gate WM: 0.000486
21:30:39 [INFO] [Episode 27] Completed - Detailed Statistics:
21:30:39 [INFO]   Total steps: 50
21:30:39 [INFO]   Episode reward: 103.7721
21:30:39 [INFO]   Avg memory div reward: -2.516771
21:30:39 [INFO]   Avg memory div abs: 51.464496
21:30:39 [INFO]   Avg WM uncertainty: 4.592214
21:30:39 [INFO]   Min/Max reward: 1.4563 / 4.5501
21:31:19 [INFO] Memory Gate Act: -0.000206
21:31:54 [INFO] [Episode 27] PPO Epoch 0 - Training Losses:
21:31:54 [INFO]   Policy loss: 0.295069
21:31:54 [INFO]   Value loss: 22.964691
21:31:54 [INFO]   Entropy: -18.590569
21:31:54 [INFO]   Total loss: 11.963321
21:34:45 [INFO] Memory Gate Act: -0.000206
21:35:41 [INFO] ================================================================================
21:35:41 [INFO] Episode 27 - Aggregated Metrics Summary:
21:35:41 [INFO]   Reward: 103.7283 (recent avg: 104.9121, std: 3.6331)
21:35:41 [INFO]   Policy Loss: 0.320346 (recent avg: 0.286386)
21:35:41 [INFO]   Value Loss: 23.113604
21:35:41 [INFO]   Entropy: -18.591685
21:35:41 [INFO]   Memory Div Reward: -2.499253 (recent avg: -2.510401)
21:35:41 [INFO]   WM Uncertainty: 4.573820
21:35:41 [INFO]   Learning Rate: 1.00e-06
21:35:41 [INFO]   Total Steps: 1400
21:35:41 [INFO]   Log Std: mean=-1.9999, std=0.0001
21:35:41 [INFO] ================================================================================
21:35:41 [INFO] Episode 27: reward=103.73, p_loss=0.3203, v_loss=23.1136, entropy=-18.5917, mem_div=-2.4993, wm_unc=4.5738, lr=1.00e-06
Training Episodes:   1%|         | 28/2000 [3:35:17<251:32:17, 459.20s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
21:35:48 [INFO] [Step 0] First memory divergence measurement: 51.5938, setting reward=0
21:36:34 [INFO] Memory Gate WM: 0.000529
21:38:11 [INFO] [Episode 28] Completed - Detailed Statistics:
21:38:11 [INFO]   Total steps: 50
21:38:11 [INFO]   Episode reward: 98.5703
21:38:11 [INFO]   Avg memory div reward: -2.533267
21:38:11 [INFO]   Avg memory div abs: 51.674166
21:38:11 [INFO]   Avg WM uncertainty: 4.504672
21:38:11 [INFO]   Min/Max reward: 1.6652 / 4.7554
21:38:27 [INFO] Memory Gate WM: 0.000531
21:39:28 [INFO] [Episode 28] PPO Epoch 0 - Training Losses:
21:39:28 [INFO]   Policy loss: 0.723723
21:39:28 [INFO]   Value loss: 21.735974
21:39:28 [INFO]   Entropy: -18.590484
21:39:28 [INFO]   Total loss: 11.777615
21:40:06 [INFO] Memory Gate Act: -0.000213
21:40:55 [INFO] Memory Gate WM: 0.000534
21:41:40 [INFO] Memory Gate WM: 0.000538
21:43:12 [INFO] ================================================================================
21:43:12 [INFO] Episode 28 - Aggregated Metrics Summary:
21:43:12 [INFO]   Reward: 103.5505 (recent avg: 104.3197, std: 4.1052)
21:43:12 [INFO]   Policy Loss: 0.324355 (recent avg: 0.304957)
21:43:12 [INFO]   Value Loss: 23.027759
21:43:12 [INFO]   Entropy: -18.591582
21:43:12 [INFO]   Memory Div Reward: -2.500426 (recent avg: -2.513602)
21:43:12 [INFO]   WM Uncertainty: 4.571435
21:43:12 [INFO]   Learning Rate: 1.00e-06
21:43:12 [INFO]   Total Steps: 1450
21:43:12 [INFO]   Log Std: mean=-1.9999, std=0.0001
21:43:12 [INFO] ================================================================================
21:43:12 [INFO] Episode 28: reward=103.55, p_loss=0.3244, v_loss=23.0278, entropy=-18.5916, mem_div=-2.5004, wm_unc=4.5714, lr=1.00e-06
Training Episodes:   1%|         | 29/2000 [3:42:48<250:05:15, 456.78s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
21:43:17 [INFO] [Step 0] First memory divergence measurement: 51.8227, setting reward=0
21:43:18 [INFO] Memory Gate WM: 0.000544
21:44:13 [INFO] Memory Gate WM: 0.000544
21:44:45 [INFO] Memory Gate Act: -0.000229
21:45:49 [INFO] [Episode 29] Completed - Detailed Statistics:
21:45:49 [INFO]   Total steps: 50
21:45:49 [INFO]   Episode reward: 95.2990
21:45:49 [INFO]   Avg memory div reward: -2.541264
21:45:49 [INFO]   Avg memory div abs: 51.848976
21:45:49 [INFO]   Avg WM uncertainty: 4.447244
21:45:49 [INFO]   Min/Max reward: 1.6004 / 4.3462
21:47:05 [INFO] [Episode 29] PPO Epoch 0 - Training Losses:
21:47:05 [INFO]   Policy loss: 0.359256
21:47:05 [INFO]   Value loss: 20.986951
21:47:05 [INFO]   Entropy: -18.590330
21:47:05 [INFO]   Total loss: 11.038636
21:49:50 [INFO] Memory Gate WM: 0.000546
21:50:19 [INFO] Memory Gate WM: 0.000550
21:50:41 [INFO] ================================================================================
21:50:41 [INFO] Episode 29 - Aggregated Metrics Summary:
21:50:41 [INFO]   Reward: 103.2754 (recent avg: 103.4757, std: 4.9238)
21:50:41 [INFO]   Policy Loss: 0.326854 (recent avg: 0.326677)
21:50:41 [INFO]   Value Loss: 22.899211
21:50:41 [INFO]   Entropy: -18.591479
21:50:41 [INFO]   Memory Div Reward: -2.501787 (recent avg: -2.517201)
21:50:41 [INFO]   WM Uncertainty: 4.567296
21:50:41 [INFO]   Learning Rate: 1.00e-06
21:50:41 [INFO]   Total Steps: 1500
21:50:41 [INFO]   Log Std: mean=-1.9999, std=0.0001
21:50:41 [INFO] ================================================================================
21:50:41 [INFO] Episode 29: reward=103.28, p_loss=0.3269, v_loss=22.8992, entropy=-18.5915, mem_div=-2.5018, wm_unc=4.5673, lr=1.00e-06
21:50:43 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000030.mp4
Training Episodes:   2%|         | 30/2000 [3:50:19<249:00:09, 455.03s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
21:50:50 [INFO] [Step 0] First memory divergence measurement: 51.8406, setting reward=0
21:53:14 [INFO] [Episode 30] Completed - Detailed Statistics:
21:53:14 [INFO]   Total steps: 50
21:53:14 [INFO]   Episode reward: 93.6485
21:53:14 [INFO]   Avg memory div reward: -2.545068
21:53:14 [INFO]   Avg memory div abs: 51.908154
21:53:14 [INFO]   Avg WM uncertainty: 4.418037
21:53:14 [INFO]   Min/Max reward: 1.5054 / 4.3857
21:54:27 [INFO] [Episode 30] PPO Epoch 0 - Training Losses:
21:54:27 [INFO]   Policy loss: 0.243991
21:54:27 [INFO]   Value loss: 20.832712
21:54:27 [INFO]   Entropy: -18.590245
21:54:27 [INFO]   Total loss: 10.846249
21:54:27 [INFO] Memory Gate Act: -0.000237
21:57:49 [INFO] Memory Gate Act: -0.000238
21:58:06 [INFO] ================================================================================
21:58:06 [INFO] Episode 30 - Aggregated Metrics Summary:
21:58:06 [INFO]   Reward: 102.9649 (recent avg: 102.3665, std: 5.7019)
21:58:06 [INFO]   Policy Loss: 0.320154 (recent avg: 0.276084)
21:58:06 [INFO]   Value Loss: 22.784762
21:58:06 [INFO]   Entropy: -18.591379
21:58:06 [INFO]   Memory Div Reward: -2.503184 (recent avg: -2.521279)
21:58:06 [INFO]   WM Uncertainty: 4.562481
21:58:06 [INFO]   Learning Rate: 1.00e-06
21:58:06 [INFO]   Total Steps: 1550
21:58:06 [INFO]   Log Std: mean=-1.9999, std=0.0001
21:58:06 [INFO] ================================================================================
21:58:06 [INFO] Episode 30: reward=102.96, p_loss=0.3202, v_loss=22.7848, entropy=-18.5914, mem_div=-2.5032, wm_unc=4.5625, lr=1.00e-06
Training Episodes:   2%|         | 31/2000 [3:57:41<246:47:49, 451.23s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
21:58:09 [INFO] [Step 0] First memory divergence measurement: 51.7432, setting reward=0
22:00:41 [INFO] [Episode 31] Completed - Detailed Statistics:
22:00:41 [INFO]   Total steps: 50
22:00:41 [INFO]   Episode reward: 106.3800
22:00:41 [INFO]   Avg memory div reward: -2.533758
22:00:41 [INFO]   Avg memory div abs: 51.747781
22:00:41 [INFO]   Avg WM uncertainty: 4.661358
22:00:41 [INFO]   Min/Max reward: 1.8551 / 4.5643
22:01:56 [INFO] [Episode 31] PPO Epoch 0 - Training Losses:
22:01:56 [INFO]   Policy loss: 0.195657
22:01:56 [INFO]   Value loss: 23.720582
22:01:56 [INFO]   Entropy: -18.590198
22:01:56 [INFO]   Total loss: 12.241849
22:03:09 [INFO] Memory Gate WM: 0.000580
22:05:24 [INFO] Memory Gate Act: -0.000263
22:05:35 [INFO] ================================================================================
22:05:35 [INFO] Episode 31 - Aggregated Metrics Summary:
22:05:35 [INFO]   Reward: 103.0716 (recent avg: 101.6246, std: 4.5276)
22:05:35 [INFO]   Policy Loss: 0.318787 (recent avg: 0.264831)
22:05:35 [INFO]   Value Loss: 22.828006
22:05:35 [INFO]   Entropy: -18.591281
22:05:35 [INFO]   Memory Div Reward: -2.504139 (recent avg: -2.523608)
22:05:35 [INFO]   WM Uncertainty: 4.565571
22:05:35 [INFO]   Learning Rate: 1.00e-06
22:05:35 [INFO]   Total Steps: 1600
22:05:35 [INFO]   Log Std: mean=-1.9999, std=0.0001
22:05:35 [INFO] ================================================================================
22:05:35 [INFO] Episode 31: reward=103.07, p_loss=0.3188, v_loss=22.8280, entropy=-18.5913, mem_div=-2.5041, wm_unc=4.5656, lr=1.00e-06
Training Episodes:   2%|         | 32/2000 [4:05:11<246:21:56, 450.67s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
22:05:40 [INFO] [Step 0] First memory divergence measurement: 51.5111, setting reward=0
22:05:44 [INFO] Memory Gate WM: 0.000588
22:08:04 [INFO] [Episode 32] Completed - Detailed Statistics:
22:08:04 [INFO]   Total steps: 50
22:08:04 [INFO]   Episode reward: 101.3342
22:08:04 [INFO]   Avg memory div reward: -2.531749
22:08:04 [INFO]   Avg memory div abs: 51.665779
22:08:04 [INFO]   Avg WM uncertainty: 4.558433
22:08:04 [INFO]   Min/Max reward: 1.5591 / 4.6536
22:09:05 [INFO] Memory Gate WM: 0.000599
22:09:16 [INFO] [Episode 32] PPO Epoch 0 - Training Losses:
22:09:16 [INFO]   Policy loss: 0.311013
22:09:16 [INFO]   Value loss: 22.312972
22:09:16 [INFO]   Entropy: -18.590148
22:09:16 [INFO]   Total loss: 11.653400
22:12:49 [INFO] ================================================================================
22:12:49 [INFO] Episode 32 - Aggregated Metrics Summary:
22:12:49 [INFO]   Reward: 103.0189 (recent avg: 101.3517, std: 4.4541)
22:12:49 [INFO]   Policy Loss: 0.320158 (recent avg: 0.252340)
22:12:49 [INFO]   Value Loss: 22.750740
22:12:49 [INFO]   Entropy: -18.591183
22:12:49 [INFO]   Memory Div Reward: -2.504976 (recent avg: -2.525726)
22:12:49 [INFO]   WM Uncertainty: 4.565354
22:12:49 [INFO]   Learning Rate: 1.00e-06
22:12:49 [INFO]   Total Steps: 1650
22:12:49 [INFO]   Log Std: mean=-1.9999, std=0.0001
22:12:49 [INFO] ================================================================================
22:12:49 [INFO] Episode 32: reward=103.02, p_loss=0.3202, v_loss=22.7507, entropy=-18.5912, mem_div=-2.5050, wm_unc=4.5654, lr=1.00e-06
Training Episodes:   2%|         | 33/2000 [4:12:24<243:26:55, 445.56s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
22:12:55 [INFO] [Step 0] First memory divergence measurement: 52.0207, setting reward=0
22:13:33 [INFO] Memory Gate WM: 0.000611
22:15:22 [INFO] Memory Gate Act: -0.000273
22:15:24 [INFO] [Episode 33] Completed - Detailed Statistics:
22:15:24 [INFO]   Total steps: 50
22:15:24 [INFO]   Episode reward: 92.1627
22:15:24 [INFO]   Avg memory div reward: -2.539999
22:15:24 [INFO]   Avg memory div abs: 51.911972
22:15:24 [INFO]   Avg WM uncertainty: 4.383254
22:15:24 [INFO]   Min/Max reward: 1.4352 / 4.4242
22:15:29 [INFO] Memory Gate Act: -0.000272
22:15:56 [INFO] Memory Gate Act: -0.000270
22:16:41 [INFO] [Episode 33] PPO Epoch 0 - Training Losses:
22:16:41 [INFO]   Policy loss: 0.371627
22:16:41 [INFO]   Value loss: 20.344229
22:16:41 [INFO]   Entropy: -18.590115
22:16:41 [INFO]   Total loss: 10.729643
22:17:03 [INFO] Memory Gate WM: 0.000609
22:17:19 [INFO] Memory Gate Act: -0.000275
22:20:31 [INFO] ================================================================================
22:20:31 [INFO] Episode 33 - Aggregated Metrics Summary:
22:20:31 [INFO]   Reward: 102.6996 (recent avg: 100.7499, std: 5.1880)
22:20:31 [INFO]   Policy Loss: 0.320694 (recent avg: 0.258723)
22:20:31 [INFO]   Value Loss: 22.667491
22:20:31 [INFO]   Entropy: -18.591084
22:20:31 [INFO]   Memory Div Reward: -2.506006 (recent avg: -2.528214)
22:20:31 [INFO]   WM Uncertainty: 4.559999
22:20:31 [INFO]   Learning Rate: 1.00e-06
22:20:31 [INFO]   Total Steps: 1700
22:20:31 [INFO]   Log Std: mean=-1.9999, std=0.0001
22:20:31 [INFO] ================================================================================
22:20:31 [INFO] Episode 33: reward=102.70, p_loss=0.3207, v_loss=22.6675, entropy=-18.5911, mem_div=-2.5060, wm_unc=4.5600, lr=1.00e-06
Training Episodes:   2%|         | 34/2000 [4:20:06<246:01:15, 450.50s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
22:20:36 [INFO] [Step 0] First memory divergence measurement: 52.2122, setting reward=0
22:21:45 [INFO] Memory Gate Act: -0.000298
22:23:01 [INFO] [Episode 34] Completed - Detailed Statistics:
22:23:01 [INFO]   Total steps: 50
22:23:01 [INFO]   Episode reward: 94.1825
22:23:01 [INFO]   Avg memory div reward: -2.555865
22:23:01 [INFO]   Avg memory div abs: 52.176788
22:23:01 [INFO]   Avg WM uncertainty: 4.439516
22:23:01 [INFO]   Min/Max reward: 1.5942 / 4.5615
22:24:07 [INFO] Memory Gate Act: -0.000304
22:24:18 [INFO] [Episode 34] PPO Epoch 0 - Training Losses:
22:24:18 [INFO]   Policy loss: 0.184857
22:24:18 [INFO]   Value loss: 20.550393
22:24:18 [INFO]   Entropy: -18.590069
22:24:18 [INFO]   Total loss: 10.645954
22:27:46 [INFO] Memory Gate Act: -0.000332
22:27:57 [INFO] ================================================================================
22:27:57 [INFO] Episode 34 - Aggregated Metrics Summary:
22:27:57 [INFO]   Reward: 102.4563 (recent avg: 99.4773, std: 5.0810)
22:27:57 [INFO]   Policy Loss: 0.316686 (recent avg: 0.245284)
22:27:57 [INFO]   Value Loss: 22.630320
22:27:57 [INFO]   Entropy: -18.590987
22:27:57 [INFO]   Memory Div Reward: -2.507430 (recent avg: -2.532363)
22:27:57 [INFO]   WM Uncertainty: 4.556556
22:27:57 [INFO]   Learning Rate: 1.00e-06
22:27:57 [INFO]   Total Steps: 1750
22:27:57 [INFO]   Log Std: mean=-1.9999, std=0.0001
22:27:57 [INFO] ================================================================================
22:27:57 [INFO] Episode 34: reward=102.46, p_loss=0.3167, v_loss=22.6303, entropy=-18.5910, mem_div=-2.5074, wm_unc=4.5566, lr=1.00e-06
Training Episodes:   2%|         | 35/2000 [4:27:33<245:15:59, 449.34s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
22:28:02 [INFO] [Step 0] First memory divergence measurement: 52.1855, setting reward=0
22:28:30 [INFO] Memory Gate Act: -0.000334
22:30:32 [INFO] [Episode 35] Completed - Detailed Statistics:
22:30:32 [INFO]   Total steps: 50
22:30:32 [INFO]   Episode reward: 108.5856
22:30:32 [INFO]   Avg memory div reward: -2.548320
22:30:32 [INFO]   Avg memory div abs: 52.096570
22:30:32 [INFO]   Avg WM uncertainty: 4.720031
22:30:32 [INFO]   Min/Max reward: 1.8747 / 4.8950
22:30:36 [INFO] Memory Gate WM: 0.000650
22:31:47 [INFO] [Episode 35] PPO Epoch 0 - Training Losses:
22:31:47 [INFO]   Policy loss: 0.560302
22:31:47 [INFO]   Value loss: 24.047006
22:31:47 [INFO]   Entropy: -18.589895
22:31:47 [INFO]   Total loss: 12.769703
22:33:27 [INFO] Memory Gate WM: 0.000667
22:34:12 [INFO] Memory Gate WM: 0.000676
22:35:28 [INFO] ================================================================================
22:35:28 [INFO] Episode 35 - Aggregated Metrics Summary:
22:35:28 [INFO]   Reward: 102.6266 (recent avg: 99.8760, std: 5.5973)
22:35:28 [INFO]   Policy Loss: 0.316496 (recent avg: 0.268509)
22:35:28 [INFO]   Value Loss: 22.634235
22:35:28 [INFO]   Entropy: -18.590889
22:35:28 [INFO]   Memory Div Reward: -2.508566 (recent avg: -2.535962)
22:35:28 [INFO]   WM Uncertainty: 4.561097
22:35:28 [INFO]   Learning Rate: 1.00e-06
22:35:28 [INFO]   Total Steps: 1800
22:35:28 [INFO]   Log Std: mean=-1.9999, std=0.0001
22:35:28 [INFO] ================================================================================
22:35:28 [INFO] Episode 35: reward=102.63, p_loss=0.3165, v_loss=22.6342, entropy=-18.5909, mem_div=-2.5086, wm_unc=4.5611, lr=1.00e-06
Training Episodes:   2%|         | 36/2000 [4:35:04<245:23:39, 449.81s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
22:35:34 [INFO] [Step 0] First memory divergence measurement: 52.2885, setting reward=0
22:35:37 [INFO] Memory Gate WM: 0.000678
22:36:03 [INFO] Memory Gate WM: 0.000678
22:37:59 [INFO] [Episode 36] Completed - Detailed Statistics:
22:37:59 [INFO]   Total steps: 50
22:37:59 [INFO]   Episode reward: 98.8543
22:37:59 [INFO]   Avg memory div reward: -2.554041
22:37:59 [INFO]   Avg memory div abs: 52.181805
22:37:59 [INFO]   Avg WM uncertainty: 4.531127
22:37:59 [INFO]   Min/Max reward: 1.6522 / 4.5703
22:39:14 [INFO] [Episode 36] PPO Epoch 0 - Training Losses:
22:39:14 [INFO]   Policy loss: 0.261947
22:39:14 [INFO]   Value loss: 21.827315
22:39:14 [INFO]   Entropy: -18.589767
22:39:14 [INFO]   Total loss: 11.361502
22:40:25 [INFO] Memory Gate WM: 0.000691
22:42:46 [INFO] ================================================================================
22:42:46 [INFO] Episode 36 - Aggregated Metrics Summary:
22:42:46 [INFO]   Reward: 102.5246 (recent avg: 99.2789, std: 5.3506)
22:42:46 [INFO]   Policy Loss: 0.316026 (recent avg: 0.277843)
22:42:46 [INFO]   Value Loss: 22.560907
22:42:46 [INFO]   Entropy: -18.590795
22:42:46 [INFO]   Memory Div Reward: -2.509795 (recent avg: -2.540010)
22:42:46 [INFO]   WM Uncertainty: 4.560287
22:42:46 [INFO]   Learning Rate: 1.00e-06
22:42:46 [INFO]   Total Steps: 1850
22:42:46 [INFO]   Log Std: mean=-1.9999, std=0.0001
22:42:46 [INFO] ================================================================================
22:42:46 [INFO] Episode 36: reward=102.52, p_loss=0.3160, v_loss=22.5609, entropy=-18.5908, mem_div=-2.5098, wm_unc=4.5603, lr=1.00e-06
Training Episodes:   2%|         | 37/2000 [4:42:22<243:19:23, 446.24s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
22:42:52 [INFO] [Step 0] First memory divergence measurement: 52.3726, setting reward=0
22:45:13 [INFO] [Episode 37] Completed - Detailed Statistics:
22:45:13 [INFO]   Total steps: 50
22:45:13 [INFO]   Episode reward: 96.6037
22:45:13 [INFO]   Avg memory div reward: -2.569917
22:45:13 [INFO]   Avg memory div abs: 52.368514
22:45:13 [INFO]   Avg WM uncertainty: 4.501991
22:45:13 [INFO]   Min/Max reward: 1.6447 / 4.5458
22:46:24 [INFO] [Episode 37] PPO Epoch 0 - Training Losses:
22:46:24 [INFO]   Policy loss: 0.205873
22:46:24 [INFO]   Value loss: 21.211012
22:46:24 [INFO]   Entropy: -18.589734
22:46:24 [INFO]   Total loss: 10.997276
22:49:58 [INFO] ================================================================================
22:49:58 [INFO] Episode 37 - Aggregated Metrics Summary:
22:49:58 [INFO]   Reward: 102.3688 (recent avg: 98.5621, std: 5.1780)
22:49:58 [INFO]   Policy Loss: 0.317909 (recent avg: 0.271744)
22:49:58 [INFO]   Value Loss: 22.474330
22:49:58 [INFO]   Entropy: -18.590703
22:49:58 [INFO]   Memory Div Reward: -2.511377 (recent avg: -2.545325)
22:49:58 [INFO]   WM Uncertainty: 4.558753
22:49:58 [INFO]   Learning Rate: 1.00e-06
22:49:58 [INFO]   Total Steps: 1900
22:49:58 [INFO]   Log Std: mean=-1.9999, std=0.0001
22:49:58 [INFO] ================================================================================
22:49:58 [INFO] Episode 37: reward=102.37, p_loss=0.3179, v_loss=22.4743, entropy=-18.5907, mem_div=-2.5114, wm_unc=4.5588, lr=1.00e-06
Training Episodes:   2%|         | 38/2000 [4:49:34<240:50:13, 441.90s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
22:50:02 [INFO] [Step 0] First memory divergence measurement: 52.0179, setting reward=0
22:51:28 [INFO] Memory Gate Act: -0.000357
22:52:16 [INFO] Memory Gate WM: 0.000738
22:52:32 [INFO] [Episode 38] Completed - Detailed Statistics:
22:52:32 [INFO]   Total steps: 50
22:52:32 [INFO]   Episode reward: 111.6493
22:52:32 [INFO]   Avg memory div reward: -2.558922
22:52:32 [INFO]   Avg memory div abs: 52.126771
22:52:32 [INFO]   Avg WM uncertainty: 4.791908
22:52:32 [INFO]   Min/Max reward: 1.7786 / 4.7005
22:53:21 [INFO] Memory Gate WM: 0.000742
22:53:43 [INFO] [Episode 38] PPO Epoch 0 - Training Losses:
22:53:43 [INFO]   Policy loss: 0.191847
22:53:43 [INFO]   Value loss: 24.842127
22:53:43 [INFO]   Entropy: -18.589672
22:53:43 [INFO]   Total loss: 12.798807
22:55:26 [INFO] Memory Gate WM: 0.000741
22:57:14 [INFO] ================================================================================
22:57:14 [INFO] Episode 38 - Aggregated Metrics Summary:
22:57:14 [INFO]   Reward: 102.6068 (recent avg: 99.8700, std: 6.4984)
22:57:14 [INFO]   Policy Loss: 0.316345 (recent avg: 0.271605)
22:57:14 [INFO]   Value Loss: 22.576375
22:57:14 [INFO]   Entropy: -18.590611
22:57:14 [INFO]   Memory Div Reward: -2.512596 (recent avg: -2.547890)
22:57:14 [INFO]   WM Uncertainty: 4.564731
22:57:14 [INFO]   Learning Rate: 1.00e-06
22:57:14 [INFO]   Total Steps: 1950
22:57:14 [INFO]   Log Std: mean=-1.9999, std=0.0001
22:57:14 [INFO] ================================================================================
22:57:14 [INFO] Episode 38: reward=102.61, p_loss=0.3163, v_loss=22.5764, entropy=-18.5906, mem_div=-2.5126, wm_unc=4.5647, lr=1.00e-06
Training Episodes:   2%|         | 39/2000 [4:56:49<239:43:05, 440.07s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
22:57:17 [INFO] [Step 0] First memory divergence measurement: 52.4224, setting reward=0
22:58:38 [INFO] Memory Gate WM: 0.000748
22:59:43 [INFO] [Episode 39] Completed - Detailed Statistics:
22:59:43 [INFO]   Total steps: 50
22:59:43 [INFO]   Episode reward: 101.2867
22:59:43 [INFO]   Avg memory div reward: -2.569008
22:59:43 [INFO]   Avg memory div abs: 52.466733
22:59:43 [INFO]   Avg WM uncertainty: 4.594742
22:59:43 [INFO]   Min/Max reward: 1.6272 / 4.8225
23:00:53 [INFO] [Episode 39] PPO Epoch 0 - Training Losses:
23:00:53 [INFO]   Policy loss: 0.978807
23:00:53 [INFO]   Value loss: 22.509694
23:00:53 [INFO]   Entropy: -18.589644
23:00:53 [INFO]   Total loss: 12.419550
23:01:13 [INFO] Memory Gate WM: 0.000741
23:02:40 [INFO] Memory Gate WM: 0.000735
23:04:23 [INFO] ================================================================================
23:04:23 [INFO] Episode 39 - Aggregated Metrics Summary:
23:04:23 [INFO]   Reward: 102.5737 (recent avg: 100.4688, std: 6.3231)
23:04:23 [INFO]   Policy Loss: 0.324792 (recent avg: 0.353292)
23:04:23 [INFO]   Value Loss: 22.610064
23:04:23 [INFO]   Entropy: -18.590516
23:04:23 [INFO]   Memory Div Reward: -2.514007 (recent avg: -2.550665)
23:04:23 [INFO]   WM Uncertainty: 4.565482
23:04:23 [INFO]   Learning Rate: 1.00e-06
23:04:23 [INFO]   Total Steps: 2000
23:04:23 [INFO]   Log Std: mean=-1.9999, std=0.0001
23:04:23 [INFO] ================================================================================
23:04:23 [INFO] Episode 39: reward=102.57, p_loss=0.3248, v_loss=22.6101, entropy=-18.5905, mem_div=-2.5140, wm_unc=4.5655, lr=1.00e-06
23:04:25 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000040.mp4
Training Episodes:   2%|         | 40/2000 [5:04:01<238:09:48, 437.44s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
23:04:29 [INFO] [Step 0] First memory divergence measurement: 52.3826, setting reward=0
23:07:01 [INFO] [Episode 40] Completed - Detailed Statistics:
23:07:01 [INFO]   Total steps: 50
23:07:01 [INFO]   Episode reward: 103.9811
23:07:01 [INFO]   Avg memory div reward: -2.571062
23:07:01 [INFO]   Avg memory div abs: 52.451958
23:07:01 [INFO]   Avg WM uncertainty: 4.650685
23:07:01 [INFO]   Min/Max reward: 1.5788 / 4.6736
23:08:11 [INFO] [Episode 40] PPO Epoch 0 - Training Losses:
23:08:11 [INFO]   Policy loss: 0.234811
23:08:11 [INFO]   Value loss: 23.154760
23:08:11 [INFO]   Entropy: -18.589493
23:08:11 [INFO]   Total loss: 11.998086
23:08:34 [INFO] Memory Gate Act: -0.000369
23:08:52 [INFO] Memory Gate WM: 0.000751
23:10:51 [INFO] Memory Gate WM: 0.000760
23:11:42 [INFO] ================================================================================
23:11:42 [INFO] Episode 40 - Aggregated Metrics Summary:
23:11:42 [INFO]   Reward: 102.6081 (recent avg: 101.5020, std: 5.9579)
23:11:42 [INFO]   Policy Loss: 0.322947 (recent avg: 0.380337)
23:11:42 [INFO]   Value Loss: 22.594830
23:11:42 [INFO]   Entropy: -18.590421
23:11:42 [INFO]   Memory Div Reward: -2.515398 (recent avg: -2.553264)
23:11:42 [INFO]   WM Uncertainty: 4.567560
23:11:42 [INFO]   Learning Rate: 1.00e-06
23:11:42 [INFO]   Total Steps: 2050
23:11:42 [INFO]   Log Std: mean=-1.9999, std=0.0001
23:11:42 [INFO] ================================================================================
23:11:42 [INFO] Episode 40: reward=102.61, p_loss=0.3229, v_loss=22.5948, entropy=-18.5904, mem_div=-2.5154, wm_unc=4.5676, lr=1.00e-06
Training Episodes:   2%|         | 41/2000 [5:11:17<237:55:42, 437.23s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
23:11:47 [INFO] [Step 0] First memory divergence measurement: 52.5410, setting reward=0
23:13:29 [INFO] Memory Gate WM: 0.000763
23:14:11 [INFO] [Episode 41] Completed - Detailed Statistics:
23:14:11 [INFO]   Total steps: 50
23:14:11 [INFO]   Episode reward: 95.9921
23:14:11 [INFO]   Avg memory div reward: -2.556992
23:14:11 [INFO]   Avg memory div abs: 52.348115
23:14:11 [INFO]   Avg WM uncertainty: 4.476834
23:14:11 [INFO]   Min/Max reward: 1.2890 / 4.2899
23:15:21 [INFO] [Episode 41] PPO Epoch 0 - Training Losses:
23:15:21 [INFO]   Policy loss: 0.410955
23:15:21 [INFO]   Value loss: 21.736540
23:15:21 [INFO]   Entropy: -18.589303
23:15:21 [INFO]   Total loss: 11.465118
23:15:39 [INFO] Memory Gate Act: -0.000386
23:17:46 [INFO] Memory Gate Act: -0.000392
23:18:52 [INFO] Memory Gate Act: -0.000402
23:18:53 [INFO] ================================================================================
23:18:53 [INFO] Episode 41 - Aggregated Metrics Summary:
23:18:53 [INFO]   Reward: 102.4506 (recent avg: 100.4632, std: 5.9223)
23:18:53 [INFO]   Policy Loss: 0.327067 (recent avg: 0.342546)
23:18:53 [INFO]   Value Loss: 22.594754
23:18:53 [INFO]   Entropy: -18.590327
23:18:53 [INFO]   Memory Div Reward: -2.516389 (recent avg: -2.555587)
23:18:53 [INFO]   WM Uncertainty: 4.565400
23:18:53 [INFO]   Learning Rate: 1.00e-06
23:18:53 [INFO]   Total Steps: 2100
23:18:53 [INFO]   Log Std: mean=-1.9999, std=0.0001
23:18:53 [INFO] ================================================================================
23:18:53 [INFO] Episode 41: reward=102.45, p_loss=0.3271, v_loss=22.5948, entropy=-18.5903, mem_div=-2.5164, wm_unc=4.5654, lr=1.00e-06
Training Episodes:   2%|         | 42/2000 [5:18:29<236:54:14, 435.57s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
23:18:57 [INFO] [Step 0] First memory divergence measurement: 52.7722, setting reward=0
23:21:29 [INFO] [Episode 42] Completed - Detailed Statistics:
23:21:29 [INFO]   Total steps: 50
23:21:29 [INFO]   Episode reward: 100.7766
23:21:29 [INFO]   Avg memory div reward: -2.575607
23:21:29 [INFO]   Avg memory div abs: 52.623667
23:21:29 [INFO]   Avg WM uncertainty: 4.591138
23:21:29 [INFO]   Min/Max reward: 1.6212 / 4.7524
23:22:43 [INFO] [Episode 42] PPO Epoch 0 - Training Losses:
23:22:43 [INFO]   Policy loss: 0.598624
23:22:43 [INFO]   Value loss: 22.047529
23:22:43 [INFO]   Entropy: -18.589266
23:22:43 [INFO]   Total loss: 11.808281
23:23:48 [INFO] Memory Gate WM: 0.000781
23:24:44 [INFO] Memory Gate Act: -0.000412
23:26:21 [INFO] ================================================================================
23:26:21 [INFO] Episode 42 - Aggregated Metrics Summary:
23:26:21 [INFO]   Reward: 102.4116 (recent avg: 100.4075, std: 5.9164)
23:26:21 [INFO]   Policy Loss: 0.317686 (recent avg: 0.367172)
23:26:21 [INFO]   Value Loss: 22.558249
23:26:21 [INFO]   Entropy: -18.590235
23:26:21 [INFO]   Memory Div Reward: -2.517766 (recent avg: -2.559973)
23:26:21 [INFO]   WM Uncertainty: 4.565998
23:26:21 [INFO]   Learning Rate: 1.00e-06
23:26:21 [INFO]   Total Steps: 2150
23:26:21 [INFO]   Log Std: mean=-1.9998, std=0.0001
23:26:21 [INFO] ================================================================================
23:26:21 [INFO] Episode 42: reward=102.41, p_loss=0.3177, v_loss=22.5582, entropy=-18.5902, mem_div=-2.5178, wm_unc=4.5660, lr=1.00e-06
Training Episodes:   2%|         | 43/2000 [5:25:57<238:43:28, 439.15s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
23:26:25 [INFO] [Step 0] First memory divergence measurement: 52.8062, setting reward=0
23:27:21 [INFO] Memory Gate WM: 0.000786
23:27:52 [INFO] Memory Gate WM: 0.000786
23:28:50 [INFO] [Episode 43] Completed - Detailed Statistics:
23:28:50 [INFO]   Total steps: 50
23:28:50 [INFO]   Episode reward: 99.2965
23:28:50 [INFO]   Avg memory div reward: -2.579968
23:28:50 [INFO]   Avg memory div abs: 52.660566
23:28:50 [INFO]   Avg WM uncertainty: 4.565897
23:28:50 [INFO]   Min/Max reward: 1.7333 / 4.5512
23:30:07 [INFO] [Episode 43] PPO Epoch 0 - Training Losses:
23:30:07 [INFO]   Policy loss: 0.342791
23:30:07 [INFO]   Value loss: 21.910938
23:30:07 [INFO]   Entropy: -18.589038
23:30:07 [INFO]   Total loss: 11.484151
23:33:55 [INFO] ================================================================================
23:33:55 [INFO] Episode 43 - Aggregated Metrics Summary:
23:33:55 [INFO]   Reward: 102.3408 (recent avg: 101.1208, std: 5.2746)
23:33:55 [INFO]   Policy Loss: 0.314507 (recent avg: 0.342066)
23:33:55 [INFO]   Value Loss: 22.507080
23:33:55 [INFO]   Entropy: -18.590143
23:33:55 [INFO]   Memory Div Reward: -2.519179 (recent avg: -2.563970)
23:33:55 [INFO]   WM Uncertainty: 4.565996
23:33:55 [INFO]   Learning Rate: 1.00e-06
23:33:55 [INFO]   Total Steps: 2200
23:33:55 [INFO]   Log Std: mean=-1.9998, std=0.0001
23:33:55 [INFO] ================================================================================
23:33:55 [INFO] Episode 43: reward=102.34, p_loss=0.3145, v_loss=22.5071, entropy=-18.5901, mem_div=-2.5192, wm_unc=4.5660, lr=1.00e-06
Training Episodes:   2%|         | 44/2000 [5:33:30<240:58:26, 443.51s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
23:33:59 [INFO] [Step 0] First memory divergence measurement: 52.8643, setting reward=0
23:36:30 [INFO] [Episode 44] Completed - Detailed Statistics:
23:36:30 [INFO]   Total steps: 50
23:36:30 [INFO]   Episode reward: 98.8704
23:36:30 [INFO]   Avg memory div reward: -2.580640
23:36:30 [INFO]   Avg memory div abs: 52.772567
23:36:30 [INFO]   Avg WM uncertainty: 4.558049
23:36:30 [INFO]   Min/Max reward: 1.7067 / 4.5504
23:37:03 [INFO] Memory Gate WM: 0.000817
23:37:45 [INFO] [Episode 44] PPO Epoch 0 - Training Losses:
23:37:45 [INFO]   Policy loss: 0.274358
23:37:45 [INFO]   Value loss: 21.685729
23:37:45 [INFO]   Entropy: -18.588915
23:37:45 [INFO]   Total loss: 11.303112
23:39:48 [INFO] Memory Gate WM: 0.000822
23:41:22 [INFO] ================================================================================
23:41:22 [INFO] Episode 44 - Aggregated Metrics Summary:
23:41:22 [INFO]   Reward: 102.2637 (recent avg: 101.5896, std: 4.8264)
23:41:22 [INFO]   Policy Loss: 0.306159 (recent avg: 0.276985)
23:41:22 [INFO]   Value Loss: 22.445781
23:41:22 [INFO]   Entropy: -18.590055
23:41:22 [INFO]   Memory Div Reward: -2.520545 (recent avg: -2.566448)
23:41:22 [INFO]   WM Uncertainty: 4.565819
23:41:22 [INFO]   Learning Rate: 1.00e-06
23:41:22 [INFO]   Total Steps: 2250
23:41:22 [INFO]   Log Std: mean=-1.9998, std=0.0001
23:41:22 [INFO] ================================================================================
23:41:22 [INFO] Episode 44: reward=102.26, p_loss=0.3062, v_loss=22.4458, entropy=-18.5901, mem_div=-2.5205, wm_unc=4.5658, lr=1.00e-06
Training Episodes:   2%|         | 45/2000 [5:40:57<241:24:29, 444.54s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
23:41:27 [INFO] [Step 0] First memory divergence measurement: 52.9710, setting reward=0
23:43:51 [INFO] [Episode 45] Completed - Detailed Statistics:
23:43:51 [INFO]   Total steps: 50
23:43:51 [INFO]   Episode reward: 98.0256
23:43:51 [INFO]   Avg memory div reward: -2.596398
23:43:51 [INFO]   Avg memory div abs: 52.999267
23:43:51 [INFO]   Avg WM uncertainty: 4.556910
23:43:51 [INFO]   Min/Max reward: 1.6179 / 4.5166
23:45:04 [INFO] [Episode 45] PPO Epoch 0 - Training Losses:
23:45:04 [INFO]   Policy loss: 1.129742
23:45:04 [INFO]   Value loss: 21.786899
23:45:04 [INFO]   Entropy: -18.588821
23:45:04 [INFO]   Total loss: 12.209080
23:46:31 [INFO] Memory Gate WM: 0.000830
23:46:34 [INFO] Memory Gate Act: -0.000422
23:48:35 [INFO] ================================================================================
23:48:35 [INFO] Episode 45 - Aggregated Metrics Summary:
23:48:35 [INFO]   Reward: 102.1716 (recent avg: 100.5336, std: 4.3075)
23:48:35 [INFO]   Policy Loss: 0.311245 (recent avg: 0.353638)
23:48:35 [INFO]   Value Loss: 22.385421
23:48:35 [INFO]   Entropy: -18.589970
23:48:35 [INFO]   Memory Div Reward: -2.522194 (recent avg: -2.571255)
23:48:35 [INFO]   WM Uncertainty: 4.565626
23:48:35 [INFO]   Learning Rate: 1.00e-06
23:48:35 [INFO]   Total Steps: 2300
23:48:35 [INFO]   Log Std: mean=-1.9998, std=0.0001
23:48:35 [INFO] ================================================================================
23:48:35 [INFO] Episode 45: reward=102.17, p_loss=0.3112, v_loss=22.3854, entropy=-18.5900, mem_div=-2.5222, wm_unc=4.5656, lr=1.00e-06
Training Episodes:   2%|         | 46/2000 [5:48:11<239:28:41, 441.21s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
23:48:41 [INFO] [Step 0] First memory divergence measurement: 52.7369, setting reward=0
23:49:33 [INFO] Memory Gate Act: -0.000425
23:49:46 [INFO] Memory Gate Act: -0.000425
23:51:11 [INFO] [Episode 46] Completed - Detailed Statistics:
23:51:11 [INFO]   Total steps: 50
23:51:11 [INFO]   Episode reward: 97.1469
23:51:11 [INFO]   Avg memory div reward: -2.587165
23:51:11 [INFO]   Avg memory div abs: 52.745782
23:51:11 [INFO]   Avg WM uncertainty: 4.530103
23:51:11 [INFO]   Min/Max reward: 1.5192 / 4.8089
23:52:08 [INFO] Memory Gate WM: 0.000835
23:52:22 [INFO] [Episode 46] PPO Epoch 0 - Training Losses:
23:52:22 [INFO]   Policy loss: 0.266340
23:52:22 [INFO]   Value loss: 20.952704
23:52:22 [INFO]   Entropy: -18.588731
23:52:22 [INFO]   Total loss: 10.928579
23:53:11 [INFO] Memory Gate Act: -0.000428
23:54:27 [INFO] Memory Gate Act: -0.000428
23:55:52 [INFO] ================================================================================
23:55:52 [INFO] Episode 46 - Aggregated Metrics Summary:
23:55:52 [INFO]   Reward: 102.0647 (recent avg: 100.3629, std: 4.4035)
23:55:52 [INFO]   Policy Loss: 0.311337 (recent avg: 0.344022)
23:55:52 [INFO]   Value Loss: 22.211425
23:55:52 [INFO]   Entropy: -18.589885
23:55:52 [INFO]   Memory Div Reward: -2.523577 (recent avg: -2.574568)
23:55:52 [INFO]   WM Uncertainty: 4.564870
23:55:52 [INFO]   Learning Rate: 1.00e-06
23:55:52 [INFO]   Total Steps: 2350
23:55:52 [INFO]   Log Std: mean=-1.9998, std=0.0001
23:55:52 [INFO] ================================================================================
23:55:52 [INFO] Episode 46: reward=102.06, p_loss=0.3113, v_loss=22.2114, entropy=-18.5899, mem_div=-2.5236, wm_unc=4.5649, lr=1.00e-06
Training Episodes:   2%|         | 47/2000 [5:55:28<238:40:44, 439.96s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
23:55:58 [INFO] [Step 0] First memory divergence measurement: 53.1812, setting reward=0
23:58:24 [INFO] [Episode 47] Completed - Detailed Statistics:
23:58:24 [INFO]   Total steps: 50
23:58:24 [INFO]   Episode reward: 101.0533
23:58:24 [INFO]   Avg memory div reward: -2.594390
23:58:24 [INFO]   Avg memory div abs: 53.055504
23:58:24 [INFO]   Avg WM uncertainty: 4.615456
23:58:24 [INFO]   Min/Max reward: 1.6489 / 4.7201
23:58:59 [INFO] Memory Gate Act: -0.000435
23:59:35 [INFO] [Episode 47] PPO Epoch 0 - Training Losses:
23:59:35 [INFO]   Policy loss: 0.159818
23:59:35 [INFO]   Value loss: 21.990189
23:59:35 [INFO]   Entropy: -18.588708
23:59:35 [INFO]   Total loss: 11.340800
00:01:04 [INFO] Memory Gate WM: 0.000893
00:03:09 [INFO] ================================================================================
00:03:09 [INFO] Episode 47 - Aggregated Metrics Summary:
00:03:09 [INFO]   Reward: 102.0436 (recent avg: 100.8078, std: 4.2222)
00:03:09 [INFO]   Policy Loss: 0.310454 (recent avg: 0.246535)
00:03:09 [INFO]   Value Loss: 22.184591
00:03:09 [INFO]   Entropy: -18.589800
00:03:09 [INFO]   Memory Div Reward: -2.525052 (recent avg: -2.577015)
00:03:09 [INFO]   WM Uncertainty: 4.565924
00:03:09 [INFO]   Learning Rate: 1.00e-06
00:03:09 [INFO]   Total Steps: 2400
00:03:09 [INFO]   Log Std: mean=-1.9998, std=0.0001
00:03:09 [INFO] ================================================================================
00:03:09 [INFO] Episode 47: reward=102.04, p_loss=0.3105, v_loss=22.1846, entropy=-18.5898, mem_div=-2.5251, wm_unc=4.5659, lr=1.00e-06
Training Episodes:   2%|         | 48/2000 [6:02:45<238:04:22, 439.07s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
00:03:15 [INFO] [Step 0] First memory divergence measurement: 53.0195, setting reward=0
00:03:30 [INFO] Memory Gate WM: 0.000914
00:05:44 [INFO] [Episode 48] Completed - Detailed Statistics:
00:05:44 [INFO]   Total steps: 50
00:05:44 [INFO]   Episode reward: 97.0071
00:05:44 [INFO]   Avg memory div reward: -2.618455
00:05:44 [INFO]   Avg memory div abs: 53.280523
00:05:44 [INFO]   Avg WM uncertainty: 4.558598
00:05:44 [INFO]   Min/Max reward: 1.3449 / 4.8024
00:06:54 [INFO] [Episode 48] PPO Epoch 0 - Training Losses:
00:06:54 [INFO]   Policy loss: 0.288814
00:06:54 [INFO]   Value loss: 21.396925
00:06:54 [INFO]   Entropy: -18.588591
00:06:54 [INFO]   Total loss: 11.173162
00:07:38 [INFO] Memory Gate WM: 0.000923
00:09:11 [INFO] Memory Gate Act: -0.000480
00:09:11 [INFO] Memory Gate Act: -0.000480
00:09:22 [INFO] Memory Gate Act: -0.000480
00:09:39 [INFO] Memory Gate Act: -0.000481
00:09:43 [INFO] Memory Gate WM: 0.000940
00:10:22 [INFO] ================================================================================
00:10:22 [INFO] Episode 48 - Aggregated Metrics Summary:
00:10:22 [INFO]   Reward: 101.9408 (recent avg: 99.3436, std: 2.3182)
00:10:22 [INFO]   Policy Loss: 0.306073 (recent avg: 0.239483)
00:10:22 [INFO]   Value Loss: 22.173950
00:10:22 [INFO]   Entropy: -18.589714
00:10:22 [INFO]   Memory Div Reward: -2.526958 (recent avg: -2.582969)
00:10:22 [INFO]   WM Uncertainty: 4.565774
00:10:22 [INFO]   Learning Rate: 1.00e-06
00:10:22 [INFO]   Total Steps: 2450
00:10:22 [INFO]   Log Std: mean=-1.9998, std=0.0001
00:10:22 [INFO] ================================================================================
00:10:22 [INFO] Episode 48: reward=101.94, p_loss=0.3061, v_loss=22.1739, entropy=-18.5897, mem_div=-2.5270, wm_unc=4.5658, lr=1.00e-06
Training Episodes:   2%|         | 49/2000 [6:09:58<237:01:42, 437.37s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
00:10:27 [INFO] [Step 0] First memory divergence measurement: 53.3872, setting reward=0
00:12:38 [INFO] Memory Gate WM: 0.000943
00:12:54 [INFO] [Episode 49] Completed - Detailed Statistics:
00:12:54 [INFO]   Total steps: 50
00:12:54 [INFO]   Episode reward: 92.0594
00:12:54 [INFO]   Avg memory div reward: -2.618863
00:12:54 [INFO]   Avg memory div abs: 53.443482
00:12:54 [INFO]   Avg WM uncertainty: 4.460052
00:12:54 [INFO]   Min/Max reward: 1.4575 / 4.3294
00:14:03 [INFO] [Episode 49] PPO Epoch 0 - Training Losses:
00:14:03 [INFO]   Policy loss: 0.366414
00:14:03 [INFO]   Value loss: 20.041888
00:14:03 [INFO]   Entropy: -18.588589
00:14:03 [INFO]   Total loss: 10.573244
00:14:04 [INFO] Memory Gate Act: -0.000492
00:14:09 [INFO] Memory Gate Act: -0.000493
00:17:29 [INFO] ================================================================================
00:17:29 [INFO] Episode 49 - Aggregated Metrics Summary:
00:17:29 [INFO]   Reward: 101.7432 (recent avg: 98.4209, std: 3.0742)
00:17:29 [INFO]   Policy Loss: 0.303429 (recent avg: 0.283239)
00:17:29 [INFO]   Value Loss: 22.026191
00:17:29 [INFO]   Entropy: -18.589630
00:17:29 [INFO]   Memory Div Reward: -2.528796 (recent avg: -2.587954)
00:17:29 [INFO]   WM Uncertainty: 4.563660
00:17:29 [INFO]   Learning Rate: 1.00e-06
00:17:29 [INFO]   Total Steps: 2500
00:17:29 [INFO]   Log Std: mean=-1.9998, std=0.0001
00:17:29 [INFO] ================================================================================
00:17:29 [INFO] Episode 49: reward=101.74, p_loss=0.3034, v_loss=22.0262, entropy=-18.5896, mem_div=-2.5288, wm_unc=4.5637, lr=1.00e-06
00:17:44 [INFO] Saved checkpoint to outputs/student_rl/checkpoint-49
00:17:46 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000050.mp4
Training Episodes:   2%|         | 50/2000 [6:17:22<237:56:26, 439.28s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
00:17:51 [INFO] [Step 0] First memory divergence measurement: 53.2559, setting reward=0
00:18:54 [INFO] Memory Gate Act: -0.000508
00:20:19 [INFO] [Episode 50] Completed - Detailed Statistics:
00:20:19 [INFO]   Total steps: 50
00:20:19 [INFO]   Episode reward: 91.1410
00:20:19 [INFO]   Avg memory div reward: -2.613083
00:20:19 [INFO]   Avg memory div abs: 53.331992
00:20:19 [INFO]   Avg WM uncertainty: 4.435903
00:20:19 [INFO]   Min/Max reward: 1.2827 / 4.6299
00:21:31 [INFO] [Episode 50] PPO Epoch 0 - Training Losses:
00:21:31 [INFO]   Policy loss: 0.262964
00:21:31 [INFO]   Value loss: 19.783779
00:21:31 [INFO]   Entropy: -18.588574
00:21:31 [INFO]   Total loss: 10.340739
00:22:24 [INFO] Memory Gate WM: 0.000971
00:23:43 [INFO] Memory Gate WM: 0.000973
00:25:01 [INFO] ================================================================================
00:25:01 [INFO] Episode 50 - Aggregated Metrics Summary:
00:25:01 [INFO]   Reward: 101.5353 (recent avg: 97.1369, std: 3.1639)
00:25:01 [INFO]   Policy Loss: 0.300329 (recent avg: 0.264791)
00:25:01 [INFO]   Value Loss: 21.894845
00:25:01 [INFO]   Entropy: -18.589546
00:25:01 [INFO]   Memory Div Reward: -2.530449 (recent avg: -2.592156)
00:25:01 [INFO]   WM Uncertainty: 4.561155
00:25:01 [INFO]   Learning Rate: 1.00e-06
00:25:01 [INFO]   Total Steps: 2550
00:25:01 [INFO]   Log Std: mean=-1.9998, std=0.0001
00:25:01 [INFO] ================================================================================
00:25:01 [INFO] Episode 50: reward=101.54, p_loss=0.3003, v_loss=21.8948, entropy=-18.5895, mem_div=-2.5304, wm_unc=4.5612, lr=1.00e-06
Training Episodes:   3%|         | 51/2000 [6:24:37<237:06:34, 437.97s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
00:25:06 [INFO] [Step 0] First memory divergence measurement: 53.4232, setting reward=0
00:27:35 [INFO] [Episode 51] Completed - Detailed Statistics:
00:27:35 [INFO]   Total steps: 50
00:27:35 [INFO]   Episode reward: 97.5135
00:27:35 [INFO]   Avg memory div reward: -2.624944
00:27:35 [INFO]   Avg memory div abs: 53.520100
00:27:35 [INFO]   Avg WM uncertainty: 4.575215
00:27:35 [INFO]   Min/Max reward: 1.6323 / 4.7687
00:28:45 [INFO] [Episode 51] PPO Epoch 0 - Training Losses:
00:28:45 [INFO]   Policy loss: 0.250680
00:28:45 [INFO]   Value loss: 21.448562
00:28:45 [INFO]   Entropy: -18.588497
00:28:45 [INFO]   Total loss: 11.160846
00:30:19 [INFO] Memory Gate WM: 0.000989
00:32:12 [INFO] Memory Gate Act: -0.000538
00:32:17 [INFO] ================================================================================
00:32:17 [INFO] Episode 51 - Aggregated Metrics Summary:
00:32:17 [INFO]   Reward: 101.4580 (recent avg: 97.2890, std: 3.1417)
00:32:17 [INFO]   Policy Loss: 0.300990 (recent avg: 0.260189)
00:32:17 [INFO]   Value Loss: 21.825267
00:32:17 [INFO]   Entropy: -18.589460
00:32:17 [INFO]   Memory Div Reward: -2.532266 (recent avg: -2.598951)
00:32:17 [INFO]   WM Uncertainty: 4.561425
00:32:17 [INFO]   Learning Rate: 1.00e-06
00:32:17 [INFO]   Total Steps: 2600
00:32:17 [INFO]   Log Std: mean=-1.9998, std=0.0001
00:32:17 [INFO] ================================================================================
00:32:17 [INFO] Episode 51: reward=101.46, p_loss=0.3010, v_loss=21.8253, entropy=-18.5895, mem_div=-2.5323, wm_unc=4.5614, lr=1.00e-06
Training Episodes:   3%|         | 52/2000 [6:31:52<236:37:16, 437.29s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
00:32:20 [INFO] [Step 0] First memory divergence measurement: 53.6603, setting reward=0
00:34:02 [INFO] [Episode 52] Completed - Detailed Statistics:
00:34:02 [INFO]   Total steps: 50
00:34:02 [INFO]   Episode reward: 105.0792
00:34:02 [INFO]   Avg memory div reward: -2.626859
00:34:02 [INFO]   Avg memory div abs: 53.613722
00:34:02 [INFO]   Avg WM uncertainty: 4.728444
00:34:02 [INFO]   Min/Max reward: 1.7776 / 4.6619
00:34:16 [INFO] Memory Gate WM: 0.000998
00:34:33 [INFO] Memory Gate WM: 0.000997
00:35:07 [INFO] Memory Gate Act: -0.000537
00:35:11 [INFO] [Episode 52] PPO Epoch 0 - Training Losses:
00:35:11 [INFO]   Policy loss: 0.195915
00:35:11 [INFO]   Value loss: 23.086994
00:35:11 [INFO]   Entropy: -18.588406
00:35:11 [INFO]   Total loss: 11.925297
00:36:01 [INFO] Memory Gate Act: -0.000540
00:36:17 [INFO] Memory Gate WM: 0.001006
00:38:36 [INFO] ================================================================================
00:38:36 [INFO] Episode 52 - Aggregated Metrics Summary:
00:38:36 [INFO]   Reward: 101.5263 (recent avg: 97.7193, std: 3.8128)
00:38:36 [INFO]   Policy Loss: 0.298626 (recent avg: 0.249897)
00:38:36 [INFO]   Value Loss: 21.830320
00:38:36 [INFO]   Entropy: -18.589375
00:38:36 [INFO]   Memory Div Reward: -2.534051 (recent avg: -2.604077)
00:38:36 [INFO]   WM Uncertainty: 4.564576
00:38:36 [INFO]   Learning Rate: 1.00e-06
00:38:36 [INFO]   Total Steps: 2650
00:38:36 [INFO]   Log Std: mean=-1.9998, std=0.0001
00:38:36 [INFO] ================================================================================
00:38:36 [INFO] Episode 52: reward=101.53, p_loss=0.2986, v_loss=21.8303, entropy=-18.5894, mem_div=-2.5341, wm_unc=4.5646, lr=1.00e-06
Training Episodes:   3%|         | 53/2000 [6:38:12<227:07:55, 419.97s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
00:38:39 [INFO] [Step 0] First memory divergence measurement: 53.8022, setting reward=0
00:40:20 [INFO] [Episode 53] Completed - Detailed Statistics:
00:40:20 [INFO]   Total steps: 50
00:40:20 [INFO]   Episode reward: 91.7155
00:40:20 [INFO]   Avg memory div reward: -2.631241
00:40:20 [INFO]   Avg memory div abs: 53.728779
00:40:20 [INFO]   Avg WM uncertainty: 4.465551
00:40:20 [INFO]   Min/Max reward: 1.5557 / 4.6512
00:41:08 [INFO] Memory Gate Act: -0.000555
00:41:29 [INFO] [Episode 53] PPO Epoch 0 - Training Losses:
00:41:29 [INFO]   Policy loss: 0.219612
00:41:29 [INFO]   Value loss: 19.995690
00:41:29 [INFO]   Entropy: -18.588404
00:41:29 [INFO]   Total loss: 10.403341
00:44:55 [INFO] ================================================================================
00:44:55 [INFO] Episode 53 - Aggregated Metrics Summary:
00:44:55 [INFO]   Reward: 101.3446 (recent avg: 96.9612, std: 4.1616)
00:44:55 [INFO]   Policy Loss: 0.296774 (recent avg: 0.253569)
00:44:55 [INFO]   Value Loss: 21.760784
00:44:55 [INFO]   Entropy: -18.589294
00:44:55 [INFO]   Memory Div Reward: -2.535851 (recent avg: -2.609204)
00:44:55 [INFO]   WM Uncertainty: 4.562743
00:44:55 [INFO]   Learning Rate: 1.00e-06
00:44:55 [INFO]   Total Steps: 2700
00:44:55 [INFO]   Log Std: mean=-1.9998, std=0.0001
00:44:55 [INFO] ================================================================================
00:44:55 [INFO] Episode 53: reward=101.34, p_loss=0.2968, v_loss=21.7608, entropy=-18.5893, mem_div=-2.5359, wm_unc=4.5627, lr=1.00e-06
Training Episodes:   3%|         | 54/2000 [6:44:31<220:19:19, 407.58s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
00:44:58 [INFO] [Step 0] First memory divergence measurement: 53.5574, setting reward=0
00:45:18 [INFO] Memory Gate WM: 0.001045
00:46:35 [INFO] Memory Gate Act: -0.000578
00:46:38 [INFO] [Episode 54] Completed - Detailed Statistics:
00:46:38 [INFO]   Total steps: 50
00:46:38 [INFO]   Episode reward: 95.7575
00:46:38 [INFO]   Avg memory div reward: -2.639269
00:46:38 [INFO]   Avg memory div abs: 53.672781
00:46:38 [INFO]   Avg WM uncertainty: 4.554419
00:46:38 [INFO]   Min/Max reward: 1.5476 / 4.5326
00:47:20 [INFO] Memory Gate Act: -0.000578
00:47:46 [INFO] [Episode 54] PPO Epoch 0 - Training Losses:
00:47:46 [INFO]   Policy loss: 0.236577
00:47:46 [INFO]   Value loss: 20.849990
00:47:46 [INFO]   Entropy: -18.588308
00:47:46 [INFO]   Total loss: 10.847455
00:50:30 [INFO] Memory Gate WM: 0.001054
00:51:11 [INFO] ================================================================================
00:51:11 [INFO] Episode 54 - Aggregated Metrics Summary:
00:51:11 [INFO]   Reward: 101.2430 (recent avg: 96.6499, std: 4.1234)
00:51:11 [INFO]   Policy Loss: 0.292215 (recent avg: 0.243718)
00:51:11 [INFO]   Value Loss: 21.755279
00:51:11 [INFO]   Entropy: -18.589213
00:51:11 [INFO]   Memory Div Reward: -2.537731 (recent avg: -2.615067)
00:51:11 [INFO]   WM Uncertainty: 4.562591
00:51:11 [INFO]   Learning Rate: 1.00e-06
00:51:11 [INFO]   Total Steps: 2750
00:51:11 [INFO]   Log Std: mean=-1.9998, std=0.0001
00:51:11 [INFO] ================================================================================
00:51:11 [INFO] Episode 54: reward=101.24, p_loss=0.2922, v_loss=21.7553, entropy=-18.5892, mem_div=-2.5377, wm_unc=4.5626, lr=1.00e-06
Training Episodes:   3%|         | 55/2000 [6:50:47<215:04:05, 398.07s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
00:51:14 [INFO] [Step 0] First memory divergence measurement: 53.6853, setting reward=0
00:52:55 [INFO] [Episode 55] Completed - Detailed Statistics:
00:52:55 [INFO]   Total steps: 50
00:52:55 [INFO]   Episode reward: 99.9363
00:52:55 [INFO]   Avg memory div reward: -2.619590
00:52:55 [INFO]   Avg memory div abs: 53.454348
00:52:55 [INFO]   Avg WM uncertainty: 4.618316
00:52:55 [INFO]   Min/Max reward: 1.5594 / 4.5986
00:53:55 [INFO] Memory Gate Act: -0.000602
00:54:02 [INFO] [Episode 55] PPO Epoch 0 - Training Losses:
00:54:02 [INFO]   Policy loss: 0.201785
00:54:02 [INFO]   Value loss: 22.086450
00:54:02 [INFO]   Entropy: -18.588252
00:54:02 [INFO]   Total loss: 11.430893
00:57:31 [INFO] ================================================================================
00:57:31 [INFO] Episode 55 - Aggregated Metrics Summary:
00:57:31 [INFO]   Reward: 101.2197 (recent avg: 96.8410, std: 4.2257)
00:57:31 [INFO]   Policy Loss: 0.292120 (recent avg: 0.243410)
00:57:31 [INFO]   Value Loss: 21.805443
00:57:31 [INFO]   Entropy: -18.589133
00:57:31 [INFO]   Memory Div Reward: -2.539193 (recent avg: -2.617386)
00:57:31 [INFO]   WM Uncertainty: 4.563586
00:57:31 [INFO]   Learning Rate: 1.00e-06
00:57:31 [INFO]   Total Steps: 2800
00:57:31 [INFO]   Log Std: mean=-1.9998, std=0.0001
00:57:31 [INFO] ================================================================================
00:57:31 [INFO] Episode 55: reward=101.22, p_loss=0.2921, v_loss=21.8054, entropy=-18.5891, mem_div=-2.5392, wm_unc=4.5636, lr=1.00e-06
Training Episodes:   3%|         | 56/2000 [6:57:06<212:00:15, 392.60s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
00:57:33 [INFO] [Step 0] First memory divergence measurement: 54.1498, setting reward=0
00:58:29 [INFO] Memory Gate WM: 0.001075
00:59:14 [INFO] [Episode 56] Completed - Detailed Statistics:
00:59:14 [INFO]   Total steps: 50
00:59:14 [INFO]   Episode reward: 101.4939
00:59:14 [INFO]   Avg memory div reward: -2.639946
00:59:14 [INFO]   Avg memory div abs: 53.963175
00:59:14 [INFO]   Avg WM uncertainty: 4.669824
00:59:14 [INFO]   Min/Max reward: 1.5196 / 4.3216
00:59:56 [INFO] Memory Gate Act: -0.000631
01:00:23 [INFO] [Episode 56] PPO Epoch 0 - Training Losses:
01:00:23 [INFO]   Policy loss: 0.172796
01:00:23 [INFO]   Value loss: 22.215930
01:00:23 [INFO]   Entropy: -18.588185
01:00:23 [INFO]   Total loss: 11.466642
01:01:09 [INFO] Memory Gate WM: 0.001091
01:01:37 [INFO] Memory Gate Act: -0.000639
01:03:48 [INFO] ================================================================================
01:03:48 [INFO] Episode 56 - Aggregated Metrics Summary:
01:03:48 [INFO]   Reward: 101.2245 (recent avg: 97.2757, std: 4.4523)
01:03:48 [INFO]   Policy Loss: 0.292750 (recent avg: 0.240825)
01:03:48 [INFO]   Value Loss: 21.745273
01:03:48 [INFO]   Entropy: -18.589052
01:03:48 [INFO]   Memory Div Reward: -2.540960 (recent avg: -2.622664)
01:03:48 [INFO]   WM Uncertainty: 4.565450
01:03:48 [INFO]   Learning Rate: 1.00e-06
01:03:48 [INFO]   Total Steps: 2850
01:03:48 [INFO]   Log Std: mean=-1.9998, std=0.0001
01:03:48 [INFO] ================================================================================
01:03:48 [INFO] Episode 56: reward=101.22, p_loss=0.2927, v_loss=21.7453, entropy=-18.5891, mem_div=-2.5410, wm_unc=4.5655, lr=1.00e-06
Training Episodes:   3%|         | 57/2000 [7:03:24<209:25:34, 388.03s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
01:03:51 [INFO] [Step 0] First memory divergence measurement: 54.1711, setting reward=0
01:05:30 [INFO] [Episode 57] Completed - Detailed Statistics:
01:05:30 [INFO]   Total steps: 50
01:05:30 [INFO]   Episode reward: 98.0560
01:05:30 [INFO]   Avg memory div reward: -2.661512
01:05:30 [INFO]   Avg memory div abs: 54.274790
01:05:30 [INFO]   Avg WM uncertainty: 4.622632
01:05:30 [INFO]   Min/Max reward: 1.5127 / 4.7664
01:06:41 [INFO] [Episode 57] PPO Epoch 0 - Training Losses:
01:06:41 [INFO]   Policy loss: 0.231908
01:06:41 [INFO]   Value loss: 21.171415
01:06:41 [INFO]   Entropy: -18.588137
01:06:41 [INFO]   Total loss: 11.003497
01:08:00 [INFO] Memory Gate Act: -0.000667
01:10:12 [INFO] ================================================================================
01:10:12 [INFO] Episode 57 - Aggregated Metrics Summary:
01:10:12 [INFO]   Reward: 101.1699 (recent avg: 96.9760, std: 4.2857)
01:10:12 [INFO]   Policy Loss: 0.290998 (recent avg: 0.250003)
01:10:12 [INFO]   Value Loss: 21.699665
01:10:12 [INFO]   Entropy: -18.588970
01:10:12 [INFO]   Memory Div Reward: -2.543039 (recent avg: -2.629376)
01:10:12 [INFO]   WM Uncertainty: 4.566436
01:10:12 [INFO]   Learning Rate: 1.00e-06
01:10:12 [INFO]   Total Steps: 2900
01:10:12 [INFO]   Log Std: mean=-1.9998, std=0.0001
01:10:12 [INFO] ================================================================================
01:10:12 [INFO] Episode 57: reward=101.17, p_loss=0.2910, v_loss=21.6997, entropy=-18.5890, mem_div=-2.5430, wm_unc=4.5664, lr=1.00e-06
Training Episodes:   3%|         | 58/2000 [7:09:48<208:38:02, 386.76s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
01:10:14 [INFO] [Step 0] First memory divergence measurement: 54.2597, setting reward=0
01:11:57 [INFO] [Episode 58] Completed - Detailed Statistics:
01:11:57 [INFO]   Total steps: 50
01:11:57 [INFO]   Episode reward: 97.3611
01:11:57 [INFO]   Avg memory div reward: -2.663748
01:11:57 [INFO]   Avg memory div abs: 54.254834
01:11:57 [INFO]   Avg WM uncertainty: 4.610971
01:11:57 [INFO]   Min/Max reward: 1.4651 / 4.7220
01:13:05 [INFO] [Episode 58] PPO Epoch 0 - Training Losses:
01:13:05 [INFO]   Policy loss: 0.319425
01:13:05 [INFO]   Value loss: 21.120022
01:13:05 [INFO]   Entropy: -18.588069
01:13:05 [INFO]   Total loss: 11.065317
01:14:18 [INFO] Memory Gate WM: 0.001156
01:16:29 [INFO] ================================================================================
01:16:29 [INFO] Episode 58 - Aggregated Metrics Summary:
01:16:29 [INFO]   Reward: 101.1053 (recent avg: 97.0114, std: 4.2873)
01:16:29 [INFO]   Policy Loss: 0.302475 (recent avg: 0.343986)
01:16:29 [INFO]   Value Loss: 21.730724
01:16:29 [INFO]   Entropy: -18.588889
01:16:29 [INFO]   Memory Div Reward: -2.545085 (recent avg: -2.633906)
01:16:29 [INFO]   WM Uncertainty: 4.567191
01:16:29 [INFO]   Learning Rate: 1.00e-06
01:16:29 [INFO]   Total Steps: 2950
01:16:29 [INFO]   Log Std: mean=-1.9998, std=0.0001
01:16:29 [INFO] ================================================================================
01:16:29 [INFO] Episode 58: reward=101.11, p_loss=0.3025, v_loss=21.7307, entropy=-18.5889, mem_div=-2.5451, wm_unc=4.5672, lr=1.00e-06
Training Episodes:   3%|         | 59/2000 [7:16:05<206:59:05, 383.90s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
01:16:32 [INFO] [Step 0] First memory divergence measurement: 54.4162, setting reward=0
01:17:47 [INFO] Memory Gate WM: 0.001161
01:18:11 [INFO] [Episode 59] Completed - Detailed Statistics:
01:18:11 [INFO]   Total steps: 50
01:18:11 [INFO]   Episode reward: 98.0492
01:18:11 [INFO]   Avg memory div reward: -2.670089
01:18:11 [INFO]   Avg memory div abs: 54.445501
01:18:11 [INFO]   Avg WM uncertainty: 4.631072
01:18:11 [INFO]   Min/Max reward: 1.6141 / 4.7929
01:19:19 [INFO] [Episode 59] PPO Epoch 0 - Training Losses:
01:19:19 [INFO]   Policy loss: 0.324468
01:19:19 [INFO]   Value loss: 21.463921
01:19:19 [INFO]   Entropy: -18.587922
01:19:19 [INFO]   Total loss: 11.242307
01:22:45 [INFO] ================================================================================
01:22:45 [INFO] Episode 59 - Aggregated Metrics Summary:
01:22:45 [INFO]   Reward: 101.0544 (recent avg: 97.6103, std: 3.9595)
01:22:45 [INFO]   Policy Loss: 0.302542 (recent avg: 0.357548)
01:22:45 [INFO]   Value Loss: 21.767276
01:22:45 [INFO]   Entropy: -18.588804
01:22:45 [INFO]   Memory Div Reward: -2.547168 (recent avg: -2.639028)
01:22:45 [INFO]   WM Uncertainty: 4.568256
01:22:45 [INFO]   Learning Rate: 1.00e-06
01:22:45 [INFO]   Total Steps: 3000
01:22:45 [INFO]   Log Std: mean=-1.9998, std=0.0001
01:22:45 [INFO] ================================================================================
01:22:45 [INFO] Episode 59: reward=101.05, p_loss=0.3025, v_loss=21.7673, entropy=-18.5888, mem_div=-2.5472, wm_unc=4.5683, lr=1.00e-06
01:22:47 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000060.mp4
Training Episodes:   3%|         | 60/2000 [7:22:23<205:58:28, 382.22s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
01:22:50 [INFO] [Step 0] First memory divergence measurement: 54.5733, setting reward=0
01:24:30 [INFO] [Episode 60] Completed - Detailed Statistics:
01:24:30 [INFO]   Total steps: 50
01:24:30 [INFO]   Episode reward: 100.1508
01:24:30 [INFO]   Avg memory div reward: -2.665041
01:24:30 [INFO]   Avg memory div abs: 54.470542
01:24:30 [INFO]   Avg WM uncertainty: 4.668057
01:24:30 [INFO]   Min/Max reward: 1.6406 / 4.6183
01:25:39 [INFO] [Episode 60] PPO Epoch 0 - Training Losses:
01:25:39 [INFO]   Policy loss: 0.250425
01:25:39 [INFO]   Value loss: 21.903817
01:25:39 [INFO]   Entropy: -18.587909
01:25:39 [INFO]   Total loss: 11.388212
01:29:07 [INFO] ================================================================================
01:29:07 [INFO] Episode 60 - Aggregated Metrics Summary:
01:29:07 [INFO]   Reward: 101.0396 (recent avg: 98.5113, std: 3.3654)
01:29:07 [INFO]   Policy Loss: 0.298066 (recent avg: 0.350179)
01:29:07 [INFO]   Value Loss: 21.681631
01:29:07 [INFO]   Entropy: -18.588728
01:29:07 [INFO]   Memory Div Reward: -2.549101 (recent avg: -2.644224)
01:29:07 [INFO]   WM Uncertainty: 4.569892
01:29:07 [INFO]   Learning Rate: 1.00e-06
01:29:07 [INFO]   Total Steps: 3050
01:29:07 [INFO]   Log Std: mean=-1.9998, std=0.0001
01:29:07 [INFO] ================================================================================
01:29:07 [INFO] Episode 60: reward=101.04, p_loss=0.2981, v_loss=21.6816, entropy=-18.5887, mem_div=-2.5491, wm_unc=4.5699, lr=1.00e-06
Training Episodes:   3%|         | 61/2000 [7:28:43<205:29:04, 381.51s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
01:29:10 [INFO] [Step 0] First memory divergence measurement: 54.3131, setting reward=0
01:29:54 [INFO] Memory Gate WM: 0.001201
01:30:46 [INFO] Memory Gate WM: 0.001201
01:30:52 [INFO] [Episode 61] Completed - Detailed Statistics:
01:30:52 [INFO]   Total steps: 50
01:30:52 [INFO]   Episode reward: 96.5926
01:30:52 [INFO]   Avg memory div reward: -2.658871
01:30:52 [INFO]   Avg memory div abs: 54.320929
01:30:52 [INFO]   Avg WM uncertainty: 4.590722
01:30:52 [INFO]   Min/Max reward: 1.3827 / 4.5074
01:32:02 [INFO] [Episode 61] PPO Epoch 0 - Training Losses:
01:32:02 [INFO]   Policy loss: 0.293008
01:32:02 [INFO]   Value loss: 21.207382
01:32:02 [INFO]   Entropy: -18.587892
01:32:02 [INFO]   Total loss: 11.082578
01:32:17 [INFO] Memory Gate Act: -0.000718
01:35:34 [INFO] ================================================================================
01:35:34 [INFO] Episode 61 - Aggregated Metrics Summary:
01:35:34 [INFO]   Reward: 100.9678 (recent avg: 98.4192, std: 3.4038)
01:35:34 [INFO]   Policy Loss: 0.298489 (recent avg: 0.230928)
01:35:34 [INFO]   Value Loss: 21.656850
01:35:35 [INFO]   Entropy: -18.588653
01:35:35 [INFO]   Memory Div Reward: -2.550871 (recent avg: -2.647617)
01:35:35 [INFO]   WM Uncertainty: 4.570228
01:35:35 [INFO]   Learning Rate: 1.00e-06
01:35:35 [INFO]   Total Steps: 3100
01:35:35 [INFO]   Log Std: mean=-1.9998, std=0.0001
01:35:35 [INFO] ================================================================================
01:35:35 [INFO] Episode 61: reward=100.97, p_loss=0.2985, v_loss=21.6568, entropy=-18.5887, mem_div=-2.5509, wm_unc=4.5702, lr=1.00e-06
Training Episodes:   3%|         | 62/2000 [7:35:10<206:18:21, 383.23s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
01:35:37 [INFO] [Step 0] First memory divergence measurement: 54.8674, setting reward=0
01:36:50 [INFO] Memory Gate Act: -0.000725
01:37:20 [INFO] [Episode 62] Completed - Detailed Statistics:
01:37:20 [INFO]   Total steps: 50
01:37:20 [INFO]   Episode reward: 92.2557
01:37:20 [INFO]   Avg memory div reward: -2.680246
01:37:20 [INFO]   Avg memory div abs: 54.774393
01:37:20 [INFO]   Avg WM uncertainty: 4.525361
01:37:20 [INFO]   Min/Max reward: 1.5531 / 4.6475
01:38:30 [INFO] [Episode 62] PPO Epoch 0 - Training Losses:
01:38:30 [INFO]   Policy loss: 0.122261
01:38:30 [INFO]   Value loss: 19.857737
01:38:30 [INFO]   Entropy: -18.587866
01:38:30 [INFO]   Total loss: 10.237008
01:40:10 [INFO] Memory Gate Act: -0.000738
01:41:38 [INFO] Memory Gate Act: -0.000746
01:41:59 [INFO] ================================================================================
01:41:59 [INFO] Episode 62 - Aggregated Metrics Summary:
01:41:59 [INFO]   Reward: 100.8295 (recent avg: 97.1369, std: 3.0504)
01:41:59 [INFO]   Policy Loss: 0.291212 (recent avg: 0.189531)
01:41:59 [INFO]   Value Loss: 21.602596
01:41:59 [INFO]   Entropy: -18.588579
01:41:59 [INFO]   Memory Div Reward: -2.552925 (recent avg: -2.652955)
01:41:59 [INFO]   WM Uncertainty: 4.569515
01:41:59 [INFO]   Learning Rate: 1.00e-06
01:41:59 [INFO]   Total Steps: 3150
01:41:59 [INFO]   Log Std: mean=-1.9998, std=0.0001
01:41:59 [INFO] ================================================================================
01:41:59 [INFO] Episode 62: reward=100.83, p_loss=0.2912, v_loss=21.6026, entropy=-18.5886, mem_div=-2.5529, wm_unc=4.5695, lr=1.00e-06
Training Episodes:   3%|         | 63/2000 [7:41:35<206:24:47, 383.63s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
01:42:02 [INFO] [Step 0] First memory divergence measurement: 55.0982, setting reward=0
01:43:42 [INFO] Memory Gate Act: -0.000750
01:43:44 [INFO] [Episode 63] Completed - Detailed Statistics:
01:43:44 [INFO]   Total steps: 50
01:43:44 [INFO]   Episode reward: 99.0653
01:43:44 [INFO]   Avg memory div reward: -2.681056
01:43:44 [INFO]   Avg memory div abs: 54.829995
01:43:44 [INFO]   Avg WM uncertainty: 4.662363
01:43:44 [INFO]   Min/Max reward: 1.6766 / 4.9400
01:44:53 [INFO] [Episode 63] PPO Epoch 0 - Training Losses:
01:44:53 [INFO]   Policy loss: 0.615927
01:44:53 [INFO]   Value loss: 21.435013
01:44:53 [INFO]   Entropy: -18.587876
01:44:53 [INFO]   Total loss: 11.519313
01:45:07 [INFO] Memory Gate WM: 0.001279
01:45:40 [INFO] Memory Gate Act: -0.000757
01:46:44 [INFO] Memory Gate Act: -0.000760
01:48:03 [INFO] Memory Gate Act: -0.000761
01:48:22 [INFO] ================================================================================
01:48:22 [INFO] Episode 63 - Aggregated Metrics Summary:
01:48:22 [INFO]   Reward: 100.8020 (recent avg: 97.8718, std: 2.4894)
01:48:22 [INFO]   Policy Loss: 0.297364 (recent avg: 0.262461)
01:48:22 [INFO]   Value Loss: 21.466305
01:48:22 [INFO]   Entropy: -18.588506
01:48:22 [INFO]   Memory Div Reward: -2.554927 (recent avg: -2.657937)
01:48:22 [INFO]   WM Uncertainty: 4.570966
01:48:22 [INFO]   Learning Rate: 1.00e-06
01:48:22 [INFO]   Total Steps: 3200
01:48:22 [INFO]   Log Std: mean=-1.9998, std=0.0001
01:48:22 [INFO] ================================================================================
01:48:22 [INFO] Episode 63: reward=100.80, p_loss=0.2974, v_loss=21.4663, entropy=-18.5885, mem_div=-2.5549, wm_unc=4.5710, lr=1.00e-06
Training Episodes:   3%|         | 64/2000 [7:47:57<206:08:38, 383.33s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
01:48:24 [INFO] [Step 0] First memory divergence measurement: 54.4437, setting reward=0
01:48:58 [INFO] Memory Gate WM: 0.001312
01:49:00 [INFO] Memory Gate WM: 0.001312
01:49:36 [INFO] Memory Gate WM: 0.001312
01:50:06 [INFO] [Episode 64] Completed - Detailed Statistics:
01:50:06 [INFO]   Total steps: 50
01:50:06 [INFO]   Episode reward: 100.3285
01:50:06 [INFO]   Avg memory div reward: -2.684867
01:50:06 [INFO]   Avg memory div abs: 54.637754
01:50:06 [INFO]   Avg WM uncertainty: 4.691437
01:50:06 [INFO]   Min/Max reward: 1.6311 / 4.7269
01:51:15 [INFO] [Episode 64] PPO Epoch 0 - Training Losses:
01:51:15 [INFO]   Policy loss: 0.220797
01:51:15 [INFO]   Value loss: 21.787831
01:51:15 [INFO]   Entropy: -18.587792
01:51:15 [INFO]   Total loss: 11.300590
01:54:46 [INFO] ================================================================================
01:54:46 [INFO] Episode 64 - Aggregated Metrics Summary:
01:54:46 [INFO]   Reward: 100.7947 (recent avg: 98.3289, std: 2.4789)
01:54:46 [INFO]   Policy Loss: 0.285879 (recent avg: 0.265953)
01:54:46 [INFO]   Value Loss: 21.437552
01:54:46 [INFO]   Entropy: -18.588435
01:54:46 [INFO]   Memory Div Reward: -2.556926 (recent avg: -2.662497)
01:54:46 [INFO]   WM Uncertainty: 4.572820
01:54:46 [INFO]   Learning Rate: 1.00e-06
01:54:46 [INFO]   Total Steps: 3250
01:54:46 [INFO]   Log Std: mean=-1.9998, std=0.0001
01:54:46 [INFO] ================================================================================
01:54:46 [INFO] Episode 64: reward=100.79, p_loss=0.2859, v_loss=21.4376, entropy=-18.5884, mem_div=-2.5569, wm_unc=4.5728, lr=1.00e-06
Training Episodes:   3%|         | 65/2000 [7:54:21<206:08:59, 383.53s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
01:54:48 [INFO] [Step 0] First memory divergence measurement: 55.3928, setting reward=0
01:56:31 [INFO] [Episode 65] Completed - Detailed Statistics:
01:56:31 [INFO]   Total steps: 50
01:56:31 [INFO]   Episode reward: 92.6198
01:56:31 [INFO]   Avg memory div reward: -2.709980
01:56:31 [INFO]   Avg memory div abs: 55.329203
01:56:31 [INFO]   Avg WM uncertainty: 4.562375
01:56:31 [INFO]   Min/Max reward: 1.3863 / 4.6559
01:57:42 [INFO] [Episode 65] PPO Epoch 0 - Training Losses:
01:57:42 [INFO]   Policy loss: 0.281072
01:57:42 [INFO]   Value loss: 20.114377
01:57:42 [INFO]   Entropy: -18.587780
01:57:42 [INFO]   Total loss: 10.524138
02:00:10 [INFO] Memory Gate WM: 0.001381
02:01:10 [INFO] Memory Gate Act: -0.000800
02:01:15 [INFO] ================================================================================
02:01:15 [INFO] Episode 65 - Aggregated Metrics Summary:
02:01:15 [INFO]   Reward: 100.6708 (recent avg: 97.5973, std: 2.9344)
02:01:15 [INFO]   Policy Loss: 0.288946 (recent avg: 0.307244)
02:01:15 [INFO]   Value Loss: 21.316029
02:01:15 [INFO]   Entropy: -18.588370
02:01:15 [INFO]   Memory Div Reward: -2.559245 (recent avg: -2.671536)
02:01:15 [INFO]   WM Uncertainty: 4.572661
02:01:15 [INFO]   Learning Rate: 1.00e-06
02:01:15 [INFO]   Total Steps: 3300
02:01:15 [INFO]   Log Std: mean=-1.9998, std=0.0001
02:01:15 [INFO] ================================================================================
02:01:15 [INFO] Episode 65: reward=100.67, p_loss=0.2889, v_loss=21.3160, entropy=-18.5884, mem_div=-2.5592, wm_unc=4.5727, lr=1.00e-06
Training Episodes:   3%|         | 66/2000 [8:00:51<206:57:50, 385.25s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
02:01:18 [INFO] [Step 0] First memory divergence measurement: 55.2563, setting reward=0
02:01:43 [INFO] Memory Gate WM: 0.001398
02:03:00 [INFO] [Episode 66] Completed - Detailed Statistics:
02:03:00 [INFO]   Total steps: 50
02:03:00 [INFO]   Episode reward: 92.3544
02:03:00 [INFO]   Avg memory div reward: -2.723355
02:03:00 [INFO]   Avg memory div abs: 55.449369
02:03:00 [INFO]   Avg WM uncertainty: 4.570442
02:03:00 [INFO]   Min/Max reward: 1.4716 / 4.6101
02:04:11 [INFO] [Episode 66] PPO Epoch 0 - Training Losses:
02:04:11 [INFO]   Policy loss: 0.186816
02:04:11 [INFO]   Value loss: 20.151182
02:04:11 [INFO]   Entropy: -18.587732
02:04:11 [INFO]   Total loss: 10.448284
02:07:46 [INFO] ================================================================================
02:07:46 [INFO] Episode 66 - Aggregated Metrics Summary:
02:07:46 [INFO]   Reward: 100.5467 (recent avg: 96.6833, std: 3.0010)
02:07:46 [INFO]   Policy Loss: 0.284036 (recent avg: 0.285288)
02:07:46 [INFO]   Value Loss: 21.252497
02:07:46 [INFO]   Entropy: -18.588308
02:07:46 [INFO]   Memory Div Reward: -2.561694 (recent avg: -2.679877)
02:07:46 [INFO]   WM Uncertainty: 4.572628
02:07:46 [INFO]   Learning Rate: 1.00e-06
02:07:46 [INFO]   Total Steps: 3350
02:07:46 [INFO]   Log Std: mean=-1.9998, std=0.0001
02:07:46 [INFO] ================================================================================
02:07:46 [INFO] Episode 66: reward=100.55, p_loss=0.2840, v_loss=21.2525, entropy=-18.5883, mem_div=-2.5617, wm_unc=4.5726, lr=1.00e-06
Training Episodes:   3%|         | 67/2000 [8:07:21<207:44:00, 386.88s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
02:07:48 [INFO] [Step 0] First memory divergence measurement: 55.4773, setting reward=0
02:09:30 [INFO] [Episode 67] Completed - Detailed Statistics:
02:09:30 [INFO]   Total steps: 50
02:09:30 [INFO]   Episode reward: 96.1770
02:09:30 [INFO]   Avg memory div reward: -2.726650
02:09:30 [INFO]   Avg memory div abs: 55.566966
02:09:30 [INFO]   Avg WM uncertainty: 4.650190
02:09:30 [INFO]   Min/Max reward: 1.5353 / 4.8834
02:10:39 [INFO] [Episode 67] PPO Epoch 0 - Training Losses:
02:10:39 [INFO]   Policy loss: 0.125236
02:10:39 [INFO]   Value loss: 20.659335
02:10:39 [INFO]   Entropy: -18.587726
02:10:39 [INFO]   Total loss: 10.640781
02:11:04 [INFO] Memory Gate WM: 0.001452
02:12:40 [INFO] Memory Gate WM: 0.001464
02:14:07 [INFO] ================================================================================
02:14:07 [INFO] Episode 67 - Aggregated Metrics Summary:
02:14:07 [INFO]   Reward: 100.4824 (recent avg: 96.4954, std: 2.9678)
02:14:07 [INFO]   Policy Loss: 0.272240 (recent avg: 0.209673)
02:14:07 [INFO]   Value Loss: 21.196966
02:14:07 [INFO]   Entropy: -18.588250
02:14:07 [INFO]   Memory Div Reward: -2.564120 (recent avg: -2.686390)
02:14:07 [INFO]   WM Uncertainty: 4.573769
02:14:07 [INFO]   Learning Rate: 1.00e-06
02:14:07 [INFO]   Total Steps: 3400
02:14:07 [INFO]   Log Std: mean=-1.9998, std=0.0001
02:14:07 [INFO] ================================================================================
02:14:07 [INFO] Episode 67: reward=100.48, p_loss=0.2722, v_loss=21.1970, entropy=-18.5883, mem_div=-2.5641, wm_unc=4.5738, lr=1.00e-06
Training Episodes:   3%|         | 68/2000 [8:13:43<206:46:14, 385.29s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
02:14:10 [INFO] [Step 0] First memory divergence measurement: 55.7769, setting reward=0
02:15:48 [INFO] [Episode 68] Completed - Detailed Statistics:
02:15:48 [INFO]   Total steps: 50
02:15:48 [INFO]   Episode reward: 97.1221
02:15:48 [INFO]   Avg memory div reward: -2.734010
02:15:48 [INFO]   Avg memory div abs: 55.789917
02:15:48 [INFO]   Avg WM uncertainty: 4.676452
02:15:48 [INFO]   Min/Max reward: 1.6944 / 4.6953
02:16:55 [INFO] [Episode 68] PPO Epoch 0 - Training Losses:
02:16:55 [INFO]   Policy loss: 0.822085
02:16:55 [INFO]   Value loss: 21.105508
02:16:55 [INFO]   Entropy: -18.587719
02:16:55 [INFO]   Total loss: 11.560716
02:19:45 [INFO] Memory Gate Act: -0.000855
02:20:21 [INFO] ================================================================================
02:20:21 [INFO] Episode 68 - Aggregated Metrics Summary:
02:20:21 [INFO]   Reward: 100.4337 (recent avg: 96.4715, std: 2.9617)
02:20:21 [INFO]   Policy Loss: 0.277211 (recent avg: 0.270215)
02:20:21 [INFO]   Value Loss: 21.164701
02:20:21 [INFO]   Entropy: -18.588199
02:20:21 [INFO]   Memory Div Reward: -2.566582 (recent avg: -2.693417)
02:20:21 [INFO]   WM Uncertainty: 4.575257
02:20:21 [INFO]   Learning Rate: 1.00e-06
02:20:21 [INFO]   Total Steps: 3450
02:20:21 [INFO]   Log Std: mean=-1.9998, std=0.0001
02:20:21 [INFO] ================================================================================
02:20:21 [INFO] Episode 68: reward=100.43, p_loss=0.2772, v_loss=21.1647, entropy=-18.5882, mem_div=-2.5666, wm_unc=4.5753, lr=1.00e-06
Training Episodes:   3%|         | 69/2000 [8:19:57<204:51:00, 381.91s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
02:20:24 [INFO] [Step 0] First memory divergence measurement: 55.9229, setting reward=0
02:22:05 [INFO] [Episode 69] Completed - Detailed Statistics:
02:22:05 [INFO]   Total steps: 50
02:22:05 [INFO]   Episode reward: 88.8949
02:22:05 [INFO]   Avg memory div reward: -2.733661
02:22:05 [INFO]   Avg memory div abs: 55.814561
02:22:05 [INFO]   Avg WM uncertainty: 4.511559
02:22:05 [INFO]   Min/Max reward: 1.4044 / 4.5469
02:23:14 [INFO] [Episode 69] PPO Epoch 0 - Training Losses:
02:23:14 [INFO]   Policy loss: 0.133693
02:23:14 [INFO]   Value loss: 19.162170
02:23:14 [INFO]   Entropy: -18.587728
02:23:14 [INFO]   Total loss: 9.900655
02:26:48 [INFO] ================================================================================
02:26:48 [INFO] Episode 69 - Aggregated Metrics Summary:
02:26:48 [INFO]   Reward: 100.2689 (recent avg: 95.5561, std: 3.6640)
02:26:48 [INFO]   Policy Loss: 0.274097 (recent avg: 0.248863)
02:26:48 [INFO]   Value Loss: 21.063791
02:26:48 [INFO]   Entropy: -18.588151
02:26:48 [INFO]   Memory Div Reward: -2.568969 (recent avg: -2.699774)
02:26:48 [INFO]   WM Uncertainty: 4.574347
02:26:48 [INFO]   Learning Rate: 1.00e-06
02:26:48 [INFO]   Total Steps: 3500
02:26:48 [INFO]   Log Std: mean=-1.9998, std=0.0001
02:26:48 [INFO] ================================================================================
02:26:48 [INFO] Episode 69: reward=100.27, p_loss=0.2741, v_loss=21.0638, entropy=-18.5882, mem_div=-2.5690, wm_unc=4.5743, lr=1.00e-06
02:26:50 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000070.mp4
Training Episodes:   4%|         | 70/2000 [8:26:25<205:48:47, 383.90s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
02:26:53 [INFO] [Step 0] First memory divergence measurement: 55.9917, setting reward=0
02:27:00 [INFO] Memory Gate Act: -0.000863
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
02:28:35 [INFO] [Episode 70] Completed - Detailed Statistics:
02:28:35 [INFO]   Total steps: 50
02:28:35 [INFO]   Episode reward: 102.5589
02:28:35 [INFO]   Avg memory div reward: -2.734989
02:28:35 [INFO]   Avg memory div abs: 55.821234
02:28:35 [INFO]   Avg WM uncertainty: 4.786167
02:28:35 [INFO]   Min/Max reward: 1.5665 / 4.6596
02:28:53 [INFO] Memory Gate Act: -0.000862
02:29:45 [INFO] [Episode 70] PPO Epoch 0 - Training Losses:
02:29:45 [INFO]   Policy loss: 0.208066
02:29:45 [INFO]   Value loss: 22.315787
02:29:45 [INFO]   Entropy: -18.587689
02:29:45 [INFO]   Total loss: 11.551837
02:31:08 [INFO] Memory Gate Act: -0.000858
02:33:17 [INFO] ================================================================================
02:33:17 [INFO] Episode 70 - Aggregated Metrics Summary:
02:33:17 [INFO]   Reward: 100.3012 (recent avg: 95.7969, std: 4.0199)
02:33:17 [INFO]   Policy Loss: 0.263017 (recent avg: 0.215411)
02:33:17 [INFO]   Value Loss: 21.085006
02:33:17 [INFO]   Entropy: -18.588109
02:33:17 [INFO]   Memory Div Reward: -2.571307 (recent avg: -2.706768)
02:33:17 [INFO]   WM Uncertainty: 4.577330
02:33:17 [INFO]   Learning Rate: 1.00e-06
02:33:17 [INFO]   Total Steps: 3550
02:33:17 [INFO]   Log Std: mean=-1.9998, std=0.0001
02:33:17 [INFO] ================================================================================
02:33:17 [INFO] Episode 70: reward=100.30, p_loss=0.2630, v_loss=21.0850, entropy=-18.5881, mem_div=-2.5713, wm_unc=4.5773, lr=1.00e-06
Training Episodes:   4%|         | 71/2000 [8:32:52<206:10:33, 384.78s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
02:33:19 [INFO] [Step 0] First memory divergence measurement: 56.1169, setting reward=0
02:35:02 [INFO] [Episode 71] Completed - Detailed Statistics:
02:35:02 [INFO]   Total steps: 50
02:35:02 [INFO]   Episode reward: 89.3920
02:35:02 [INFO]   Avg memory div reward: -2.747893
02:35:02 [INFO]   Avg memory div abs: 56.090535
02:35:02 [INFO]   Avg WM uncertainty: 4.535734
02:35:02 [INFO]   Min/Max reward: 1.5468 / 4.5735
02:35:42 [INFO] Memory Gate WM: 0.001520
02:36:12 [INFO] [Episode 71] PPO Epoch 0 - Training Losses:
02:36:12 [INFO]   Policy loss: 0.133990
02:36:12 [INFO]   Value loss: 19.083198
02:36:12 [INFO]   Entropy: -18.587716
02:36:12 [INFO]   Total loss: 9.861466
02:39:42 [INFO] Memory Gate Act: -0.000875
02:39:43 [INFO] ================================================================================
02:39:43 [INFO] Episode 71 - Aggregated Metrics Summary:
02:39:43 [INFO]   Reward: 100.1496 (recent avg: 95.0769, std: 4.4363)
02:39:43 [INFO]   Policy Loss: 0.262317 (recent avg: 0.215333)
02:39:43 [INFO]   Value Loss: 21.010335
02:39:43 [INFO]   Entropy: -18.588068
02:39:43 [INFO]   Memory Div Reward: -2.573760 (recent avg: -2.715671)
02:39:43 [INFO]   WM Uncertainty: 4.576753
02:39:43 [INFO]   Learning Rate: 1.00e-06
02:39:43 [INFO]   Total Steps: 3600
02:39:43 [INFO]   Log Std: mean=-1.9998, std=0.0001
02:39:43 [INFO] ================================================================================
02:39:43 [INFO] Episode 71: reward=100.15, p_loss=0.2623, v_loss=21.0103, entropy=-18.5881, mem_div=-2.5738, wm_unc=4.5768, lr=1.00e-06
Training Episodes:   4%|         | 72/2000 [8:39:19<206:24:11, 385.40s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
02:39:46 [INFO] [Step 0] First memory divergence measurement: 56.2864, setting reward=0
02:40:27 [INFO] Memory Gate Act: -0.000875
02:41:06 [INFO] Memory Gate WM: 0.001565
02:41:29 [INFO] [Episode 72] Completed - Detailed Statistics:
02:41:29 [INFO]   Total steps: 50
02:41:29 [INFO]   Episode reward: 86.0021
02:41:29 [INFO]   Avg memory div reward: -2.754849
02:41:29 [INFO]   Avg memory div abs: 56.250344
02:41:29 [INFO]   Avg WM uncertainty: 4.474890
02:41:29 [INFO]   Min/Max reward: 1.2709 / 4.5335
02:42:40 [INFO] [Episode 72] PPO Epoch 0 - Training Losses:
02:42:40 [INFO]   Policy loss: 0.226357
02:42:40 [INFO]   Value loss: 18.493284
02:42:40 [INFO]   Entropy: -18.587672
02:42:40 [INFO]   Total loss: 9.658876
02:45:25 [INFO] Memory Gate Act: -0.000884
02:46:13 [INFO] ================================================================================
02:46:13 [INFO] Episode 72 - Aggregated Metrics Summary:
02:46:13 [INFO]   Reward: 99.9558 (recent avg: 94.4515, std: 5.1700)
02:46:13 [INFO]   Policy Loss: 0.260902 (recent avg: 0.221151)
02:46:13 [INFO]   Value Loss: 20.870600
02:46:13 [INFO]   Entropy: -18.588027
02:46:13 [INFO]   Memory Div Reward: -2.576241 (recent avg: -2.723131)
02:46:13 [INFO]   WM Uncertainty: 4.575357
02:46:13 [INFO]   Learning Rate: 1.00e-06
02:46:13 [INFO]   Total Steps: 3650
02:46:13 [INFO]   Log Std: mean=-1.9998, std=0.0001
02:46:13 [INFO] ================================================================================
02:46:13 [INFO] Episode 72: reward=99.96, p_loss=0.2609, v_loss=20.8706, entropy=-18.5880, mem_div=-2.5762, wm_unc=4.5754, lr=1.00e-06
Training Episodes:   4%|         | 73/2000 [8:45:49<207:00:04, 386.72s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
02:46:16 [INFO] [Step 0] First memory divergence measurement: 56.2413, setting reward=0
02:47:57 [INFO] [Episode 73] Completed - Detailed Statistics:
02:47:57 [INFO]   Total steps: 50
02:47:57 [INFO]   Episode reward: 95.3954
02:47:57 [INFO]   Avg memory div reward: -2.748386
02:47:57 [INFO]   Avg memory div abs: 56.069212
02:47:57 [INFO]   Avg WM uncertainty: 4.656295
02:47:57 [INFO]   Min/Max reward: 1.4104 / 4.6712
02:48:01 [INFO] Memory Gate Act: -0.000888
02:48:34 [INFO] Memory Gate Act: -0.000891
02:49:07 [INFO] [Episode 73] PPO Epoch 0 - Training Losses:
02:49:07 [INFO]   Policy loss: 0.222586
02:49:07 [INFO]   Value loss: 20.602792
02:49:07 [INFO]   Entropy: -18.587656
02:49:07 [INFO]   Total loss: 10.709859
02:51:58 [INFO] Memory Gate WM: 0.001600
02:52:01 [INFO] Memory Gate WM: 0.001601
02:52:41 [INFO] ================================================================================
02:52:41 [INFO] Episode 73 - Aggregated Metrics Summary:
02:52:41 [INFO]   Reward: 99.8942 (recent avg: 94.0845, std: 4.9552)
02:52:41 [INFO]   Policy Loss: 0.259615 (recent avg: 0.210534)
02:52:41 [INFO]   Value Loss: 20.838892
02:52:41 [INFO]   Entropy: -18.587991
02:52:41 [INFO]   Memory Div Reward: -2.578567 (recent avg: -2.729864)
02:52:41 [INFO]   WM Uncertainty: 4.576451
02:52:41 [INFO]   Learning Rate: 1.00e-06
02:52:41 [INFO]   Total Steps: 3700
02:52:41 [INFO]   Log Std: mean=-1.9998, std=0.0001
02:52:41 [INFO] ================================================================================
02:52:41 [INFO] Episode 73: reward=99.89, p_loss=0.2596, v_loss=20.8389, entropy=-18.5880, mem_div=-2.5786, wm_unc=4.5765, lr=1.00e-06
Training Episodes:   4%|         | 74/2000 [8:52:16<207:00:55, 386.94s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
02:52:44 [INFO] [Step 0] First memory divergence measurement: 56.0469, setting reward=0
02:52:45 [INFO] Memory Gate Act: -0.000907
02:54:24 [INFO] [Episode 74] Completed - Detailed Statistics:
02:54:24 [INFO]   Total steps: 50
02:54:24 [INFO]   Episode reward: 98.1305
02:54:24 [INFO]   Avg memory div reward: -2.755280
02:54:24 [INFO]   Avg memory div abs: 56.143050
02:54:24 [INFO]   Avg WM uncertainty: 4.717891
02:54:24 [INFO]   Min/Max reward: 1.4808 / 4.7727
02:55:32 [INFO] [Episode 74] PPO Epoch 0 - Training Losses:
02:55:32 [INFO]   Policy loss: 0.195636
02:55:32 [INFO]   Value loss: 21.225563
02:55:32 [INFO]   Entropy: -18.587680
02:55:32 [INFO]   Total loss: 10.994294
02:57:35 [INFO] Memory Gate Act: -0.000905
02:58:38 [INFO] Memory Gate WM: 0.001627
02:59:00 [INFO] ================================================================================
02:59:00 [INFO] Episode 74 - Aggregated Metrics Summary:
02:59:00 [INFO]   Reward: 99.8707 (recent avg: 93.8647, std: 4.7164)
02:59:00 [INFO]   Policy Loss: 0.256739 (recent avg: 0.216452)
02:59:00 [INFO]   Value Loss: 20.886245
02:59:00 [INFO]   Entropy: -18.587953
02:59:00 [INFO]   Memory Div Reward: -2.580923 (recent avg: -2.736905)
02:59:00 [INFO]   WM Uncertainty: 4.578337
02:59:00 [INFO]   Learning Rate: 1.00e-06
02:59:00 [INFO]   Total Steps: 3750
02:59:00 [INFO]   Log Std: mean=-1.9998, std=0.0001
02:59:00 [INFO] ================================================================================
02:59:00 [INFO] Episode 74: reward=99.87, p_loss=0.2567, v_loss=20.8862, entropy=-18.5880, mem_div=-2.5809, wm_unc=4.5783, lr=1.00e-06
Training Episodes:   4%|         | 75/2000 [8:58:35<205:37:54, 384.56s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
02:59:02 [INFO] [Step 0] First memory divergence measurement: 56.3155, setting reward=0
03:00:41 [INFO] [Episode 75] Completed - Detailed Statistics:
03:00:41 [INFO]   Total steps: 50
03:00:41 [INFO]   Episode reward: 97.3509
03:00:41 [INFO]   Avg memory div reward: -2.748556
03:00:41 [INFO]   Avg memory div abs: 56.202299
03:00:41 [INFO]   Avg WM uncertainty: 4.695573
03:00:41 [INFO]   Min/Max reward: 1.5185 / 4.6556
03:01:41 [INFO] Memory Gate WM: 0.001632
03:01:49 [INFO] [Episode 75] PPO Epoch 0 - Training Losses:
03:01:49 [INFO]   Policy loss: 0.212931
03:01:49 [INFO]   Value loss: 21.241686
03:01:49 [INFO]   Entropy: -18.587637
03:01:49 [INFO]   Total loss: 11.019650
03:02:30 [INFO] Memory Gate WM: 0.001624
03:03:47 [INFO] Memory Gate Act: -0.000925
03:04:44 [INFO] Memory Gate WM: 0.001636
03:05:15 [INFO] ================================================================================
03:05:15 [INFO] Episode 75 - Aggregated Metrics Summary:
03:05:15 [INFO]   Reward: 99.8375 (recent avg: 94.3378, std: 4.8042)
03:05:15 [INFO]   Policy Loss: 0.255462 (recent avg: 0.223538)
03:05:15 [INFO]   Value Loss: 20.944545
03:05:15 [INFO]   Entropy: -18.587917
03:05:15 [INFO]   Memory Div Reward: -2.583129 (recent avg: -2.740763)
03:05:15 [INFO]   WM Uncertainty: 4.579880
03:05:15 [INFO]   Learning Rate: 1.00e-06
03:05:15 [INFO]   Total Steps: 3800
03:05:15 [INFO]   Log Std: mean=-1.9998, std=0.0001
03:05:15 [INFO] ================================================================================
03:05:15 [INFO] Episode 75: reward=99.84, p_loss=0.2555, v_loss=20.9445, entropy=-18.5879, mem_div=-2.5831, wm_unc=4.5799, lr=1.00e-06
Training Episodes:   4%|         | 76/2000 [9:04:51<204:06:37, 381.91s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
03:05:18 [INFO] [Step 0] First memory divergence measurement: 56.6004, setting reward=0
03:06:56 [INFO] [Episode 76] Completed - Detailed Statistics:
03:06:56 [INFO]   Total steps: 50
03:06:56 [INFO]   Episode reward: 86.2920
03:06:56 [INFO]   Avg memory div reward: -2.778169
03:06:56 [INFO]   Avg memory div abs: 56.667973
03:06:56 [INFO]   Avg WM uncertainty: 4.504010
03:06:56 [INFO]   Min/Max reward: 1.3815 / 4.7677
03:07:19 [INFO] Memory Gate WM: 0.001637
03:08:03 [INFO] [Episode 76] PPO Epoch 0 - Training Losses:
03:08:03 [INFO]   Policy loss: 0.321833
03:08:03 [INFO]   Value loss: 18.508246
03:08:03 [INFO]   Entropy: -18.587604
03:08:03 [INFO]   Total loss: 9.761832
03:10:17 [INFO] Memory Gate Act: -0.000951
03:11:32 [INFO] ================================================================================
03:11:32 [INFO] Episode 76 - Aggregated Metrics Summary:
03:11:32 [INFO]   Reward: 99.6616 (recent avg: 93.7316, std: 5.3659)
03:11:32 [INFO]   Policy Loss: 0.253540 (recent avg: 0.220160)
03:11:32 [INFO]   Value Loss: 20.826916
03:11:32 [INFO]   Entropy: -18.587882
03:11:32 [INFO]   Memory Div Reward: -2.585662 (recent avg: -2.746244)
03:11:32 [INFO]   WM Uncertainty: 4.578894
03:11:32 [INFO]   Learning Rate: 1.00e-06
03:11:32 [INFO]   Total Steps: 3850
03:11:32 [INFO]   Log Std: mean=-1.9998, std=0.0001
03:11:32 [INFO] ================================================================================
03:11:32 [INFO] Episode 76: reward=99.66, p_loss=0.2535, v_loss=20.8269, entropy=-18.5879, mem_div=-2.5857, wm_unc=4.5789, lr=1.00e-06
Training Episodes:   4%|         | 77/2000 [9:11:08<203:13:28, 380.45s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
03:11:35 [INFO] [Step 0] First memory divergence measurement: 56.0678, setting reward=0
03:12:15 [INFO] Memory Gate WM: 0.001654
03:12:17 [INFO] Memory Gate Act: -0.000959
03:12:17 [INFO] Memory Gate Act: -0.000959
03:13:15 [INFO] [Episode 77] Completed - Detailed Statistics:
03:13:15 [INFO]   Total steps: 50
03:13:15 [INFO]   Episode reward: 96.8302
03:13:15 [INFO]   Avg memory div reward: -2.776780
03:13:15 [INFO]   Avg memory div abs: 56.470066
03:13:15 [INFO]   Avg WM uncertainty: 4.713383
03:13:15 [INFO]   Min/Max reward: 1.5496 / 4.7008
03:14:21 [INFO] [Episode 77] PPO Epoch 0 - Training Losses:
03:14:21 [INFO]   Policy loss: 0.266485
03:14:21 [INFO]   Value loss: 20.831109
03:14:21 [INFO]   Entropy: -18.587524
03:14:21 [INFO]   Total loss: 10.867914
03:17:23 [INFO] Memory Gate WM: 0.001698
03:17:43 [INFO] ================================================================================
03:17:43 [INFO] Episode 77 - Aggregated Metrics Summary:
03:17:43 [INFO]   Reward: 99.6253 (recent avg: 93.7969, std: 5.3992)
03:17:43 [INFO]   Policy Loss: 0.253099 (recent avg: 0.219697)
03:17:43 [INFO]   Value Loss: 20.736650
03:17:43 [INFO]   Entropy: -18.587845
03:17:43 [INFO]   Memory Div Reward: -2.588112 (recent avg: -2.751257)
03:17:43 [INFO]   WM Uncertainty: 4.580618
03:17:43 [INFO]   Learning Rate: 1.00e-06
03:17:43 [INFO]   Total Steps: 3900
03:17:43 [INFO]   Log Std: mean=-1.9998, std=0.0001
03:17:43 [INFO] ================================================================================
03:17:43 [INFO] Episode 77: reward=99.63, p_loss=0.2531, v_loss=20.7367, entropy=-18.5878, mem_div=-2.5881, wm_unc=4.5806, lr=1.00e-06
Training Episodes:   4%|         | 78/2000 [9:17:18<201:27:05, 377.33s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
03:17:45 [INFO] [Step 0] First memory divergence measurement: 56.6805, setting reward=0
03:19:24 [INFO] [Episode 78] Completed - Detailed Statistics:
03:19:24 [INFO]   Total steps: 50
03:19:24 [INFO]   Episode reward: 92.6497
03:19:24 [INFO]   Avg memory div reward: -2.780734
03:19:24 [INFO]   Avg memory div abs: 56.720188
03:19:24 [INFO]   Avg WM uncertainty: 4.633728
03:19:24 [INFO]   Min/Max reward: 1.4411 / 4.3898
03:20:29 [INFO] [Episode 78] PPO Epoch 0 - Training Losses:
03:20:29 [INFO]   Policy loss: 0.192733
03:20:29 [INFO]   Value loss: 19.846452
03:20:29 [INFO]   Entropy: -18.587451
03:20:29 [INFO]   Total loss: 10.301834
03:23:16 [INFO] Memory Gate WM: 0.001714
03:23:46 [INFO] ================================================================================
03:23:46 [INFO] Episode 78 - Aggregated Metrics Summary:
03:23:46 [INFO]   Reward: 99.5370 (recent avg: 93.3497, std: 5.2893)
03:23:46 [INFO]   Policy Loss: 0.248300 (recent avg: 0.196809)
03:23:46 [INFO]   Value Loss: 20.730539
03:23:46 [INFO]   Entropy: -18.587807
03:23:46 [INFO]   Memory Div Reward: -2.590550 (recent avg: -2.755930)
03:23:46 [INFO]   WM Uncertainty: 4.581291
03:23:46 [INFO]   Learning Rate: 1.00e-06
03:23:46 [INFO]   Total Steps: 3950
03:23:46 [INFO]   Log Std: mean=-1.9998, std=0.0001
03:23:46 [INFO] ================================================================================
03:23:46 [INFO] Episode 78: reward=99.54, p_loss=0.2483, v_loss=20.7305, entropy=-18.5878, mem_div=-2.5906, wm_unc=4.5813, lr=1.00e-06
Training Episodes:   4%|         | 79/2000 [9:23:21<199:03:34, 373.04s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
03:23:48 [INFO] [Step 0] First memory divergence measurement: 56.3279, setting reward=0
03:25:28 [INFO] [Episode 79] Completed - Detailed Statistics:
03:25:28 [INFO]   Total steps: 50
03:25:28 [INFO]   Episode reward: 89.4986
03:25:28 [INFO]   Avg memory div reward: -2.773407
03:25:28 [INFO]   Avg memory div abs: 56.439473
03:25:28 [INFO]   Avg WM uncertainty: 4.563378
03:25:28 [INFO]   Min/Max reward: 1.2927 / 4.9396
03:26:36 [INFO] [Episode 79] PPO Epoch 0 - Training Losses:
03:26:36 [INFO]   Policy loss: 0.345724
03:26:36 [INFO]   Value loss: 18.760521
03:26:36 [INFO]   Entropy: -18.587368
03:26:36 [INFO]   Total loss: 9.911859
03:30:01 [INFO] ================================================================================
03:30:01 [INFO] Episode 79 - Aggregated Metrics Summary:
03:30:01 [INFO]   Reward: 99.4115 (recent avg: 93.4100, std: 5.2414)
03:30:01 [INFO]   Policy Loss: 0.247936 (recent avg: 0.198273)
03:30:01 [INFO]   Value Loss: 20.646956
03:30:01 [INFO]   Entropy: -18.587769
03:30:01 [INFO]   Memory Div Reward: -2.592836 (recent avg: -2.759904)
03:30:01 [INFO]   WM Uncertainty: 4.581067
03:30:01 [INFO]   Learning Rate: 1.00e-06
03:30:01 [INFO]   Total Steps: 4000
03:30:01 [INFO]   Log Std: mean=-1.9998, std=0.0001
03:30:01 [INFO] ================================================================================
03:30:01 [INFO] Episode 79: reward=99.41, p_loss=0.2479, v_loss=20.6470, entropy=-18.5878, mem_div=-2.5928, wm_unc=4.5811, lr=1.00e-06
03:30:03 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000080.mp4
Training Episodes:   4%|         | 80/2000 [9:29:39<199:41:43, 374.43s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
03:30:06 [INFO] [Step 0] First memory divergence measurement: 56.7671, setting reward=0
03:31:37 [INFO] Memory Gate WM: 0.001687
03:31:44 [INFO] [Episode 80] Completed - Detailed Statistics:
03:31:44 [INFO]   Total steps: 50
03:31:44 [INFO]   Episode reward: 93.0665
03:31:44 [INFO]   Avg memory div reward: -2.778514
03:31:44 [INFO]   Avg memory div abs: 56.770930
03:31:44 [INFO]   Avg WM uncertainty: 4.639844
03:31:44 [INFO]   Min/Max reward: 1.3212 / 4.7769
03:32:49 [INFO] [Episode 80] PPO Epoch 0 - Training Losses:
03:32:49 [INFO]   Policy loss: 0.240010
03:32:49 [INFO]   Value loss: 19.653702
03:32:49 [INFO]   Entropy: -18.587287
03:32:49 [INFO]   Total loss: 10.252734
03:35:35 [INFO] Memory Gate WM: 0.001693
03:35:48 [INFO] Memory Gate WM: 0.001694
03:36:05 [INFO] ================================================================================
03:36:05 [INFO] Episode 80 - Aggregated Metrics Summary:
03:36:05 [INFO]   Reward: 99.3332 (recent avg: 92.4608, std: 4.2676)
03:36:05 [INFO]   Policy Loss: 0.248063 (recent avg: 0.213391)
03:36:05 [INFO]   Value Loss: 20.549584
03:36:05 [INFO]   Entropy: -18.587732
03:36:05 [INFO]   Memory Div Reward: -2.595128 (recent avg: -2.764257)
03:36:05 [INFO]   WM Uncertainty: 4.581792
03:36:05 [INFO]   Learning Rate: 1.00e-06
03:36:05 [INFO]   Total Steps: 4050
03:36:05 [INFO]   Log Std: mean=-1.9998, std=0.0001
03:36:05 [INFO] ================================================================================
03:36:05 [INFO] Episode 80: reward=99.33, p_loss=0.2481, v_loss=20.5496, entropy=-18.5877, mem_div=-2.5951, wm_unc=4.5818, lr=1.00e-06
Training Episodes:   4%|         | 81/2000 [9:35:41<197:37:28, 370.74s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
03:36:08 [INFO] [Step 0] First memory divergence measurement: 56.4862, setting reward=0
03:37:35 [INFO] Memory Gate WM: 0.001696
03:37:45 [INFO] [Episode 81] Completed - Detailed Statistics:
03:37:45 [INFO]   Total steps: 50
03:37:45 [INFO]   Episode reward: 85.9370
03:37:45 [INFO]   Avg memory div reward: -2.766202
03:37:45 [INFO]   Avg memory div abs: 56.463062
03:37:45 [INFO]   Avg WM uncertainty: 4.484941
03:37:45 [INFO]   Min/Max reward: 1.2878 / 4.5243
03:38:50 [INFO] [Episode 81] PPO Epoch 0 - Training Losses:
03:38:50 [INFO]   Policy loss: 0.214910
03:38:50 [INFO]   Value loss: 18.356693
03:38:50 [INFO]   Entropy: -18.587256
03:38:50 [INFO]   Total loss: 9.579128
03:42:09 [INFO] ================================================================================
03:42:09 [INFO] Episode 81 - Aggregated Metrics Summary:
03:42:09 [INFO]   Reward: 99.1698 (recent avg: 92.1153, std: 4.6268)
03:42:09 [INFO]   Policy Loss: 0.246274 (recent avg: 0.218617)
03:42:09 [INFO]   Value Loss: 20.395091
03:42:09 [INFO]   Entropy: -18.587695
03:42:09 [INFO]   Memory Div Reward: -2.597214 (recent avg: -2.766088)
03:42:09 [INFO]   WM Uncertainty: 4.580611
03:42:09 [INFO]   Learning Rate: 1.00e-06
03:42:09 [INFO]   Total Steps: 4100
03:42:09 [INFO]   Log Std: mean=-1.9998, std=0.0001
03:42:09 [INFO] ================================================================================
03:42:09 [INFO] Episode 81: reward=99.17, p_loss=0.2463, v_loss=20.3951, entropy=-18.5877, mem_div=-2.5972, wm_unc=4.5806, lr=1.00e-06
Training Episodes:   4%|         | 82/2000 [9:41:44<196:19:36, 368.50s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
03:42:11 [INFO] [Step 0] First memory divergence measurement: 56.6451, setting reward=0
03:43:46 [INFO] [Episode 82] Completed - Detailed Statistics:
03:43:46 [INFO]   Total steps: 50
03:43:46 [INFO]   Episode reward: 92.5703
03:43:46 [INFO]   Avg memory div reward: -2.756470
03:43:46 [INFO]   Avg memory div abs: 56.357435
03:43:46 [INFO]   Avg WM uncertainty: 4.607875
03:43:46 [INFO]   Min/Max reward: 1.3748 / 4.6608
03:44:52 [INFO] [Episode 82] PPO Epoch 0 - Training Losses:
03:44:52 [INFO]   Policy loss: 0.180773
03:44:52 [INFO]   Value loss: 20.079689
03:44:52 [INFO]   Entropy: -18.587255
03:44:52 [INFO]   Total loss: 10.406490
03:48:13 [INFO] ================================================================================
03:48:13 [INFO] Episode 82 - Aggregated Metrics Summary:
03:48:13 [INFO]   Reward: 99.0903 (recent avg: 92.7721, std: 4.1545)
03:48:13 [INFO]   Policy Loss: 0.246449 (recent avg: 0.225549)
03:48:13 [INFO]   Value Loss: 20.351447
03:48:13 [INFO]   Entropy: -18.587660
03:48:13 [INFO]   Memory Div Reward: -2.599133 (recent avg: -2.766250)
03:48:13 [INFO]   WM Uncertainty: 4.580940
03:48:13 [INFO]   Learning Rate: 1.00e-06
03:48:13 [INFO]   Total Steps: 4150
03:48:13 [INFO]   Log Std: mean=-1.9998, std=0.0001
03:48:13 [INFO] ================================================================================
03:48:13 [INFO] Episode 82: reward=99.09, p_loss=0.2464, v_loss=20.3514, entropy=-18.5877, mem_div=-2.5991, wm_unc=4.5809, lr=1.00e-06
Training Episodes:   4%|         | 83/2000 [9:47:49<195:37:31, 367.37s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
03:48:16 [INFO] [Step 0] First memory divergence measurement: 56.5146, setting reward=0
03:49:34 [INFO] Memory Gate WM: 0.001671
03:49:53 [INFO] [Episode 83] Completed - Detailed Statistics:
03:49:53 [INFO]   Total steps: 50
03:49:53 [INFO]   Episode reward: 90.0357
03:49:53 [INFO]   Avg memory div reward: -2.770887
03:49:53 [INFO]   Avg memory div abs: 56.544448
03:49:53 [INFO]   Avg WM uncertainty: 4.571601
03:49:53 [INFO]   Min/Max reward: 1.4784 / 4.5781
03:50:26 [INFO] Memory Gate Act: -0.000987
03:51:01 [INFO] [Episode 83] PPO Epoch 0 - Training Losses:
03:51:01 [INFO]   Policy loss: 0.222426
03:51:01 [INFO]   Value loss: 19.363761
03:51:01 [INFO]   Entropy: -18.587177
03:51:01 [INFO]   Total loss: 10.090178
03:52:13 [INFO] Memory Gate Act: -0.000991
03:52:16 [INFO] Memory Gate WM: 0.001675
03:54:21 [INFO] ================================================================================
03:54:21 [INFO] Episode 83 - Aggregated Metrics Summary:
03:54:21 [INFO]   Reward: 98.9825 (recent avg: 92.2361, std: 4.1271)
03:54:21 [INFO]   Policy Loss: 0.233311 (recent avg: 0.214207)
03:54:21 [INFO]   Value Loss: 20.281194
03:54:21 [INFO]   Entropy: -18.587625
03:54:21 [INFO]   Memory Div Reward: -2.601178 (recent avg: -2.768500)
03:54:21 [INFO]   WM Uncertainty: 4.580829
03:54:21 [INFO]   Learning Rate: 1.00e-06
03:54:21 [INFO]   Total Steps: 4200
03:54:21 [INFO]   Log Std: mean=-1.9998, std=0.0001
03:54:21 [INFO] ================================================================================
03:54:21 [INFO] Episode 83: reward=98.98, p_loss=0.2333, v_loss=20.2812, entropy=-18.5876, mem_div=-2.6012, wm_unc=4.5808, lr=1.00e-06
Training Episodes:   4%|         | 84/2000 [9:53:56<195:31:01, 367.36s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
03:54:23 [INFO] [Step 0] First memory divergence measurement: 56.7966, setting reward=0
03:55:44 [INFO] Memory Gate Act: -0.001005
03:55:59 [INFO] [Episode 84] Completed - Detailed Statistics:
03:55:59 [INFO]   Total steps: 50
03:55:59 [INFO]   Episode reward: 90.4434
03:55:59 [INFO]   Avg memory div reward: -2.784032
03:55:59 [INFO]   Avg memory div abs: 56.826178
03:55:59 [INFO]   Avg WM uncertainty: 4.592900
03:55:59 [INFO]   Min/Max reward: 1.4274 / 4.4795
03:56:38 [INFO] Memory Gate Act: -0.001010
03:57:03 [INFO] [Episode 84] PPO Epoch 0 - Training Losses:
03:57:03 [INFO]   Policy loss: 0.186337
03:57:03 [INFO]   Value loss: 19.655784
03:57:03 [INFO]   Entropy: -18.587142
03:57:03 [INFO]   Total loss: 10.200100
03:59:34 [INFO] Memory Gate Act: -0.001022
04:00:16 [INFO] ================================================================================
04:00:16 [INFO] Episode 84 - Aggregated Metrics Summary:
04:00:16 [INFO]   Reward: 98.8821 (recent avg: 91.4674, std: 3.6454)
04:00:16 [INFO]   Policy Loss: 0.229651 (recent avg: 0.200802)
04:00:16 [INFO]   Value Loss: 20.208892
04:00:16 [INFO]   Entropy: -18.587594
04:00:16 [INFO]   Memory Div Reward: -2.603329 (recent avg: -2.771375)
04:00:16 [INFO]   WM Uncertainty: 4.580971
04:00:16 [INFO]   Learning Rate: 1.00e-06
04:00:16 [INFO]   Total Steps: 4250
04:00:16 [INFO]   Log Std: mean=-1.9998, std=0.0001
04:00:16 [INFO] ================================================================================
04:00:16 [INFO] Episode 84: reward=98.88, p_loss=0.2297, v_loss=20.2089, entropy=-18.5876, mem_div=-2.6033, wm_unc=4.5810, lr=1.00e-06
Training Episodes:   4%|         | 85/2000 [9:59:52<193:28:19, 363.71s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
04:00:17 [INFO] Memory Gate Act: -0.001017
04:00:19 [INFO] [Step 0] First memory divergence measurement: 56.4971, setting reward=0
04:01:56 [INFO] [Episode 85] Completed - Detailed Statistics:
04:01:56 [INFO]   Total steps: 50
04:01:56 [INFO]   Episode reward: 89.6773
04:01:56 [INFO]   Avg memory div reward: -2.761281
04:01:56 [INFO]   Avg memory div abs: 56.473302
04:01:56 [INFO]   Avg WM uncertainty: 4.554826
04:01:56 [INFO]   Min/Max reward: 1.3807 / 4.7332
04:03:02 [INFO] [Episode 85] PPO Epoch 0 - Training Losses:
04:03:02 [INFO]   Policy loss: 0.120884
04:03:02 [INFO]   Value loss: 19.293401
04:03:02 [INFO]   Entropy: -18.587148
04:03:02 [INFO]   Total loss: 9.953455
04:04:06 [INFO] Memory Gate Act: -0.001018
04:06:22 [INFO] ================================================================================
04:06:22 [INFO] Episode 85 - Aggregated Metrics Summary:
04:06:22 [INFO]   Reward: 98.7750 (recent avg: 90.7001, std: 3.0918)
04:06:22 [INFO]   Policy Loss: 0.226026 (recent avg: 0.145420)
04:06:22 [INFO]   Value Loss: 20.104464
04:06:22 [INFO]   Entropy: -18.587563
04:06:22 [INFO]   Memory Div Reward: -2.605166 (recent avg: -2.772647)
04:06:22 [INFO]   WM Uncertainty: 4.580667
04:06:22 [INFO]   Learning Rate: 1.00e-06
04:06:22 [INFO]   Total Steps: 4300
04:06:22 [INFO]   Log Std: mean=-1.9998, std=0.0001
04:06:22 [INFO] ================================================================================
04:06:22 [INFO] Episode 85: reward=98.78, p_loss=0.2260, v_loss=20.1045, entropy=-18.5876, mem_div=-2.6052, wm_unc=4.5807, lr=1.00e-06
Training Episodes:   4%|         | 86/2000 [10:05:58<193:48:44, 364.54s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
04:06:25 [INFO] [Step 0] First memory divergence measurement: 56.6345, setting reward=0
04:08:04 [INFO] [Episode 86] Completed - Detailed Statistics:
04:08:04 [INFO]   Total steps: 50
04:08:04 [INFO]   Episode reward: 92.8423
04:08:04 [INFO]   Avg memory div reward: -2.777085
04:08:04 [INFO]   Avg memory div abs: 56.654536
04:08:04 [INFO]   Avg WM uncertainty: 4.633930
04:08:04 [INFO]   Min/Max reward: 1.3325 / 4.4842
04:08:09 [INFO] Memory Gate WM: 0.001717
04:08:48 [INFO] Memory Gate Act: -0.000996
04:09:12 [INFO] [Episode 86] PPO Epoch 0 - Training Losses:
04:09:12 [INFO]   Policy loss: 0.199111
04:09:12 [INFO]   Value loss: 19.885691
04:09:12 [INFO]   Entropy: -18.587105
04:09:12 [INFO]   Total loss: 10.327828
04:11:12 [INFO] Memory Gate WM: 0.001694
04:12:32 [INFO] ================================================================================
04:12:32 [INFO] Episode 86 - Aggregated Metrics Summary:
04:12:32 [INFO]   Reward: 98.7069 (recent avg: 91.3551, std: 2.7651)
04:12:32 [INFO]   Policy Loss: 0.225062 (recent avg: 0.174680)
04:12:32 [INFO]   Value Loss: 20.051584
04:12:32 [INFO]   Entropy: -18.587532
04:12:32 [INFO]   Memory Div Reward: -2.607142 (recent avg: -2.772539)
04:12:32 [INFO]   WM Uncertainty: 4.581279
04:12:32 [INFO]   Learning Rate: 1.00e-06
04:12:32 [INFO]   Total Steps: 4350
04:12:32 [INFO]   Log Std: mean=-1.9998, std=0.0001
04:12:32 [INFO] ================================================================================
04:12:32 [INFO] Episode 86: reward=98.71, p_loss=0.2251, v_loss=20.0516, entropy=-18.5875, mem_div=-2.6071, wm_unc=4.5813, lr=1.00e-06
Training Episodes:   4%|         | 87/2000 [10:12:08<194:34:15, 366.16s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
04:12:35 [INFO] [Step 0] First memory divergence measurement: 56.5587, setting reward=0
04:14:13 [INFO] [Episode 87] Completed - Detailed Statistics:
04:14:13 [INFO]   Total steps: 50
04:14:13 [INFO]   Episode reward: 82.9912
04:14:13 [INFO]   Avg memory div reward: -2.772999
04:14:13 [INFO]   Avg memory div abs: 56.563980
04:14:13 [INFO]   Avg WM uncertainty: 4.432823
04:14:13 [INFO]   Min/Max reward: 1.0617 / 4.6331
04:14:19 [INFO] Memory Gate Act: -0.000975
04:14:32 [INFO] Memory Gate WM: 0.001686
04:15:18 [INFO] [Episode 87] PPO Epoch 0 - Training Losses:
04:15:18 [INFO]   Policy loss: 0.499235
04:15:18 [INFO]   Value loss: 17.496346
04:15:18 [INFO]   Entropy: -18.587089
04:15:18 [INFO]   Total loss: 9.433279
04:15:34 [INFO] Memory Gate Act: -0.000976
04:18:33 [INFO] ================================================================================
04:18:33 [INFO] Episode 87 - Aggregated Metrics Summary:
04:18:33 [INFO]   Reward: 98.5283 (recent avg: 89.9712, std: 3.1191)
04:18:33 [INFO]   Policy Loss: 0.232624 (recent avg: 0.240895)
04:18:33 [INFO]   Value Loss: 19.957197
04:18:33 [INFO]   Entropy: -18.587496
04:18:33 [INFO]   Memory Div Reward: -2.609027 (recent avg: -2.772161)
04:18:33 [INFO]   WM Uncertainty: 4.579592
04:18:33 [INFO]   Learning Rate: 1.00e-06
04:18:33 [INFO]   Total Steps: 4400
04:18:33 [INFO]   Log Std: mean=-1.9998, std=0.0001
04:18:33 [INFO] ================================================================================
04:18:33 [INFO] Episode 87: reward=98.53, p_loss=0.2326, v_loss=19.9572, entropy=-18.5875, mem_div=-2.6090, wm_unc=4.5796, lr=1.00e-06
Training Episodes:   4%|         | 88/2000 [10:18:09<193:35:27, 364.50s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
04:18:36 [INFO] [Step 0] First memory divergence measurement: 56.6991, setting reward=0
04:19:04 [INFO] Memory Gate WM: 0.001711
04:19:31 [INFO] Memory Gate WM: 0.001711
04:19:33 [INFO] Memory Gate WM: 0.001711
04:20:13 [INFO] [Episode 88] Completed - Detailed Statistics:
04:20:13 [INFO]   Total steps: 50
04:20:13 [INFO]   Episode reward: 90.0972
04:20:13 [INFO]   Avg memory div reward: -2.757299
04:20:13 [INFO]   Avg memory div abs: 56.374334
04:20:13 [INFO]   Avg WM uncertainty: 4.559243
04:20:13 [INFO]   Min/Max reward: 1.2930 / 4.6424
04:21:16 [INFO] [Episode 88] PPO Epoch 0 - Training Losses:
04:21:16 [INFO]   Policy loss: 0.107336
04:21:16 [INFO]   Value loss: 19.086324
04:21:16 [INFO]   Entropy: -18.586918
04:21:16 [INFO]   Total loss: 9.836367
04:22:34 [INFO] Memory Gate Act: -0.000985
04:23:17 [INFO] Memory Gate Act: -0.000989
04:24:06 [INFO] Memory Gate WM: 0.001715
04:24:35 [INFO] ================================================================================
04:24:35 [INFO] Episode 88 - Aggregated Metrics Summary:
04:24:35 [INFO]   Reward: 98.4335 (recent avg: 89.7159, std: 2.9912)
04:24:35 [INFO]   Policy Loss: 0.221331 (recent avg: 0.224705)
04:24:35 [INFO]   Value Loss: 19.863270
04:24:35 [INFO]   Entropy: -18.587459
04:24:35 [INFO]   Memory Div Reward: -2.610693 (recent avg: -2.769817)
04:24:35 [INFO]   WM Uncertainty: 4.579363
04:24:35 [INFO]   Learning Rate: 1.00e-06
04:24:35 [INFO]   Total Steps: 4450
04:24:35 [INFO]   Log Std: mean=-1.9998, std=0.0001
04:24:35 [INFO] ================================================================================
04:24:35 [INFO] Episode 88: reward=98.43, p_loss=0.2213, v_loss=19.8633, entropy=-18.5875, mem_div=-2.6107, wm_unc=4.5794, lr=1.00e-06
Training Episodes:   4%|         | 89/2000 [10:24:11<193:07:27, 363.81s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
04:24:38 [INFO] [Step 0] First memory divergence measurement: 56.7763, setting reward=0
04:25:07 [INFO] Memory Gate Act: -0.000984
04:26:17 [INFO] [Episode 89] Completed - Detailed Statistics:
04:26:17 [INFO]   Total steps: 50
04:26:17 [INFO]   Episode reward: 99.5450
04:26:17 [INFO]   Avg memory div reward: -2.778203
04:26:17 [INFO]   Avg memory div abs: 56.736615
04:26:17 [INFO]   Avg WM uncertainty: 4.769102
04:26:17 [INFO]   Min/Max reward: 1.6467 / 4.7746
04:27:24 [INFO] [Episode 89] PPO Epoch 0 - Training Losses:
04:27:24 [INFO]   Policy loss: 0.570247
04:27:24 [INFO]   Value loss: 21.545560
04:27:24 [INFO]   Entropy: -18.586884
04:27:24 [INFO]   Total loss: 11.528897
04:29:44 [INFO] Memory Gate Act: -0.000990
04:30:24 [INFO] Memory Gate WM: 0.001728
04:30:50 [INFO] ================================================================================
04:30:50 [INFO] Episode 89 - Aggregated Metrics Summary:
04:30:50 [INFO]   Reward: 98.4459 (recent avg: 90.7206, std: 4.1946)
04:30:50 [INFO]   Policy Loss: 0.224319 (recent avg: 0.214264)
04:30:50 [INFO]   Value Loss: 19.853582
04:30:50 [INFO]   Entropy: -18.587422
04:30:50 [INFO]   Memory Div Reward: -2.612554 (recent avg: -2.770297)
04:30:50 [INFO]   WM Uncertainty: 4.581471
04:30:50 [INFO]   Learning Rate: 1.00e-06
04:30:50 [INFO]   Total Steps: 4500
04:30:50 [INFO]   Log Std: mean=-1.9998, std=0.0001
04:30:50 [INFO] ================================================================================
04:30:50 [INFO] Episode 89: reward=98.45, p_loss=0.2243, v_loss=19.8536, entropy=-18.5874, mem_div=-2.6126, wm_unc=4.5815, lr=1.00e-06
04:30:52 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000090.mp4
Training Episodes:   4%|         | 90/2000 [10:30:27<195:01:19, 367.58s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
04:30:54 [INFO] [Step 0] First memory divergence measurement: 56.8194, setting reward=0
04:32:34 [INFO] [Episode 90] Completed - Detailed Statistics:
04:32:34 [INFO]   Total steps: 50
04:32:34 [INFO]   Episode reward: 91.0577
04:32:34 [INFO]   Avg memory div reward: -2.773994
04:32:34 [INFO]   Avg memory div abs: 56.763640
04:32:34 [INFO]   Avg WM uncertainty: 4.595147
04:32:34 [INFO]   Min/Max reward: 1.4152 / 4.7111
04:33:41 [INFO] [Episode 90] PPO Epoch 0 - Training Losses:
04:33:41 [INFO]   Policy loss: 0.146711
04:33:41 [INFO]   Value loss: 19.536455
04:33:41 [INFO]   Entropy: -18.586825
04:33:41 [INFO]   Total loss: 10.100807
04:34:33 [INFO] Memory Gate WM: 0.001733
04:36:03 [INFO] Memory Gate WM: 0.001725
04:36:06 [INFO] Memory Gate WM: 0.001725
04:37:06 [INFO] ================================================================================
04:37:06 [INFO] Episode 90 - Aggregated Metrics Summary:
04:37:06 [INFO]   Reward: 98.3647 (recent avg: 90.5197, std: 4.1249)
04:37:06 [INFO]   Policy Loss: 0.216108 (recent avg: 0.209759)
04:37:06 [INFO]   Value Loss: 19.830363
04:37:06 [INFO]   Entropy: -18.587383
04:37:06 [INFO]   Memory Div Reward: -2.614328 (recent avg: -2.769845)
04:37:06 [INFO]   WM Uncertainty: 4.581622
04:37:06 [INFO]   Learning Rate: 1.00e-06
04:37:06 [INFO]   Total Steps: 4550
04:37:06 [INFO]   Log Std: mean=-1.9998, std=0.0001
04:37:06 [INFO] ================================================================================
04:37:06 [INFO] Episode 90: reward=98.36, p_loss=0.2161, v_loss=19.8304, entropy=-18.5874, mem_div=-2.6143, wm_unc=4.5816, lr=1.00e-06
Training Episodes:   5%|         | 91/2000 [10:36:42<196:04:12, 369.75s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
04:37:09 [INFO] [Step 0] First memory divergence measurement: 56.9305, setting reward=0
04:37:57 [INFO] Memory Gate Act: -0.000998
04:38:28 [INFO] Memory Gate WM: 0.001725
04:38:48 [INFO] [Episode 91] Completed - Detailed Statistics:
04:38:48 [INFO]   Total steps: 50
04:38:48 [INFO]   Episode reward: 88.5890
04:38:48 [INFO]   Avg memory div reward: -2.787804
04:38:48 [INFO]   Avg memory div abs: 56.877561
04:38:48 [INFO]   Avg WM uncertainty: 4.559583
04:38:48 [INFO]   Min/Max reward: 1.3325 / 4.5898
04:39:22 [INFO] Memory Gate WM: 0.001725
04:39:53 [INFO] [Episode 91] PPO Epoch 0 - Training Losses:
04:39:53 [INFO]   Policy loss: 0.149330
04:39:53 [INFO]   Value loss: 18.857162
04:39:53 [INFO]   Entropy: -18.586712
04:39:53 [INFO]   Total loss: 9.763779
04:40:12 [INFO] Memory Gate WM: 0.001725
04:40:55 [INFO] Memory Gate Act: -0.001002
04:42:31 [INFO] Memory Gate Act: -0.001009
04:43:14 [INFO] ================================================================================
04:43:14 [INFO] Episode 91 - Aggregated Metrics Summary:
04:43:14 [INFO]   Reward: 98.2584 (recent avg: 90.7849, std: 3.9010)
04:43:14 [INFO]   Policy Loss: 0.212182 (recent avg: 0.165292)
04:43:14 [INFO]   Value Loss: 19.778721
04:43:14 [INFO]   Entropy: -18.587342
04:43:14 [INFO]   Memory Div Reward: -2.616213 (recent avg: -2.772005)
04:43:14 [INFO]   WM Uncertainty: 4.581382
04:43:14 [INFO]   Learning Rate: 1.00e-06
04:43:14 [INFO]   Total Steps: 4600
04:43:14 [INFO]   Log Std: mean=-1.9998, std=0.0001
04:43:14 [INFO] ================================================================================
04:43:14 [INFO] Episode 91: reward=98.26, p_loss=0.2122, v_loss=19.7787, entropy=-18.5873, mem_div=-2.6162, wm_unc=4.5814, lr=1.00e-06
Training Episodes:   5%|         | 92/2000 [10:42:50<195:39:55, 369.18s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
04:43:17 [INFO] [Step 0] First memory divergence measurement: 56.9807, setting reward=0
04:44:54 [INFO] [Episode 92] Completed - Detailed Statistics:
04:44:54 [INFO]   Total steps: 50
04:44:54 [INFO]   Episode reward: 89.1769
04:44:54 [INFO]   Avg memory div reward: -2.791033
04:44:54 [INFO]   Avg memory div abs: 56.939115
04:44:54 [INFO]   Avg WM uncertainty: 4.574572
04:44:54 [INFO]   Min/Max reward: 1.2472 / 4.5230
04:46:01 [INFO] [Episode 92] PPO Epoch 0 - Training Losses:
04:46:01 [INFO]   Policy loss: 0.094626
04:46:01 [INFO]   Value loss: 18.926012
04:46:01 [INFO]   Entropy: -18.586671
04:46:01 [INFO]   Total loss: 9.743498
04:46:53 [INFO] Memory Gate WM: 0.001749
04:47:39 [INFO] Memory Gate WM: 0.001747
04:49:24 [INFO] ================================================================================
04:49:24 [INFO] Episode 92 - Aggregated Metrics Summary:
04:49:24 [INFO]   Reward: 98.1608 (recent avg: 90.4456, std: 3.8784)
04:49:24 [INFO]   Policy Loss: 0.212882 (recent avg: 0.157137)
04:49:24 [INFO]   Value Loss: 19.709238
04:49:24 [INFO]   Entropy: -18.587298
04:49:24 [INFO]   Memory Div Reward: -2.618093 (recent avg: -2.775462)
04:49:24 [INFO]   WM Uncertainty: 4.581309
04:49:24 [INFO]   Learning Rate: 1.00e-06
04:49:24 [INFO]   Total Steps: 4650
04:49:24 [INFO]   Log Std: mean=-1.9998, std=0.0001
04:49:24 [INFO] ================================================================================
04:49:24 [INFO] Episode 92: reward=98.16, p_loss=0.2129, v_loss=19.7092, entropy=-18.5873, mem_div=-2.6181, wm_unc=4.5813, lr=1.00e-06
Training Episodes:   5%|         | 93/2000 [10:49:00<195:44:06, 369.51s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
04:49:28 [INFO] [Step 0] First memory divergence measurement: 57.0829, setting reward=0
04:51:05 [INFO] [Episode 93] Completed - Detailed Statistics:
04:51:05 [INFO]   Total steps: 50
04:51:05 [INFO]   Episode reward: 84.1719
04:51:05 [INFO]   Avg memory div reward: -2.787490
04:51:05 [INFO]   Avg memory div abs: 56.959985
04:51:05 [INFO]   Avg WM uncertainty: 4.470928
04:51:05 [INFO]   Min/Max reward: 1.2344 / 4.6035
04:52:11 [INFO] [Episode 93] PPO Epoch 0 - Training Losses:
04:52:11 [INFO]   Policy loss: 0.287563
04:52:11 [INFO]   Value loss: 17.606745
04:52:11 [INFO]   Entropy: -18.586609
04:52:11 [INFO]   Total loss: 9.276802
04:55:32 [INFO] ================================================================================
04:55:32 [INFO] Episode 93 - Aggregated Metrics Summary:
04:55:32 [INFO]   Reward: 98.0120 (recent avg: 89.8592, std: 4.3148)
04:55:32 [INFO]   Policy Loss: 0.205829 (recent avg: 0.187619)
04:55:32 [INFO]   Value Loss: 19.569349
04:55:32 [INFO]   Entropy: -18.587255
04:55:32 [INFO]   Memory Div Reward: -2.619895 (recent avg: -2.777122)
04:55:32 [INFO]   WM Uncertainty: 4.580135
04:55:32 [INFO]   Learning Rate: 1.00e-06
04:55:32 [INFO]   Total Steps: 4700
04:55:32 [INFO]   Log Std: mean=-1.9998, std=0.0001
04:55:32 [INFO] ================================================================================
04:55:32 [INFO] Episode 93: reward=98.01, p_loss=0.2058, v_loss=19.5693, entropy=-18.5873, mem_div=-2.6199, wm_unc=4.5801, lr=1.00e-06
Training Episodes:   5%|         | 94/2000 [10:55:08<195:19:26, 368.92s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
04:55:35 [INFO] [Step 0] First memory divergence measurement: 56.8186, setting reward=0
04:56:36 [INFO] Memory Gate WM: 0.001749
04:56:38 [INFO] Memory Gate WM: 0.001749
04:56:52 [INFO] Memory Gate Act: -0.001024
04:57:14 [INFO] [Episode 94] Completed - Detailed Statistics:
04:57:14 [INFO]   Total steps: 50
04:57:14 [INFO]   Episode reward: 89.1663
04:57:14 [INFO]   Avg memory div reward: -2.781497
04:57:14 [INFO]   Avg memory div abs: 56.786737
04:57:14 [INFO]   Avg WM uncertainty: 4.564823
04:57:14 [INFO]   Min/Max reward: 1.3002 / 4.6187
04:58:21 [INFO] [Episode 94] PPO Epoch 0 - Training Losses:
04:58:21 [INFO]   Policy loss: 0.135817
04:58:21 [INFO]   Value loss: 19.155678
04:58:21 [INFO]   Entropy: -18.586651
04:58:21 [INFO]   Total loss: 9.899522
04:58:26 [INFO] Memory Gate WM: 0.001751
05:01:15 [INFO] Memory Gate WM: 0.001751
05:01:45 [INFO] ================================================================================
05:01:46 [INFO] Episode 94 - Aggregated Metrics Summary:
05:01:46 [INFO]   Reward: 97.9189 (recent avg: 89.7315, std: 4.3145)
05:01:46 [INFO]   Policy Loss: 0.205514 (recent avg: 0.180267)
05:01:46 [INFO]   Value Loss: 19.569134
05:01:46 [INFO]   Entropy: -18.587212
05:01:46 [INFO]   Memory Div Reward: -2.621596 (recent avg: -2.776868)
05:01:46 [INFO]   WM Uncertainty: 4.579974
05:01:46 [INFO]   Learning Rate: 1.00e-06
05:01:46 [INFO]   Total Steps: 4750
05:01:46 [INFO]   Log Std: mean=-1.9998, std=0.0001
05:01:46 [INFO] ================================================================================
05:01:46 [INFO] Episode 94: reward=97.92, p_loss=0.2055, v_loss=19.5691, entropy=-18.5872, mem_div=-2.6216, wm_unc=4.5800, lr=1.00e-06
Training Episodes:   5%|         | 95/2000 [11:01:21<195:56:52, 370.30s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
05:01:48 [INFO] [Step 0] First memory divergence measurement: 56.7382, setting reward=0
05:01:57 [INFO] Memory Gate WM: 0.001751
05:02:19 [INFO] Memory Gate WM: 0.001751
05:03:26 [INFO] [Episode 95] Completed - Detailed Statistics:
05:03:26 [INFO]   Total steps: 50
05:03:26 [INFO]   Episode reward: 92.8242
05:03:26 [INFO]   Avg memory div reward: -2.791455
05:03:26 [INFO]   Avg memory div abs: 56.880172
05:03:26 [INFO]   Avg WM uncertainty: 4.647938
05:03:26 [INFO]   Min/Max reward: 1.5178 / 4.6459
05:04:08 [INFO] Memory Gate WM: 0.001751
05:04:17 [INFO] Memory Gate Act: -0.001026
05:04:35 [INFO] [Episode 95] PPO Epoch 0 - Training Losses:
05:04:35 [INFO]   Policy loss: 0.113184
05:04:35 [INFO]   Value loss: 19.924125
05:04:35 [INFO]   Entropy: -18.586564
05:04:35 [INFO]   Total loss: 10.261112
05:07:09 [INFO] Memory Gate Act: -0.001024
05:07:57 [INFO] ================================================================================
05:07:57 [INFO] Episode 95 - Aggregated Metrics Summary:
05:07:57 [INFO]   Reward: 97.8658 (recent avg: 90.0462, std: 4.4127)
05:07:57 [INFO]   Policy Loss: 0.200768 (recent avg: 0.154124)
05:07:57 [INFO]   Value Loss: 19.473472
05:07:57 [INFO]   Entropy: -18.587165
05:07:57 [INFO]   Memory Div Reward: -2.623366 (recent avg: -2.779886)
05:07:57 [INFO]   WM Uncertainty: 4.580681
05:07:57 [INFO]   Learning Rate: 1.00e-06
05:07:57 [INFO]   Total Steps: 4800
05:07:57 [INFO]   Log Std: mean=-1.9998, std=0.0001
05:07:57 [INFO] ================================================================================
05:07:57 [INFO] Episode 95: reward=97.87, p_loss=0.2008, v_loss=19.4735, entropy=-18.5872, mem_div=-2.6234, wm_unc=4.5807, lr=1.00e-06
Training Episodes:   5%|         | 96/2000 [11:07:33<196:06:09, 370.78s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
05:08:00 [INFO] [Step 0] First memory divergence measurement: 56.6593, setting reward=0
05:08:58 [INFO] Memory Gate WM: 0.001743
05:09:14 [INFO] Memory Gate Act: -0.001024
05:09:40 [INFO] [Episode 96] Completed - Detailed Statistics:
05:09:40 [INFO]   Total steps: 50
05:09:40 [INFO]   Episode reward: 91.9802
05:09:40 [INFO]   Avg memory div reward: -2.786221
05:09:40 [INFO]   Avg memory div abs: 56.763046
05:09:40 [INFO]   Avg WM uncertainty: 4.625825
05:09:40 [INFO]   Min/Max reward: 1.4447 / 4.5928
05:10:07 [INFO] Memory Gate Act: -0.001021
05:10:47 [INFO] [Episode 96] PPO Epoch 0 - Training Losses:
05:10:47 [INFO]   Policy loss: 0.152582
05:10:47 [INFO]   Value loss: 19.799370
05:10:47 [INFO]   Entropy: -18.586474
05:10:47 [INFO]   Total loss: 10.238131
05:12:09 [INFO] Memory Gate Act: -0.001030
05:12:25 [INFO] Memory Gate Act: -0.001032
05:14:14 [INFO] ================================================================================
05:14:14 [INFO] Episode 96 - Aggregated Metrics Summary:
05:14:14 [INFO]   Reward: 97.8051 (recent avg: 89.9599, std: 4.3654)
05:14:14 [INFO]   Policy Loss: 0.198428 (recent avg: 0.151534)
05:14:14 [INFO]   Value Loss: 19.502244
05:14:14 [INFO]   Entropy: -18.587115
05:14:14 [INFO]   Memory Div Reward: -2.625045 (recent avg: -2.780800)
05:14:14 [INFO]   WM Uncertainty: 4.581147
05:14:14 [INFO]   Learning Rate: 1.00e-06
05:14:14 [INFO]   Total Steps: 4850
05:14:14 [INFO]   Log Std: mean=-1.9998, std=0.0001
05:14:14 [INFO] ================================================================================
05:14:14 [INFO] Episode 96: reward=97.81, p_loss=0.1984, v_loss=19.5022, entropy=-18.5871, mem_div=-2.6250, wm_unc=4.5811, lr=1.00e-06
Training Episodes:   5%|         | 97/2000 [11:13:50<196:57:47, 372.61s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
05:14:17 [INFO] [Step 0] First memory divergence measurement: 56.7856, setting reward=0
05:15:55 [INFO] [Episode 97] Completed - Detailed Statistics:
05:15:55 [INFO]   Total steps: 50
05:15:55 [INFO]   Episode reward: 93.5354
05:15:55 [INFO]   Avg memory div reward: -2.779121
05:15:55 [INFO]   Avg memory div abs: 56.803788
05:15:55 [INFO]   Avg WM uncertainty: 4.649830
05:15:55 [INFO]   Min/Max reward: 1.3645 / 4.6307
05:17:02 [INFO] [Episode 97] PPO Epoch 0 - Training Losses:
05:17:02 [INFO]   Policy loss: 0.110638
05:17:02 [INFO]   Value loss: 20.135570
05:17:02 [INFO]   Entropy: -18.586406
05:17:02 [INFO]   Total loss: 10.364288
05:17:45 [INFO] Memory Gate WM: 0.001739
05:20:24 [INFO] ================================================================================
05:20:24 [INFO] Episode 97 - Aggregated Metrics Summary:
05:20:24 [INFO]   Reward: 97.7615 (recent avg: 91.0144, std: 3.7904)
05:20:24 [INFO]   Policy Loss: 0.196980 (recent avg: 0.149681)
05:20:24 [INFO]   Value Loss: 19.567939
05:20:24 [INFO]   Entropy: -18.587063
05:20:24 [INFO]   Memory Div Reward: -2.626617 (recent avg: -2.781412)
05:20:24 [INFO]   WM Uncertainty: 4.581848
05:20:24 [INFO]   Learning Rate: 1.00e-06
05:20:24 [INFO]   Total Steps: 4900
05:20:24 [INFO]   Log Std: mean=-1.9998, std=0.0001
05:20:24 [INFO] ================================================================================
05:20:24 [INFO] Episode 97: reward=97.76, p_loss=0.1970, v_loss=19.5679, entropy=-18.5871, mem_div=-2.6266, wm_unc=4.5818, lr=1.00e-06
Training Episodes:   5%|         | 98/2000 [11:19:59<196:20:37, 371.63s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
05:20:26 [INFO] [Step 0] First memory divergence measurement: 57.0209, setting reward=0
05:21:49 [INFO] Memory Gate Act: -0.001012
05:22:07 [INFO] [Episode 98] Completed - Detailed Statistics:
05:22:07 [INFO]   Total steps: 50
05:22:07 [INFO]   Episode reward: 86.0182
05:22:07 [INFO]   Avg memory div reward: -2.792533
05:22:07 [INFO]   Avg memory div abs: 57.013733
05:22:07 [INFO]   Avg WM uncertainty: 4.512896
05:22:07 [INFO]   Min/Max reward: 1.4966 / 4.5519
05:23:01 [INFO] Memory Gate WM: 0.001745
05:23:14 [INFO] [Episode 98] PPO Epoch 0 - Training Losses:
05:23:14 [INFO]   Policy loss: 0.266026
05:23:14 [INFO]   Value loss: 18.244380
05:23:14 [INFO]   Entropy: -18.586295
05:23:14 [INFO]   Total loss: 9.574079
05:26:12 [INFO] Memory Gate WM: 0.001757
05:26:39 [INFO] ================================================================================
05:26:39 [INFO] Episode 98 - Aggregated Metrics Summary:
05:26:39 [INFO]   Reward: 97.6429 (recent avg: 90.6065, std: 4.0759)
05:26:39 [INFO]   Policy Loss: 0.201338 (recent avg: 0.224748)
05:26:39 [INFO]   Value Loss: 19.473547
05:26:39 [INFO]   Entropy: -18.587003
05:26:39 [INFO]   Memory Div Reward: -2.628293 (recent avg: -2.784935)
05:26:39 [INFO]   WM Uncertainty: 4.581151
05:26:39 [INFO]   Learning Rate: 1.00e-06
05:26:39 [INFO]   Total Steps: 4950
05:26:39 [INFO]   Log Std: mean=-1.9997, std=0.0001
05:26:39 [INFO] ================================================================================
05:26:39 [INFO] Episode 98: reward=97.64, p_loss=0.2013, v_loss=19.4735, entropy=-18.5870, mem_div=-2.6283, wm_unc=4.5812, lr=1.00e-06
Training Episodes:   5%|         | 99/2000 [11:26:15<196:49:41, 372.74s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
05:26:42 [INFO] [Step 0] First memory divergence measurement: 57.1507, setting reward=0
05:26:58 [INFO] Memory Gate WM: 0.001757
05:27:52 [INFO] Memory Gate Act: -0.001013
05:28:22 [INFO] [Episode 99] Completed - Detailed Statistics:
05:28:22 [INFO]   Total steps: 50
05:28:22 [INFO]   Episode reward: 92.0720
05:28:22 [INFO]   Avg memory div reward: -2.777184
05:28:22 [INFO]   Avg memory div abs: 56.924795
05:28:22 [INFO]   Avg WM uncertainty: 4.618624
05:28:22 [INFO]   Min/Max reward: 1.2149 / 4.9362
05:29:30 [INFO] [Episode 99] PPO Epoch 0 - Training Losses:
05:29:30 [INFO]   Policy loss: 0.238039
05:29:30 [INFO]   Value loss: 19.369443
05:29:30 [INFO]   Entropy: -18.585881
05:29:30 [INFO]   Total loss: 10.108620
05:30:39 [INFO] Memory Gate Act: -0.001011
05:32:54 [INFO] ================================================================================
05:32:54 [INFO] Episode 99 - Aggregated Metrics Summary:
05:32:54 [INFO]   Reward: 97.5872 (recent avg: 89.8592, std: 2.8774)
05:32:54 [INFO]   Policy Loss: 0.198509 (recent avg: 0.235400)
05:32:54 [INFO]   Value Loss: 19.399223
05:32:54 [INFO]   Entropy: -18.586931
05:32:54 [INFO]   Memory Div Reward: -2.629782 (recent avg: -2.784833)
05:32:54 [INFO]   WM Uncertainty: 4.581526
05:32:54 [INFO]   Learning Rate: 1.00e-06
05:32:54 [INFO]   Total Steps: 5000
05:32:54 [INFO]   Log Std: mean=-1.9997, std=0.0001
05:32:54 [INFO] ================================================================================
05:32:54 [INFO] Episode 99: reward=97.59, p_loss=0.1985, v_loss=19.3992, entropy=-18.5869, mem_div=-2.6298, wm_unc=4.5815, lr=1.00e-06
05:33:09 [INFO] Saved checkpoint to outputs/student_rl/checkpoint-99
05:33:11 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000100.mp4
Training Episodes:   5%|         | 100/2000 [11:32:46<199:44:49, 378.47s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
05:33:13 [INFO] [Step 0] First memory divergence measurement: 56.9892, setting reward=0
05:33:35 [INFO] Memory Gate WM: 0.001749
05:34:54 [INFO] [Episode 100] Completed - Detailed Statistics:
05:34:54 [INFO]   Total steps: 50
05:34:54 [INFO]   Episode reward: 91.7954
05:34:54 [INFO]   Avg memory div reward: -2.787619
05:34:54 [INFO]   Avg memory div abs: 56.897628
05:34:54 [INFO]   Avg WM uncertainty: 4.623528
05:34:54 [INFO]   Min/Max reward: 1.2505 / 4.8958
05:36:02 [INFO] [Episode 100] PPO Epoch 0 - Training Losses:
05:36:02 [INFO]   Policy loss: 0.237698
05:36:02 [INFO]   Value loss: 19.456192
05:36:02 [INFO]   Entropy: -18.585840
05:36:02 [INFO]   Total loss: 10.151653
05:36:19 [INFO] Memory Gate Act: -0.001006
05:36:27 [INFO] Memory Gate WM: 0.001753
05:36:54 [INFO] Memory Gate Act: -0.001009
05:39:24 [INFO] ================================================================================
05:39:24 [INFO] Episode 100 - Aggregated Metrics Summary:
05:39:24 [INFO]   Reward: 97.4632 (recent avg: 89.9329, std: 2.9164)
05:39:24 [INFO]   Policy Loss: 0.198746 (recent avg: 0.221002)
05:39:24 [INFO]   Value Loss: 19.327821
05:39:24 [INFO]   Entropy: -18.586859
05:39:24 [INFO]   Memory Div Reward: -2.632733 (recent avg: -2.786196)
05:39:24 [INFO]   WM Uncertainty: 4.581998
05:39:24 [INFO]   Learning Rate: 1.00e-06
05:39:24 [INFO]   Total Steps: 5050
05:39:24 [INFO]   Log Std: mean=-1.9997, std=0.0001
05:39:24 [INFO] ================================================================================
05:39:24 [INFO] Episode 100: reward=97.46, p_loss=0.1987, v_loss=19.3278, entropy=-18.5869, mem_div=-2.6327, wm_unc=4.5820, lr=1.00e-06
Training Episodes:   5%|         | 101/2000 [11:38:59<198:45:54, 376.81s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
05:39:27 [INFO] [Step 0] First memory divergence measurement: 56.9922, setting reward=0
05:39:48 [INFO] Memory Gate Act: -0.001028
05:41:06 [INFO] [Episode 101] Completed - Detailed Statistics:
05:41:06 [INFO]   Total steps: 50
05:41:06 [INFO]   Episode reward: 90.8795
05:41:06 [INFO]   Avg memory div reward: -2.796723
05:41:06 [INFO]   Avg memory div abs: 57.011779
05:41:06 [INFO]   Avg WM uncertainty: 4.614313
05:41:06 [INFO]   Min/Max reward: 1.4899 / 4.5120
05:42:12 [INFO] [Episode 101] PPO Epoch 0 - Training Losses:
05:42:12 [INFO]   Policy loss: 0.198411
05:42:12 [INFO]   Value loss: 19.337746
05:42:12 [INFO]   Entropy: -18.585748
05:42:12 [INFO]   Total loss: 10.053141
05:44:33 [INFO] Memory Gate WM: 0.001766
05:45:30 [INFO] ================================================================================
05:45:30 [INFO] Episode 101 - Aggregated Metrics Summary:
05:45:30 [INFO]   Reward: 97.3333 (recent avg: 90.1620, std: 2.8917)
05:45:30 [INFO]   Policy Loss: 0.194567 (recent avg: 0.168264)
05:45:30 [INFO]   Value Loss: 19.360987
05:45:30 [INFO]   Entropy: -18.586785
05:45:30 [INFO]   Memory Div Reward: -2.635845 (recent avg: -2.787088)
05:45:30 [INFO]   WM Uncertainty: 4.582511
05:45:30 [INFO]   Learning Rate: 1.00e-06
05:45:30 [INFO]   Total Steps: 5100
05:45:30 [INFO]   Log Std: mean=-1.9997, std=0.0001
05:45:30 [INFO] ================================================================================
05:45:30 [INFO] Episode 101: reward=97.33, p_loss=0.1946, v_loss=19.3610, entropy=-18.5868, mem_div=-2.6358, wm_unc=4.5825, lr=1.00e-06
Training Episodes:   5%|         | 102/2000 [11:45:06<197:00:19, 373.67s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
05:45:33 [INFO] [Step 0] First memory divergence measurement: 56.8589, setting reward=0
05:46:06 [INFO] Memory Gate Act: -0.001035
05:46:21 [INFO] Memory Gate Act: -0.001035
05:46:51 [INFO] Memory Gate Act: -0.001035
05:47:13 [INFO] [Episode 102] Completed - Detailed Statistics:
05:47:13 [INFO]   Total steps: 50
05:47:13 [INFO]   Episode reward: 91.5450
05:47:13 [INFO]   Avg memory div reward: -2.792307
05:47:13 [INFO]   Avg memory div abs: 56.998172
05:47:13 [INFO]   Avg WM uncertainty: 4.623207
05:47:13 [INFO]   Min/Max reward: 1.3986 / 4.9181
05:48:21 [INFO] [Episode 102] PPO Epoch 0 - Training Losses:
05:48:21 [INFO]   Policy loss: 0.207337
05:48:21 [INFO]   Value loss: 19.439104
05:48:21 [INFO]   Entropy: -18.585698
05:48:21 [INFO]   Total loss: 10.112746
05:48:43 [INFO] Memory Gate WM: 0.001756
05:51:42 [INFO] Memory Gate Act: -0.001023
05:51:43 [INFO] ================================================================================
05:51:44 [INFO] Episode 102 - Aggregated Metrics Summary:
05:51:44 [INFO]   Reward: 97.2399 (recent avg: 90.3988, std: 2.8983)
05:51:44 [INFO]   Policy Loss: 0.193634 (recent avg: 0.165807)
05:51:44 [INFO]   Value Loss: 19.305329
05:51:44 [INFO]   Entropy: -18.586712
05:51:44 [INFO]   Memory Div Reward: -2.638933 (recent avg: -2.787215)
05:51:44 [INFO]   WM Uncertainty: 4.583731
05:51:44 [INFO]   Learning Rate: 1.00e-06
05:51:44 [INFO]   Total Steps: 5150
05:51:44 [INFO]   Log Std: mean=-1.9997, std=0.0001
05:51:44 [INFO] ================================================================================
05:51:44 [INFO] Episode 102: reward=97.24, p_loss=0.1936, v_loss=19.3053, entropy=-18.5867, mem_div=-2.6389, wm_unc=4.5837, lr=1.00e-06
Training Episodes:   5%|         | 103/2000 [11:51:19<196:51:55, 373.60s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
05:51:46 [INFO] [Step 0] First memory divergence measurement: 56.7399, setting reward=0
05:53:24 [INFO] [Episode 103] Completed - Detailed Statistics:
05:53:24 [INFO]   Total steps: 50
05:53:24 [INFO]   Episode reward: 90.9192
05:53:24 [INFO]   Avg memory div reward: -2.800539
05:53:24 [INFO]   Avg memory div abs: 57.068890
05:53:24 [INFO]   Avg WM uncertainty: 4.618923
05:53:24 [INFO]   Min/Max reward: 1.3535 / 4.9232
05:53:58 [INFO] Memory Gate WM: 0.001763
05:54:28 [INFO] [Episode 103] PPO Epoch 0 - Training Losses:
05:54:28 [INFO]   Policy loss: 0.153523
05:54:28 [INFO]   Value loss: 19.352050
05:54:28 [INFO]   Entropy: -18.585620
05:54:28 [INFO]   Total loss: 10.015405
05:54:36 [INFO] Memory Gate WM: 0.001760
05:54:49 [INFO] Memory Gate Act: -0.001017
05:55:48 [INFO] Memory Gate Act: -0.001009
05:57:15 [INFO] Memory Gate WM: 0.001761
05:57:48 [INFO] ================================================================================
05:57:48 [INFO] Episode 103 - Aggregated Metrics Summary:
05:57:48 [INFO]   Reward: 97.0767 (recent avg: 91.0735, std: 2.0235)
05:57:48 [INFO]   Policy Loss: 0.194126 (recent avg: 0.178887)
05:57:48 [INFO]   Value Loss: 19.285688
05:57:48 [INFO]   Entropy: -18.586640
05:57:48 [INFO]   Memory Div Reward: -2.642045 (recent avg: -2.788520)
05:57:48 [INFO]   WM Uncertainty: 4.583579
05:57:48 [INFO]   Learning Rate: 1.00e-06
05:57:48 [INFO]   Total Steps: 5200
05:57:48 [INFO]   Log Std: mean=-1.9997, std=0.0001
05:57:48 [INFO] ================================================================================
05:57:48 [INFO] Episode 103: reward=97.08, p_loss=0.1941, v_loss=19.2857, entropy=-18.5866, mem_div=-2.6420, wm_unc=4.5836, lr=1.00e-06
Training Episodes:   5%|         | 104/2000 [11:57:24<195:23:41, 371.00s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
05:57:52 [INFO] [Step 0] First memory divergence measurement: 57.0722, setting reward=0
05:59:31 [INFO] [Episode 104] Completed - Detailed Statistics:
05:59:31 [INFO]   Total steps: 50
05:59:31 [INFO]   Episode reward: 89.8038
05:59:31 [INFO]   Avg memory div reward: -2.784536
05:59:31 [INFO]   Avg memory div abs: 56.921044
05:59:31 [INFO]   Avg WM uncertainty: 4.580611
05:59:31 [INFO]   Min/Max reward: 1.2933 / 4.7930
06:00:00 [INFO] Memory Gate WM: 0.001760
06:00:39 [INFO] [Episode 104] PPO Epoch 0 - Training Losses:
06:00:39 [INFO]   Policy loss: 0.255908
06:00:39 [INFO]   Value loss: 19.154180
06:00:39 [INFO]   Entropy: -18.585603
06:00:39 [INFO]   Total loss: 10.018854
06:03:04 [INFO] Memory Gate Act: -0.001011
06:04:03 [INFO] ================================================================================
06:04:03 [INFO] Episode 104 - Aggregated Metrics Summary:
06:04:03 [INFO]   Reward: 96.9033 (recent avg: 91.1373, std: 1.9717)
06:04:03 [INFO]   Policy Loss: 0.193957 (recent avg: 0.194840)
06:04:03 [INFO]   Value Loss: 19.301456
06:04:03 [INFO]   Entropy: -18.586571
06:04:03 [INFO]   Memory Div Reward: -2.645007 (recent avg: -2.788824)
06:04:03 [INFO]   WM Uncertainty: 4.583074
06:04:03 [INFO]   Learning Rate: 1.00e-06
06:04:03 [INFO]   Total Steps: 5250
06:04:03 [INFO]   Log Std: mean=-1.9997, std=0.0001
06:04:03 [INFO] ================================================================================
06:04:03 [INFO] Episode 104: reward=96.90, p_loss=0.1940, v_loss=19.3015, entropy=-18.5866, mem_div=-2.6450, wm_unc=4.5831, lr=1.00e-06
Training Episodes:   5%|         | 105/2000 [12:03:39<195:52:24, 372.11s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
06:04:06 [INFO] [Step 0] First memory divergence measurement: 57.0859, setting reward=0
06:05:44 [INFO] [Episode 105] Completed - Detailed Statistics:
06:05:44 [INFO]   Total steps: 50
06:05:44 [INFO]   Episode reward: 93.3196
06:05:44 [INFO]   Avg memory div reward: -2.791519
06:05:44 [INFO]   Avg memory div abs: 56.939725
06:05:44 [INFO]   Avg WM uncertainty: 4.657912
06:05:44 [INFO]   Min/Max reward: 1.4969 / 4.6961
06:06:50 [INFO] [Episode 105] PPO Epoch 0 - Training Losses:
06:06:50 [INFO]   Policy loss: 0.100240
06:06:50 [INFO]   Value loss: 19.885293
06:06:50 [INFO]   Entropy: -18.585621
06:06:50 [INFO]   Total loss: 10.228743
06:07:12 [INFO] Memory Gate WM: 0.001766
06:08:36 [INFO] Memory Gate Act: -0.000996
06:10:14 [INFO] ================================================================================
06:10:14 [INFO] Episode 105 - Aggregated Metrics Summary:
06:10:14 [INFO]   Reward: 96.7794 (recent avg: 91.1868, std: 2.0192)
06:10:14 [INFO]   Policy Loss: 0.187049 (recent avg: 0.157935)
06:10:14 [INFO]   Value Loss: 19.310756
06:10:14 [INFO]   Entropy: -18.586503
06:10:14 [INFO]   Memory Div Reward: -2.648054 (recent avg: -2.788830)
06:10:14 [INFO]   WM Uncertainty: 4.583641
06:10:14 [INFO]   Learning Rate: 1.00e-06
06:10:14 [INFO]   Total Steps: 5300
06:10:14 [INFO]   Log Std: mean=-1.9997, std=0.0001
06:10:14 [INFO] ================================================================================
06:10:14 [INFO] Episode 105: reward=96.78, p_loss=0.1870, v_loss=19.3108, entropy=-18.5865, mem_div=-2.6481, wm_unc=4.5836, lr=1.00e-06
Training Episodes:   5%|         | 106/2000 [12:09:50<195:33:50, 371.72s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
06:10:16 [INFO] [Step 0] First memory divergence measurement: 56.9005, setting reward=0
06:11:34 [INFO] Memory Gate Act: -0.000990
06:11:55 [INFO] [Episode 106] Completed - Detailed Statistics:
06:11:55 [INFO]   Total steps: 50
06:11:55 [INFO]   Episode reward: 94.7797
06:11:55 [INFO]   Avg memory div reward: -2.777184
06:11:55 [INFO]   Avg memory div abs: 56.705539
06:11:55 [INFO]   Avg WM uncertainty: 4.672777
06:11:55 [INFO]   Min/Max reward: 1.3550 / 4.6442
06:13:04 [INFO] [Episode 106] PPO Epoch 0 - Training Losses:
06:13:04 [INFO]   Policy loss: 0.137547
06:13:04 [INFO]   Value loss: 20.231880
06:13:04 [INFO]   Entropy: -18.585559
06:13:04 [INFO]   Total loss: 10.439342
06:16:24 [INFO] Memory Gate Act: -0.000986
06:16:31 [INFO] ================================================================================
06:16:31 [INFO] Episode 106 - Aggregated Metrics Summary:
06:16:31 [INFO]   Reward: 96.7161 (recent avg: 91.4668, std: 2.2862)
06:16:31 [INFO]   Policy Loss: 0.185037 (recent avg: 0.127178)
06:16:31 [INFO]   Value Loss: 19.385831
06:16:31 [INFO]   Entropy: -18.586434
06:16:31 [INFO]   Memory Div Reward: -2.650799 (recent avg: -2.787927)
06:16:31 [INFO]   WM Uncertainty: 4.585120
06:16:31 [INFO]   Learning Rate: 1.00e-06
06:16:31 [INFO]   Total Steps: 5350
06:16:31 [INFO]   Log Std: mean=-1.9997, std=0.0001
06:16:31 [INFO] ================================================================================
06:16:31 [INFO] Episode 106: reward=96.72, p_loss=0.1850, v_loss=19.3858, entropy=-18.5864, mem_div=-2.6508, wm_unc=4.5851, lr=1.00e-06
Training Episodes:   5%|         | 107/2000 [12:16:06<196:14:52, 373.21s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
06:16:33 [INFO] [Step 0] First memory divergence measurement: 56.8104, setting reward=0
06:16:46 [INFO] Memory Gate Act: -0.000986
06:18:10 [INFO] [Episode 107] Completed - Detailed Statistics:
06:18:10 [INFO]   Total steps: 50
06:18:10 [INFO]   Episode reward: 89.2206
06:18:10 [INFO]   Avg memory div reward: -2.795390
06:18:10 [INFO]   Avg memory div abs: 56.979234
06:18:10 [INFO]   Avg WM uncertainty: 4.579803
06:18:10 [INFO]   Min/Max reward: 1.3937 / 4.7627
06:19:15 [INFO] [Episode 107] PPO Epoch 0 - Training Losses:
06:19:15 [INFO]   Policy loss: 0.173207
06:19:15 [INFO]   Value loss: 18.706433
06:19:15 [INFO]   Entropy: -18.585499
06:19:15 [INFO]   Total loss: 9.712279
06:21:14 [INFO] Memory Gate WM: 0.001715
06:21:49 [INFO] Memory Gate WM: 0.001713
06:22:36 [INFO] ================================================================================
06:22:36 [INFO] Episode 107 - Aggregated Metrics Summary:
06:22:36 [INFO]   Reward: 96.5322 (recent avg: 91.0353, std: 2.2621)
06:22:36 [INFO]   Policy Loss: 0.183786 (recent avg: 0.161292)
06:22:36 [INFO]   Value Loss: 19.330834
06:22:36 [INFO]   Entropy: -18.586363
06:22:36 [INFO]   Memory Div Reward: -2.653815 (recent avg: -2.789553)
06:22:36 [INFO]   WM Uncertainty: 4.584459
06:22:36 [INFO]   Learning Rate: 1.00e-06
06:22:36 [INFO]   Total Steps: 5400
06:22:36 [INFO]   Log Std: mean=-1.9997, std=0.0002
06:22:36 [INFO] ================================================================================
06:22:36 [INFO] Episode 107: reward=96.53, p_loss=0.1838, v_loss=19.3308, entropy=-18.5864, mem_div=-2.6538, wm_unc=4.5845, lr=1.00e-06
Training Episodes:   5%|         | 108/2000 [12:22:11<194:52:34, 370.80s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
06:22:39 [INFO] [Step 0] First memory divergence measurement: 56.5902, setting reward=0
06:24:00 [INFO] Memory Gate WM: 0.001709
06:24:13 [INFO] Memory Gate Act: -0.000970
06:24:16 [INFO] [Episode 108] Completed - Detailed Statistics:
06:24:16 [INFO]   Total steps: 50
06:24:16 [INFO]   Episode reward: 90.6614
06:24:16 [INFO]   Avg memory div reward: -2.762827
06:24:16 [INFO]   Avg memory div abs: 56.581661
06:24:16 [INFO]   Avg WM uncertainty: 4.576055
06:24:16 [INFO]   Min/Max reward: 1.4922 / 4.5054
06:25:25 [INFO] [Episode 108] PPO Epoch 0 - Training Losses:
06:25:25 [INFO]   Policy loss: 0.159993
06:25:25 [INFO]   Value loss: 19.270947
06:25:25 [INFO]   Entropy: -18.585346
06:25:25 [INFO]   Total loss: 9.981320
06:27:09 [INFO] Memory Gate Act: -0.000971
06:28:50 [INFO] ================================================================================
06:28:50 [INFO] Episode 108 - Aggregated Metrics Summary:
06:28:50 [INFO]   Reward: 96.4396 (recent avg: 91.4996, std: 1.5486)
06:28:50 [INFO]   Policy Loss: 0.183482 (recent avg: 0.200180)
06:28:50 [INFO]   Value Loss: 19.326977
06:28:50 [INFO]   Entropy: -18.586288
06:28:50 [INFO]   Memory Div Reward: -2.656522 (recent avg: -2.786583)
06:28:50 [INFO]   WM Uncertainty: 4.585314
06:28:50 [INFO]   Learning Rate: 1.00e-06
06:28:50 [INFO]   Total Steps: 5450
06:28:50 [INFO]   Log Std: mean=-1.9997, std=0.0002
06:28:50 [INFO] ================================================================================
06:28:50 [INFO] Episode 108: reward=96.44, p_loss=0.1835, v_loss=19.3270, entropy=-18.5863, mem_div=-2.6565, wm_unc=4.5853, lr=1.00e-06
Training Episodes:   5%|         | 109/2000 [12:28:25<195:16:13, 371.75s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
06:28:52 [INFO] [Step 0] First memory divergence measurement: 56.8460, setting reward=0
06:29:21 [INFO] Memory Gate Act: -0.000969
06:30:31 [INFO] [Episode 109] Completed - Detailed Statistics:
06:30:31 [INFO]   Total steps: 50
06:30:31 [INFO]   Episode reward: 87.4155
06:30:31 [INFO]   Avg memory div reward: -2.790067
06:30:31 [INFO]   Avg memory div abs: 56.901193
06:30:31 [INFO]   Avg WM uncertainty: 4.538378
06:30:31 [INFO]   Min/Max reward: 1.4601 / 4.5183
06:31:38 [INFO] [Episode 109] PPO Epoch 0 - Training Losses:
06:31:38 [INFO]   Policy loss: 0.116869
06:31:38 [INFO]   Value loss: 18.306870
06:31:38 [INFO]   Entropy: -18.585275
06:31:38 [INFO]   Total loss: 9.456157
06:32:08 [INFO] Memory Gate WM: 0.001720
06:32:57 [INFO] Memory Gate Act: -0.000974
06:35:00 [INFO] ================================================================================
06:35:00 [INFO] Episode 109 - Aggregated Metrics Summary:
06:35:00 [INFO]   Reward: 96.3396 (recent avg: 91.0340, std: 1.9536)
06:35:00 [INFO]   Policy Loss: 0.189545 (recent avg: 0.228449)
06:35:00 [INFO]   Value Loss: 19.272954
06:35:00 [INFO]   Entropy: -18.586211
06:35:00 [INFO]   Memory Div Reward: -2.659439 (recent avg: -2.787871)
06:35:00 [INFO]   WM Uncertainty: 4.586230
06:35:00 [INFO]   Learning Rate: 1.00e-06
06:35:00 [INFO]   Total Steps: 5500
06:35:00 [INFO]   Log Std: mean=-1.9997, std=0.0001
06:35:00 [INFO] ================================================================================
06:35:00 [INFO] Episode 109: reward=96.34, p_loss=0.1895, v_loss=19.2730, entropy=-18.5862, mem_div=-2.6594, wm_unc=4.5862, lr=1.00e-06
06:35:03 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000110.mp4
Training Episodes:   6%|         | 110/2000 [12:34:38<195:20:16, 372.07s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
06:35:05 [INFO] [Step 0] First memory divergence measurement: 56.5408, setting reward=0
06:35:22 [INFO] Memory Gate WM: 0.001726
06:36:35 [INFO] Memory Gate WM: 0.001726
06:36:44 [INFO] [Episode 110] Completed - Detailed Statistics:
06:36:44 [INFO]   Total steps: 50
06:36:44 [INFO]   Episode reward: 84.0639
06:36:44 [INFO]   Avg memory div reward: -2.781817
06:36:44 [INFO]   Avg memory div abs: 56.715819
06:36:44 [INFO]   Avg WM uncertainty: 4.463094
06:36:44 [INFO]   Min/Max reward: 1.3256 / 4.6194
06:37:00 [INFO] Memory Gate WM: 0.001727
06:37:50 [INFO] [Episode 110] PPO Epoch 0 - Training Losses:
06:37:50 [INFO]   Policy loss: 0.113230
06:37:50 [INFO]   Value loss: 17.674111
06:37:50 [INFO]   Entropy: -18.585056
06:37:50 [INFO]   Total loss: 9.136136
06:40:31 [INFO] Memory Gate WM: 0.001721
06:40:53 [INFO] Memory Gate Act: -0.000968
06:41:11 [INFO] ================================================================================
06:41:11 [INFO] Episode 110 - Aggregated Metrics Summary:
06:41:11 [INFO]   Reward: 96.1104 (recent avg: 90.2608, std: 2.8318)
06:41:11 [INFO]   Policy Loss: 0.189912 (recent avg: 0.215323)
06:41:11 [INFO]   Value Loss: 19.208159
06:41:11 [INFO]   Entropy: -18.586126
06:41:11 [INFO]   Memory Div Reward: -2.662175 (recent avg: -2.787291)
06:41:11 [INFO]   WM Uncertainty: 4.584383
06:41:11 [INFO]   Learning Rate: 1.00e-06
06:41:11 [INFO]   Total Steps: 5550
06:41:11 [INFO]   Log Std: mean=-1.9997, std=0.0001
06:41:11 [INFO] ================================================================================
06:41:11 [INFO] Episode 110: reward=96.11, p_loss=0.1899, v_loss=19.2082, entropy=-18.5861, mem_div=-2.6622, wm_unc=4.5844, lr=1.00e-06
Training Episodes:   6%|         | 111/2000 [12:40:47<194:40:01, 370.99s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
06:41:14 [INFO] [Step 0] First memory divergence measurement: 56.9063, setting reward=0
06:41:48 [INFO] Memory Gate Act: -0.000966
06:42:51 [INFO] [Episode 111] Completed - Detailed Statistics:
06:42:51 [INFO]   Total steps: 50
06:42:51 [INFO]   Episode reward: 95.7635
06:42:51 [INFO]   Avg memory div reward: -2.781897
06:42:51 [INFO]   Avg memory div abs: 56.770406
06:42:51 [INFO]   Avg WM uncertainty: 4.697167
06:42:51 [INFO]   Min/Max reward: 1.3633 / 4.7433
06:43:42 [INFO] Memory Gate WM: 0.001713
06:43:58 [INFO] [Episode 111] PPO Epoch 0 - Training Losses:
06:43:58 [INFO]   Policy loss: 0.068733
06:43:58 [INFO]   Value loss: 20.260481
06:43:58 [INFO]   Entropy: -18.584952
06:43:58 [INFO]   Total loss: 10.384823
06:47:25 [INFO] ================================================================================
06:47:25 [INFO] Episode 111 - Aggregated Metrics Summary:
06:47:25 [INFO]   Reward: 96.0116 (recent avg: 90.7492, std: 3.2818)
06:47:25 [INFO]   Policy Loss: 0.184959 (recent avg: 0.160355)
06:47:25 [INFO]   Value Loss: 19.223125
06:47:25 [INFO]   Entropy: -18.586039
06:47:25 [INFO]   Memory Div Reward: -2.665217 (recent avg: -2.785808)
06:47:25 [INFO]   WM Uncertainty: 4.585448
06:47:25 [INFO]   Learning Rate: 1.00e-06
06:47:25 [INFO]   Total Steps: 5600
06:47:25 [INFO]   Log Std: mean=-1.9997, std=0.0001
06:47:25 [INFO] ================================================================================
06:47:25 [INFO] Episode 111: reward=96.01, p_loss=0.1850, v_loss=19.2231, entropy=-18.5860, mem_div=-2.6652, wm_unc=4.5854, lr=1.00e-06
Training Episodes:   6%|         | 112/2000 [12:47:01<195:00:52, 371.85s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
06:47:27 [INFO] [Step 0] First memory divergence measurement: 56.9036, setting reward=0
06:48:27 [INFO] Memory Gate Act: -0.000962
06:49:06 [INFO] [Episode 112] Completed - Detailed Statistics:
06:49:06 [INFO]   Total steps: 50
06:49:06 [INFO]   Episode reward: 90.7431
06:49:06 [INFO]   Avg memory div reward: -2.778200
06:49:06 [INFO]   Avg memory div abs: 56.789969
06:49:06 [INFO]   Avg WM uncertainty: 4.593063
06:49:06 [INFO]   Min/Max reward: 1.3924 / 4.6304
06:49:38 [INFO] Memory Gate WM: 0.001722
06:50:12 [INFO] [Episode 112] PPO Epoch 0 - Training Losses:
06:50:12 [INFO]   Policy loss: 0.411550
06:50:12 [INFO]   Value loss: 19.352378
06:50:12 [INFO]   Entropy: -18.584888
06:50:12 [INFO]   Total loss: 10.273588
06:50:18 [INFO] Memory Gate WM: 0.001727
06:53:34 [INFO] ================================================================================
06:53:34 [INFO] Episode 112 - Aggregated Metrics Summary:
06:53:34 [INFO]   Reward: 95.8734 (recent avg: 90.6690, std: 3.2712)
06:53:34 [INFO]   Policy Loss: 0.182863 (recent avg: 0.174799)
06:53:34 [INFO]   Value Loss: 19.297379
06:53:34 [INFO]   Entropy: -18.585955
06:53:34 [INFO]   Memory Div Reward: -2.668089 (recent avg: -2.784398)
06:53:34 [INFO]   WM Uncertainty: 4.585557
06:53:34 [INFO]   Learning Rate: 1.00e-06
06:53:34 [INFO]   Total Steps: 5650
06:53:34 [INFO]   Log Std: mean=-1.9997, std=0.0002
06:53:34 [INFO] ================================================================================
06:53:34 [INFO] Episode 112: reward=95.87, p_loss=0.1829, v_loss=19.2974, entropy=-18.5860, mem_div=-2.6681, wm_unc=4.5856, lr=1.00e-06
Training Episodes:   6%|         | 113/2000 [12:53:09<194:26:09, 370.94s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
06:53:36 [INFO] [Step 0] First memory divergence measurement: 56.7528, setting reward=0
06:55:13 [INFO] [Episode 113] Completed - Detailed Statistics:
06:55:13 [INFO]   Total steps: 50
06:55:13 [INFO]   Episode reward: 83.9927
06:55:13 [INFO]   Avg memory div reward: -2.782090
06:55:13 [INFO]   Avg memory div abs: 56.740477
06:55:13 [INFO]   Avg WM uncertainty: 4.461944
06:55:13 [INFO]   Min/Max reward: 1.3108 / 4.6017
06:56:17 [INFO] [Episode 113] PPO Epoch 0 - Training Losses:
06:56:17 [INFO]   Policy loss: 0.128808
06:56:17 [INFO]   Value loss: 17.519360
06:56:17 [INFO]   Entropy: -18.584837
06:56:17 [INFO]   Total loss: 9.074336
06:56:31 [INFO] Memory Gate Act: -0.000959
06:59:13 [INFO] Memory Gate WM: 0.001725
06:59:37 [INFO] ================================================================================
06:59:37 [INFO] Episode 113 - Aggregated Metrics Summary:
06:59:37 [INFO]   Reward: 95.7091 (recent avg: 89.9764, std: 3.8304)
06:59:37 [INFO]   Policy Loss: 0.181606 (recent avg: 0.165831)
06:59:37 [INFO]   Value Loss: 19.234654
06:59:37 [INFO]   Entropy: -18.585872
06:59:37 [INFO]   Memory Div Reward: -2.671047 (recent avg: -2.782553)
06:59:37 [INFO]   WM Uncertainty: 4.585229
06:59:37 [INFO]   Learning Rate: 1.00e-06
06:59:37 [INFO]   Total Steps: 5700
06:59:37 [INFO]   Log Std: mean=-1.9997, std=0.0002
06:59:37 [INFO] ================================================================================
06:59:37 [INFO] Episode 113: reward=95.71, p_loss=0.1816, v_loss=19.2347, entropy=-18.5859, mem_div=-2.6710, wm_unc=4.5852, lr=1.00e-06
Training Episodes:   6%|         | 114/2000 [12:59:13<193:11:18, 368.76s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
06:59:40 [INFO] [Step 0] First memory divergence measurement: 56.7590, setting reward=0
07:01:17 [INFO] [Episode 114] Completed - Detailed Statistics:
07:01:17 [INFO]   Total steps: 50
07:01:17 [INFO]   Episode reward: 88.1843
07:01:17 [INFO]   Avg memory div reward: -2.785281
07:01:17 [INFO]   Avg memory div abs: 56.782081
07:01:17 [INFO]   Avg WM uncertainty: 4.548968
07:01:17 [INFO]   Min/Max reward: 1.3971 / 4.6751
07:02:23 [INFO] [Episode 114] PPO Epoch 0 - Training Losses:
07:02:23 [INFO]   Policy loss: 0.093528
07:02:23 [INFO]   Value loss: 18.587904
07:02:23 [INFO]   Entropy: -18.584803
07:02:23 [INFO]   Total loss: 9.573328
07:05:46 [INFO] ================================================================================
07:05:46 [INFO] Episode 114 - Aggregated Metrics Summary:
07:05:46 [INFO]   Reward: 95.6219 (recent avg: 89.8144, std: 3.8683)
07:05:46 [INFO]   Policy Loss: 0.173827 (recent avg: 0.122123)
07:05:46 [INFO]   Value Loss: 19.116275
07:05:46 [INFO]   Entropy: -18.585788
07:05:46 [INFO]   Memory Div Reward: -2.673927 (recent avg: -2.782627)
07:05:46 [INFO]   WM Uncertainty: 4.586365
07:05:46 [INFO]   Learning Rate: 1.00e-06
07:05:46 [INFO]   Total Steps: 5750
07:05:46 [INFO]   Log Std: mean=-1.9997, std=0.0002
07:05:46 [INFO] ================================================================================
07:05:46 [INFO] Episode 114: reward=95.62, p_loss=0.1738, v_loss=19.1163, entropy=-18.5858, mem_div=-2.6739, wm_unc=4.5864, lr=1.00e-06
Training Episodes:   6%|         | 115/2000 [13:05:22<193:02:51, 368.68s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
07:05:49 [INFO] [Step 0] First memory divergence measurement: 56.6039, setting reward=0
07:07:26 [INFO] [Episode 115] Completed - Detailed Statistics:
07:07:26 [INFO]   Total steps: 50
07:07:26 [INFO]   Episode reward: 92.1999
07:07:26 [INFO]   Avg memory div reward: -2.775630
07:07:26 [INFO]   Avg memory div abs: 56.749145
07:07:26 [INFO]   Avg WM uncertainty: 4.619628
07:07:26 [INFO]   Min/Max reward: 1.2083 / 4.7077
07:07:26 [INFO] Memory Gate WM: 0.001724
07:08:32 [INFO] [Episode 115] PPO Epoch 0 - Training Losses:
07:08:32 [INFO]   Policy loss: 0.108973
07:08:32 [INFO]   Value loss: 19.351622
07:08:32 [INFO]   Entropy: -18.584756
07:08:32 [INFO]   Total loss: 9.970632
07:08:51 [INFO] Memory Gate Act: -0.000972
07:10:11 [INFO] Memory Gate Act: -0.000966
07:11:15 [INFO] Memory Gate Act: -0.000963
07:11:58 [INFO] ================================================================================
07:11:58 [INFO] Episode 115 - Aggregated Metrics Summary:
07:11:58 [INFO]   Reward: 95.4863 (recent avg: 89.7025, std: 3.7804)
07:11:58 [INFO]   Policy Loss: 0.172966 (recent avg: 0.114948)
07:11:58 [INFO]   Value Loss: 19.108978
07:11:58 [INFO]   Entropy: -18.585707
07:11:58 [INFO]   Memory Div Reward: -2.676733 (recent avg: -2.781038)
07:11:58 [INFO]   WM Uncertainty: 4.586460
07:11:58 [INFO]   Learning Rate: 1.00e-06
07:11:58 [INFO]   Total Steps: 5800
07:11:58 [INFO]   Log Std: mean=-1.9997, std=0.0002
07:11:58 [INFO] ================================================================================
07:11:58 [INFO] Episode 115: reward=95.49, p_loss=0.1730, v_loss=19.1090, entropy=-18.5857, mem_div=-2.6767, wm_unc=4.5865, lr=1.00e-06
Training Episodes:   6%|         | 116/2000 [13:11:33<193:25:06, 369.59s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
07:12:00 [INFO] [Step 0] First memory divergence measurement: 56.4873, setting reward=0
07:13:38 [INFO] [Episode 116] Completed - Detailed Statistics:
07:13:38 [INFO]   Total steps: 50
07:13:38 [INFO]   Episode reward: 91.5775
07:13:38 [INFO]   Avg memory div reward: -2.782048
07:13:38 [INFO]   Avg memory div abs: 56.666516
07:13:38 [INFO]   Avg WM uncertainty: 4.613597
07:13:38 [INFO]   Min/Max reward: 1.3545 / 4.5065
07:13:49 [INFO] Memory Gate WM: 0.001705
07:14:44 [INFO] [Episode 116] PPO Epoch 0 - Training Losses:
07:14:44 [INFO]   Policy loss: 0.115100
07:14:44 [INFO]   Value loss: 19.373844
07:14:44 [INFO]   Entropy: -18.584699
07:14:44 [INFO]   Total loss: 9.987869
07:17:31 [INFO] Memory Gate Act: -0.000970
07:18:06 [INFO] ================================================================================
07:18:06 [INFO] Episode 116 - Aggregated Metrics Summary:
07:18:06 [INFO]   Reward: 95.4271 (recent avg: 89.3822, std: 3.4587)
07:18:06 [INFO]   Policy Loss: 0.171601 (recent avg: 0.120660)
07:18:06 [INFO]   Value Loss: 19.129604
07:18:06 [INFO]   Entropy: -18.585625
07:18:06 [INFO]   Memory Div Reward: -2.679553 (recent avg: -2.781525)
07:18:06 [INFO]   WM Uncertainty: 4.588095
07:18:06 [INFO]   Learning Rate: 1.00e-06
07:18:06 [INFO]   Total Steps: 5850
07:18:06 [INFO]   Log Std: mean=-1.9997, std=0.0002
07:18:06 [INFO] ================================================================================
07:18:06 [INFO] Episode 116: reward=95.43, p_loss=0.1716, v_loss=19.1296, entropy=-18.5856, mem_div=-2.6796, wm_unc=4.5881, lr=1.00e-06
Training Episodes:   6%|         | 117/2000 [13:17:41<193:04:39, 369.13s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
07:18:08 [INFO] [Step 0] First memory divergence measurement: 56.9075, setting reward=0
07:18:55 [INFO] Memory Gate WM: 0.001709
07:19:47 [INFO] [Episode 117] Completed - Detailed Statistics:
07:19:47 [INFO]   Total steps: 50
07:19:47 [INFO]   Episode reward: 89.1879
07:19:47 [INFO]   Avg memory div reward: -2.779294
07:19:47 [INFO]   Avg memory div abs: 56.815096
07:19:47 [INFO]   Avg WM uncertainty: 4.563051
07:19:47 [INFO]   Min/Max reward: 1.5164 / 4.6337
07:20:52 [INFO] [Episode 117] PPO Epoch 0 - Training Losses:
07:20:52 [INFO]   Policy loss: 0.155621
07:20:52 [INFO]   Value loss: 18.594158
07:20:52 [INFO]   Entropy: -18.584617
07:20:52 [INFO]   Total loss: 9.638546
07:23:04 [INFO] Memory Gate WM: 0.001716
07:24:09 [INFO] ================================================================================
07:24:09 [INFO] Episode 117 - Aggregated Metrics Summary:
07:24:09 [INFO]   Reward: 95.2950 (recent avg: 89.3790, std: 3.4589)
07:24:09 [INFO]   Policy Loss: 0.172075 (recent avg: 0.140918)
07:24:09 [INFO]   Value Loss: 19.116454
07:24:09 [INFO]   Entropy: -18.585544
07:24:09 [INFO]   Memory Div Reward: -2.682281 (recent avg: -2.779915)
07:24:09 [INFO]   WM Uncertainty: 4.588181
07:24:09 [INFO]   Learning Rate: 1.00e-06
07:24:09 [INFO]   Total Steps: 5900
07:24:09 [INFO]   Log Std: mean=-1.9997, std=0.0002
07:24:09 [INFO] ================================================================================
07:24:09 [INFO] Episode 117: reward=95.30, p_loss=0.1721, v_loss=19.1165, entropy=-18.5855, mem_div=-2.6823, wm_unc=4.5882, lr=1.00e-06
Training Episodes:   6%|         | 118/2000 [13:23:45<192:04:08, 367.40s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
07:24:11 [INFO] [Step 0] First memory divergence measurement: 56.7223, setting reward=0
07:25:03 [INFO] Memory Gate WM: 0.001718
07:25:48 [INFO] [Episode 118] Completed - Detailed Statistics:
07:25:48 [INFO]   Total steps: 50
07:25:48 [INFO]   Episode reward: 95.3535
07:25:48 [INFO]   Avg memory div reward: -2.781656
07:25:48 [INFO]   Avg memory div abs: 56.714491
07:25:48 [INFO]   Avg WM uncertainty: 4.688726
07:25:48 [INFO]   Min/Max reward: 1.2778 / 4.6142
07:26:54 [INFO] [Episode 118] PPO Epoch 0 - Training Losses:
07:26:54 [INFO]   Policy loss: 0.243356
07:26:54 [INFO]   Value loss: 20.334967
07:26:54 [INFO]   Entropy: -18.584583
07:26:54 [INFO]   Total loss: 10.596685
07:30:13 [INFO] ================================================================================
07:30:13 [INFO] Episode 118 - Aggregated Metrics Summary:
07:30:13 [INFO]   Reward: 95.2036 (recent avg: 89.8482, std: 3.8922)
07:30:13 [INFO]   Policy Loss: 0.169898 (recent avg: 0.159517)
07:30:13 [INFO]   Value Loss: 19.225493
07:30:13 [INFO]   Entropy: -18.585459
07:30:13 [INFO]   Memory Div Reward: -2.685085 (recent avg: -2.781798)
07:30:13 [INFO]   WM Uncertainty: 4.589157
07:30:13 [INFO]   Learning Rate: 1.00e-06
07:30:13 [INFO]   Total Steps: 5950
07:30:13 [INFO]   Log Std: mean=-1.9997, std=0.0002
07:30:13 [INFO] ================================================================================
07:30:13 [INFO] Episode 118: reward=95.20, p_loss=0.1699, v_loss=19.2255, entropy=-18.5855, mem_div=-2.6851, wm_unc=4.5892, lr=1.00e-06
Training Episodes:   6%|         | 119/2000 [13:29:49<191:29:29, 366.49s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
07:30:16 [INFO] [Step 0] First memory divergence measurement: 56.8150, setting reward=0
07:30:35 [INFO] Memory Gate WM: 0.001722
07:30:41 [INFO] Memory Gate Act: -0.000985
07:31:54 [INFO] [Episode 119] Completed - Detailed Statistics:
07:31:54 [INFO]   Total steps: 50
07:31:54 [INFO]   Episode reward: 90.0556
07:31:54 [INFO]   Avg memory div reward: -2.766333
07:31:54 [INFO]   Avg memory div abs: 56.604780
07:31:54 [INFO]   Avg WM uncertainty: 4.567445
07:31:54 [INFO]   Min/Max reward: 1.4663 / 4.2915
07:33:00 [INFO] [Episode 119] PPO Epoch 0 - Training Losses:
07:33:00 [INFO]   Policy loss: 0.140020
07:33:00 [INFO]   Value loss: 19.270679
07:33:00 [INFO]   Entropy: -18.584411
07:33:00 [INFO]   Total loss: 9.961204
07:33:23 [INFO] Memory Gate Act: -0.000985
07:36:28 [INFO] ================================================================================
07:36:28 [INFO] Episode 119 - Aggregated Metrics Summary:
07:36:28 [INFO]   Reward: 95.0668 (recent avg: 90.1122, std: 3.8068)
07:36:28 [INFO]   Policy Loss: 0.168531 (recent avg: 0.144833)
07:36:28 [INFO]   Value Loss: 19.230135
07:36:28 [INFO]   Entropy: -18.585368
07:36:28 [INFO]   Memory Div Reward: -2.687695 (recent avg: -2.779425)
07:36:28 [INFO]   WM Uncertainty: 4.589031
07:36:28 [INFO]   Learning Rate: 1.00e-06
07:36:28 [INFO]   Total Steps: 6000
07:36:28 [INFO]   Log Std: mean=-1.9997, std=0.0002
07:36:28 [INFO] ================================================================================
07:36:28 [INFO] Episode 119: reward=95.07, p_loss=0.1685, v_loss=19.2301, entropy=-18.5854, mem_div=-2.6877, wm_unc=4.5890, lr=1.00e-06
07:36:30 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000120.mp4
Training Episodes:   6%|         | 120/2000 [13:36:05<192:54:58, 369.41s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
07:36:32 [INFO] [Step 0] First memory divergence measurement: 56.5900, setting reward=0
07:37:04 [INFO] Memory Gate WM: 0.001718
07:38:11 [INFO] [Episode 120] Completed - Detailed Statistics:
07:38:11 [INFO]   Total steps: 50
07:38:11 [INFO]   Episode reward: 98.9628
07:38:11 [INFO]   Avg memory div reward: -2.783368
07:38:11 [INFO]   Avg memory div abs: 56.748818
07:38:11 [INFO]   Avg WM uncertainty: 4.762624
07:38:11 [INFO]   Min/Max reward: 1.5281 / 4.6419
07:39:17 [INFO] [Episode 120] PPO Epoch 0 - Training Losses:
07:39:17 [INFO]   Policy loss: 0.140562
07:39:17 [INFO]   Value loss: 21.309862
07:39:17 [INFO]   Entropy: -18.584258
07:39:17 [INFO]   Total loss: 10.981335
07:42:19 [INFO] Memory Gate Act: -0.000971
07:42:38 [INFO] ================================================================================
07:42:38 [INFO] Episode 120 - Aggregated Metrics Summary:
07:42:38 [INFO]   Reward: 95.0090 (recent avg: 91.6021, std: 4.0555)
07:42:38 [INFO]   Policy Loss: 0.167789 (recent avg: 0.118005)
07:42:38 [INFO]   Value Loss: 19.285517
07:42:38 [INFO]   Entropy: -18.585275
07:42:38 [INFO]   Memory Div Reward: -2.690486 (recent avg: -2.779580)
07:42:38 [INFO]   WM Uncertainty: 4.590666
07:42:38 [INFO]   Learning Rate: 1.00e-06
07:42:38 [INFO]   Total Steps: 6050
07:42:38 [INFO]   Log Std: mean=-1.9997, std=0.0002
07:42:38 [INFO] ================================================================================
07:42:38 [INFO] Episode 120: reward=95.01, p_loss=0.1678, v_loss=19.2855, entropy=-18.5853, mem_div=-2.6905, wm_unc=4.5907, lr=1.00e-06
Training Episodes:   6%|         | 121/2000 [13:42:13<192:35:55, 369.00s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
07:42:41 [INFO] [Step 0] First memory divergence measurement: 56.8779, setting reward=0
07:44:19 [INFO] [Episode 121] Completed - Detailed Statistics:
07:44:19 [INFO]   Total steps: 50
07:44:19 [INFO]   Episode reward: 94.6356
07:44:19 [INFO]   Avg memory div reward: -2.783615
07:44:19 [INFO]   Avg memory div abs: 56.842920
07:44:19 [INFO]   Avg WM uncertainty: 4.676326
07:44:19 [INFO]   Min/Max reward: 1.5309 / 4.7087
07:45:24 [INFO] [Episode 121] PPO Epoch 0 - Training Losses:
07:45:24 [INFO]   Policy loss: 0.114994
07:45:24 [INFO]   Value loss: 20.023131
07:45:24 [INFO]   Entropy: -18.584128
07:45:24 [INFO]   Total loss: 10.312401
07:48:45 [INFO] Memory Gate Act: -0.000960
07:48:52 [INFO] ================================================================================
07:48:52 [INFO] Episode 121 - Aggregated Metrics Summary:
07:48:52 [INFO]   Reward: 94.8174 (recent avg: 91.4893, std: 3.9526)
07:48:52 [INFO]   Policy Loss: 0.166338 (recent avg: 0.115889)
07:48:52 [INFO]   Value Loss: 19.294461
07:48:52 [INFO]   Entropy: -18.585181
07:48:52 [INFO]   Memory Div Reward: -2.693217 (recent avg: -2.779752)
07:48:52 [INFO]   WM Uncertainty: 4.589565
07:48:52 [INFO]   Learning Rate: 1.00e-06
07:48:52 [INFO]   Total Steps: 6100
07:48:52 [INFO]   Log Std: mean=-1.9997, std=0.0002
07:48:52 [INFO] ================================================================================
07:48:52 [INFO] Episode 121: reward=94.82, p_loss=0.1663, v_loss=19.2945, entropy=-18.5852, mem_div=-2.6932, wm_unc=4.5896, lr=1.00e-06
Training Episodes:   6%|         | 122/2000 [13:48:28<193:19:32, 370.59s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
07:48:55 [INFO] [Step 0] First memory divergence measurement: 56.2696, setting reward=0
07:50:36 [INFO] Memory Gate WM: 0.001719
07:50:38 [INFO] [Episode 122] Completed - Detailed Statistics:
07:50:38 [INFO]   Total steps: 50
07:50:38 [INFO]   Episode reward: 93.3470
07:50:38 [INFO]   Avg memory div reward: -2.770456
07:50:38 [INFO]   Avg memory div abs: 56.383179
07:50:38 [INFO]   Avg WM uncertainty: 4.637397
07:50:38 [INFO]   Min/Max reward: 1.3261 / 4.8616
07:51:44 [INFO] [Episode 122] PPO Epoch 0 - Training Losses:
07:51:44 [INFO]   Policy loss: 0.138385
07:51:44 [INFO]   Value loss: 19.590328
07:51:44 [INFO]   Entropy: -18.584062
07:51:44 [INFO]   Total loss: 10.119390
07:52:21 [INFO] Memory Gate Act: -0.000966
07:53:55 [INFO] Memory Gate Act: -0.000973
07:54:11 [INFO] Memory Gate Act: -0.000973
07:55:10 [INFO] ================================================================================
07:55:10 [INFO] Episode 122 - Aggregated Metrics Summary:
07:55:10 [INFO]   Reward: 94.7102 (recent avg: 91.7497, std: 3.9805)
07:55:10 [INFO]   Policy Loss: 0.166546 (recent avg: 0.134571)
07:55:10 [INFO]   Value Loss: 19.272575
07:55:10 [INFO]   Entropy: -18.585087
07:55:10 [INFO]   Memory Div Reward: -2.695816 (recent avg: -2.778977)
07:55:10 [INFO]   WM Uncertainty: 4.590020
07:55:10 [INFO]   Learning Rate: 1.00e-06
07:55:10 [INFO]   Total Steps: 6150
07:55:10 [INFO]   Log Std: mean=-1.9997, std=0.0002
07:55:10 [INFO] ================================================================================
07:55:10 [INFO] Episode 122: reward=94.71, p_loss=0.1665, v_loss=19.2726, entropy=-18.5851, mem_div=-2.6958, wm_unc=4.5900, lr=1.00e-06
Training Episodes:   6%|         | 123/2000 [13:54:46<194:25:07, 372.89s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
07:55:13 [INFO] [Step 0] First memory divergence measurement: 57.0414, setting reward=0
07:56:54 [INFO] Memory Gate WM: 0.001732
07:56:56 [INFO] [Episode 123] Completed - Detailed Statistics:
07:56:56 [INFO]   Total steps: 50
07:56:56 [INFO]   Episode reward: 79.6552
07:56:56 [INFO]   Avg memory div reward: -2.789101
07:56:56 [INFO]   Avg memory div abs: 56.990022
07:56:56 [INFO]   Avg WM uncertainty: 4.382204
07:56:56 [INFO]   Min/Max reward: 1.3149 / 4.4869
07:57:05 [INFO] Memory Gate Act: -0.000975
07:57:21 [INFO] Memory Gate Act: -0.000977
07:58:01 [INFO] [Episode 123] PPO Epoch 0 - Training Losses:
07:58:01 [INFO]   Policy loss: 0.118781
07:58:01 [INFO]   Value loss: 16.346435
07:58:01 [INFO]   Entropy: -18.583976
07:58:01 [INFO]   Total loss: 8.477837
07:58:58 [INFO] Memory Gate Act: -0.000969
07:59:36 [INFO] Memory Gate WM: 0.001722
08:01:28 [INFO] ================================================================================
08:01:28 [INFO] Episode 123 - Aggregated Metrics Summary:
08:01:28 [INFO]   Reward: 94.5249 (recent avg: 91.3159, std: 4.9261)
08:01:28 [INFO]   Policy Loss: 0.159871 (recent avg: 0.148619)
08:01:28 [INFO]   Value Loss: 19.196709
08:01:28 [INFO]   Entropy: -18.584999
08:01:28 [INFO]   Memory Div Reward: -2.698556 (recent avg: -2.779678)
08:01:28 [INFO]   WM Uncertainty: 4.589055
08:01:28 [INFO]   Learning Rate: 1.00e-06
08:01:28 [INFO]   Total Steps: 6200
08:01:28 [INFO]   Log Std: mean=-1.9997, std=0.0002
08:01:28 [INFO] ================================================================================
08:01:28 [INFO] Episode 123: reward=94.52, p_loss=0.1599, v_loss=19.1967, entropy=-18.5850, mem_div=-2.6986, wm_unc=4.5891, lr=1.00e-06
Training Episodes:   6%|         | 124/2000 [14:01:04<195:05:09, 374.37s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
08:01:31 [INFO] [Step 0] First memory divergence measurement: 56.7290, setting reward=0
08:03:13 [INFO] Memory Gate Act: -0.000953
08:03:14 [INFO] [Episode 124] Completed - Detailed Statistics:
08:03:14 [INFO]   Total steps: 50
08:03:14 [INFO]   Episode reward: 92.0768
08:03:14 [INFO]   Avg memory div reward: -2.767763
08:03:14 [INFO]   Avg memory div abs: 56.721400
08:03:14 [INFO]   Avg WM uncertainty: 4.609299
08:03:14 [INFO]   Min/Max reward: 1.3995 / 4.7514
08:04:17 [INFO] Memory Gate Act: -0.000946
08:04:24 [INFO] [Episode 124] PPO Epoch 0 - Training Losses:
08:04:24 [INFO]   Policy loss: 0.101565
08:04:24 [INFO]   Value loss: 19.578074
08:04:24 [INFO]   Entropy: -18.583864
08:04:24 [INFO]   Total loss: 10.076441
08:07:51 [INFO] ================================================================================
08:07:51 [INFO] Episode 124 - Aggregated Metrics Summary:
08:07:51 [INFO]   Reward: 94.3766 (recent avg: 91.7052, std: 4.8159)
08:07:51 [INFO]   Policy Loss: 0.155760 (recent avg: 0.125271)
08:07:51 [INFO]   Value Loss: 19.205080
08:07:51 [INFO]   Entropy: -18.584915
08:07:51 [INFO]   Memory Div Reward: -2.701090 (recent avg: -2.777926)
08:07:51 [INFO]   WM Uncertainty: 4.588623
08:07:51 [INFO]   Learning Rate: 1.00e-06
08:07:51 [INFO]   Total Steps: 6250
08:07:51 [INFO]   Log Std: mean=-1.9997, std=0.0002
08:07:51 [INFO] ================================================================================
08:07:51 [INFO] Episode 124: reward=94.38, p_loss=0.1558, v_loss=19.2051, entropy=-18.5849, mem_div=-2.7011, wm_unc=4.5886, lr=1.00e-06
Training Episodes:   6%|         | 125/2000 [14:07:26<196:15:43, 376.82s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
08:07:53 [INFO] [Step 0] First memory divergence measurement: 56.7223, setting reward=0
08:08:17 [INFO] Memory Gate Act: -0.000960
08:09:35 [INFO] [Episode 125] Completed - Detailed Statistics:
08:09:35 [INFO]   Total steps: 50
08:09:35 [INFO]   Episode reward: 88.2782
08:09:35 [INFO]   Avg memory div reward: -2.785514
08:09:35 [INFO]   Avg memory div abs: 56.806594
08:09:35 [INFO]   Avg WM uncertainty: 4.551078
08:09:35 [INFO]   Min/Max reward: 1.3812 / 4.7600
08:10:43 [INFO] [Episode 125] PPO Epoch 0 - Training Losses:
08:10:43 [INFO]   Policy loss: 0.190112
08:10:43 [INFO]   Value loss: 18.360496
08:10:43 [INFO]   Entropy: -18.583646
08:10:43 [INFO]   Total loss: 9.556196
08:11:05 [INFO] Memory Gate WM: 0.001744
08:11:57 [INFO] Memory Gate WM: 0.001748
08:14:07 [INFO] ================================================================================
08:14:07 [INFO] Episode 125 - Aggregated Metrics Summary:
08:14:07 [INFO]   Reward: 94.2134 (recent avg: 91.3130, std: 4.9182)
08:14:07 [INFO]   Policy Loss: 0.152669 (recent avg: 0.124598)
08:14:07 [INFO]   Value Loss: 19.161215
08:14:07 [INFO]   Entropy: -18.584829
08:14:07 [INFO]   Memory Div Reward: -2.703822 (recent avg: -2.778915)
08:14:07 [INFO]   WM Uncertainty: 4.588090
08:14:07 [INFO]   Learning Rate: 1.00e-06
08:14:07 [INFO]   Total Steps: 6300
08:14:07 [INFO]   Log Std: mean=-1.9997, std=0.0002
08:14:07 [INFO] ================================================================================
08:14:07 [INFO] Episode 125: reward=94.21, p_loss=0.1527, v_loss=19.1612, entropy=-18.5848, mem_div=-2.7038, wm_unc=4.5881, lr=1.00e-06
Training Episodes:   6%|         | 126/2000 [14:13:42<196:02:11, 376.59s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
08:14:09 [INFO] Memory Gate Act: -0.000970
08:14:10 [INFO] [Step 0] First memory divergence measurement: 56.8269, setting reward=0
08:14:37 [INFO] Memory Gate Act: -0.000970
08:14:38 [INFO] Memory Gate WM: 0.001742
08:15:57 [INFO] [Episode 126] Completed - Detailed Statistics:
08:15:57 [INFO]   Total steps: 50
08:15:57 [INFO]   Episode reward: 86.6995
08:15:57 [INFO]   Avg memory div reward: -2.796184
08:15:57 [INFO]   Avg memory div abs: 56.989018
08:15:57 [INFO]   Avg WM uncertainty: 4.530174
08:15:57 [INFO]   Min/Max reward: 1.3900 / 4.5527
08:16:11 [INFO] Memory Gate WM: 0.001742
08:17:05 [INFO] [Episode 126] PPO Epoch 0 - Training Losses:
08:17:06 [INFO]   Policy loss: 0.118142
08:17:06 [INFO]   Value loss: 18.302157
08:17:06 [INFO]   Entropy: -18.583684
08:17:06 [INFO]   Total loss: 9.455058
08:19:25 [INFO] Memory Gate Act: -0.000965
08:20:18 [INFO] Memory Gate WM: 0.001743
08:20:32 [INFO] ================================================================================
08:20:32 [INFO] Episode 126 - Aggregated Metrics Summary:
08:20:32 [INFO]   Reward: 94.0322 (recent avg: 90.8252, std: 5.1061)
08:20:32 [INFO]   Policy Loss: 0.151466 (recent avg: 0.108858)
08:20:32 [INFO]   Value Loss: 19.119795
08:20:32 [INFO]   Entropy: -18.584744
08:20:32 [INFO]   Memory Div Reward: -2.706648 (recent avg: -2.780328)
08:20:32 [INFO]   WM Uncertainty: 4.587291
08:20:32 [INFO]   Learning Rate: 1.00e-06
08:20:32 [INFO]   Total Steps: 6350
08:20:32 [INFO]   Log Std: mean=-1.9997, std=0.0002
08:20:32 [INFO] ================================================================================
08:20:32 [INFO] Episode 126: reward=94.03, p_loss=0.1515, v_loss=19.1198, entropy=-18.5847, mem_div=-2.7066, wm_unc=4.5873, lr=1.00e-06
Training Episodes:   6%|         | 127/2000 [14:20:08<197:18:41, 379.24s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
08:20:35 [INFO] [Step 0] First memory divergence measurement: 56.8006, setting reward=0
08:21:28 [INFO] Memory Gate WM: 0.001743
08:22:23 [INFO] [Episode 127] Completed - Detailed Statistics:
08:22:23 [INFO]   Total steps: 50
08:22:23 [INFO]   Episode reward: 92.6331
08:22:23 [INFO]   Avg memory div reward: -2.787973
08:22:23 [INFO]   Avg memory div abs: 56.838096
08:22:23 [INFO]   Avg WM uncertainty: 4.640636
08:22:23 [INFO]   Min/Max reward: 1.4146 / 4.6739
08:23:31 [INFO] [Episode 127] PPO Epoch 0 - Training Losses:
08:23:31 [INFO]   Policy loss: 0.113238
08:23:31 [INFO]   Value loss: 19.478170
08:23:31 [INFO]   Entropy: -18.583560
08:23:31 [INFO]   Total loss: 10.038159
08:24:55 [INFO] Memory Gate WM: 0.001765
08:26:51 [INFO] ================================================================================
08:26:51 [INFO] Episode 127 - Aggregated Metrics Summary:
08:26:51 [INFO]   Reward: 93.9208 (recent avg: 91.1697, std: 5.1002)
08:26:51 [INFO]   Policy Loss: 0.146315 (recent avg: 0.086783)
08:26:51 [INFO]   Value Loss: 19.121366
08:26:51 [INFO]   Entropy: -18.584659
08:26:51 [INFO]   Memory Div Reward: -2.709360 (recent avg: -2.781196)
08:26:51 [INFO]   WM Uncertainty: 4.587776
08:26:51 [INFO]   Learning Rate: 1.00e-06
08:26:51 [INFO]   Total Steps: 6400
08:26:51 [INFO]   Log Std: mean=-1.9997, std=0.0002
08:26:51 [INFO] ================================================================================
08:26:51 [INFO] Episode 127: reward=93.92, p_loss=0.1463, v_loss=19.1214, entropy=-18.5847, mem_div=-2.7094, wm_unc=4.5878, lr=1.00e-06
Training Episodes:   6%|         | 128/2000 [14:26:27<197:11:51, 379.23s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
08:26:54 [INFO] [Step 0] First memory divergence measurement: 57.0573, setting reward=0
08:28:41 [INFO] [Episode 128] Completed - Detailed Statistics:
08:28:41 [INFO]   Total steps: 50
08:28:41 [INFO]   Episode reward: 97.0523
08:28:41 [INFO]   Avg memory div reward: -2.798736
08:28:41 [INFO]   Avg memory div abs: 57.080819
08:28:41 [INFO]   Avg WM uncertainty: 4.739782
08:28:41 [INFO]   Min/Max reward: 1.4319 / 4.7920
08:29:48 [INFO] [Episode 128] PPO Epoch 0 - Training Losses:
08:29:48 [INFO]   Policy loss: 0.047198
08:29:48 [INFO]   Value loss: 20.870035
08:29:48 [INFO]   Entropy: -18.583519
08:29:48 [INFO]   Total loss: 10.668050
08:30:03 [INFO] Memory Gate WM: 0.001761
08:33:02 [INFO] ================================================================================
08:33:02 [INFO] Episode 128 - Aggregated Metrics Summary:
08:33:02 [INFO]   Reward: 93.9056 (recent avg: 91.3396, std: 5.2625)
08:33:02 [INFO]   Policy Loss: 0.141482 (recent avg: 0.069430)
08:33:02 [INFO]   Value Loss: 19.182070
08:33:02 [INFO]   Entropy: -18.584575
08:33:02 [INFO]   Memory Div Reward: -2.712015 (recent avg: -2.782904)
08:33:02 [INFO]   WM Uncertainty: 4.590127
08:33:02 [INFO]   Learning Rate: 1.00e-06
08:33:02 [INFO]   Total Steps: 6450
08:33:02 [INFO]   Log Std: mean=-1.9997, std=0.0002
08:33:02 [INFO] ================================================================================
08:33:02 [INFO] Episode 128: reward=93.91, p_loss=0.1415, v_loss=19.1821, entropy=-18.5846, mem_div=-2.7120, wm_unc=4.5901, lr=1.00e-06
Training Episodes:   6%|         | 129/2000 [14:32:38<195:47:44, 376.73s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
08:33:05 [INFO] [Step 0] First memory divergence measurement: 56.8741, setting reward=0
08:33:28 [INFO] Memory Gate Act: -0.000962
08:34:06 [INFO] Memory Gate WM: 0.001760
08:34:56 [INFO] [Episode 129] Completed - Detailed Statistics:
08:34:56 [INFO]   Total steps: 50
08:34:56 [INFO]   Episode reward: 88.7034
08:34:56 [INFO]   Avg memory div reward: -2.793364
08:34:56 [INFO]   Avg memory div abs: 56.990565
08:34:56 [INFO]   Avg WM uncertainty: 4.567431
08:34:56 [INFO]   Min/Max reward: 1.3056 / 4.5031
08:35:18 [INFO] Memory Gate WM: 0.001759
08:35:29 [INFO] Memory Gate Act: -0.000962
08:36:03 [INFO] [Episode 129] PPO Epoch 0 - Training Losses:
08:36:03 [INFO]   Policy loss: 0.293279
08:36:03 [INFO]   Value loss: 18.642278
08:36:03 [INFO]   Entropy: -18.583496
08:36:03 [INFO]   Total loss: 9.800253
08:38:42 [INFO] Memory Gate WM: 0.001773
08:39:31 [INFO] ================================================================================
08:39:31 [INFO] Episode 129 - Aggregated Metrics Summary:
08:39:31 [INFO]   Reward: 93.8396 (recent avg: 91.2044, std: 5.3109)
08:39:31 [INFO]   Policy Loss: 0.139818 (recent avg: 0.098078)
08:39:31 [INFO]   Value Loss: 19.161484
08:39:31 [INFO]   Entropy: -18.584488
08:39:31 [INFO]   Memory Div Reward: -2.714536 (recent avg: -2.785607)
08:39:31 [INFO]   WM Uncertainty: 4.591329
08:39:31 [INFO]   Learning Rate: 1.00e-06
08:39:31 [INFO]   Total Steps: 6500
08:39:31 [INFO]   Log Std: mean=-1.9997, std=0.0002
08:39:31 [INFO] ================================================================================
08:39:31 [INFO] Episode 129: reward=93.84, p_loss=0.1398, v_loss=19.1615, entropy=-18.5845, mem_div=-2.7145, wm_unc=4.5913, lr=1.00e-06
08:39:33 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000130.mp4
Training Episodes:   6%|         | 130/2000 [14:39:09<197:54:32, 381.00s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
08:39:37 [INFO] [Step 0] First memory divergence measurement: 57.1076, setting reward=0
08:39:57 [INFO] Memory Gate WM: 0.001773
08:41:29 [INFO] [Episode 130] Completed - Detailed Statistics:
08:41:29 [INFO]   Total steps: 50
08:41:29 [INFO]   Episode reward: 95.1285
08:41:29 [INFO]   Avg memory div reward: -2.793239
08:41:29 [INFO]   Avg memory div abs: 57.002924
08:41:29 [INFO]   Avg WM uncertainty: 4.695809
08:41:29 [INFO]   Min/Max reward: 1.3833 / 4.9081
08:41:38 [INFO] Memory Gate WM: 0.001772
08:41:54 [INFO] Memory Gate WM: 0.001772
08:42:36 [INFO] [Episode 130] PPO Epoch 0 - Training Losses:
08:42:36 [INFO]   Policy loss: 0.081583
08:42:36 [INFO]   Value loss: 20.220273
08:42:36 [INFO]   Entropy: -18.583303
08:42:36 [INFO]   Total loss: 10.377552
08:42:51 [INFO] Memory Gate Act: -0.000991
08:44:34 [INFO] Memory Gate WM: 0.001778
08:46:05 [INFO] ================================================================================
08:46:05 [INFO] Episode 130 - Aggregated Metrics Summary:
08:46:05 [INFO]   Reward: 93.8544 (recent avg: 90.8210, std: 4.8558)
08:46:05 [INFO]   Policy Loss: 0.139820 (recent avg: 0.111367)
08:46:05 [INFO]   Value Loss: 19.174928
08:46:05 [INFO]   Entropy: -18.584396
08:46:05 [INFO]   Memory Div Reward: -2.717018 (recent avg: -2.786594)
08:46:05 [INFO]   WM Uncertainty: 4.594106
08:46:05 [INFO]   Learning Rate: 1.00e-06
08:46:05 [INFO]   Total Steps: 6550
08:46:05 [INFO]   Log Std: mean=-1.9997, std=0.0002
08:46:05 [INFO] ================================================================================
08:46:05 [INFO] Episode 130: reward=93.85, p_loss=0.1398, v_loss=19.1749, entropy=-18.5844, mem_div=-2.7170, wm_unc=4.5941, lr=1.00e-06
Training Episodes:   7%|         | 131/2000 [14:45:41<199:31:37, 384.32s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
08:46:08 [INFO] [Step 0] First memory divergence measurement: 57.0157, setting reward=0
08:46:18 [INFO] Memory Gate WM: 0.001778
08:47:57 [INFO] [Episode 131] Completed - Detailed Statistics:
08:47:57 [INFO]   Total steps: 50
08:47:57 [INFO]   Episode reward: 86.6812
08:47:57 [INFO]   Avg memory div reward: -2.801215
08:47:57 [INFO]   Avg memory div abs: 57.090423
08:47:57 [INFO]   Avg WM uncertainty: 4.534839
08:47:57 [INFO]   Min/Max reward: 1.3569 / 4.5353
08:49:02 [INFO] [Episode 131] PPO Epoch 0 - Training Losses:
08:49:02 [INFO]   Policy loss: 0.208844
08:49:02 [INFO]   Value loss: 18.150656
08:49:02 [INFO]   Entropy: -18.583217
08:49:02 [INFO]   Total loss: 9.470005
08:51:41 [INFO] Memory Gate WM: 0.001767
08:52:28 [INFO] ================================================================================
08:52:28 [INFO] Episode 131 - Aggregated Metrics Summary:
08:52:28 [INFO]   Reward: 93.6575 (recent avg: 90.0255, std: 4.8171)
08:52:28 [INFO]   Policy Loss: 0.141079 (recent avg: 0.130085)
08:52:28 [INFO]   Value Loss: 19.091748
08:52:28 [INFO]   Entropy: -18.584304
08:52:28 [INFO]   Memory Div Reward: -2.719692 (recent avg: -2.788354)
08:52:28 [INFO]   WM Uncertainty: 4.592841
08:52:28 [INFO]   Learning Rate: 1.00e-06
08:52:28 [INFO]   Total Steps: 6600
08:52:28 [INFO]   Log Std: mean=-1.9997, std=0.0002
08:52:28 [INFO] ================================================================================
08:52:28 [INFO] Episode 131: reward=93.66, p_loss=0.1411, v_loss=19.0917, entropy=-18.5843, mem_div=-2.7197, wm_unc=4.5928, lr=1.00e-06
Training Episodes:   7%|         | 132/2000 [14:52:04<199:12:32, 383.91s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
08:52:33 [INFO] [Step 0] First memory divergence measurement: 56.9120, setting reward=0
08:53:00 [INFO] Memory Gate WM: 0.001758
08:55:00 [INFO] [Episode 132] Completed - Detailed Statistics:
08:55:00 [INFO]   Total steps: 50
08:55:00 [INFO]   Episode reward: 89.3209
08:55:00 [INFO]   Avg memory div reward: -2.785442
08:55:00 [INFO]   Avg memory div abs: 56.840988
08:55:00 [INFO]   Avg WM uncertainty: 4.571860
08:55:00 [INFO]   Min/Max reward: 1.4567 / 4.5243
08:56:08 [INFO] [Episode 132] PPO Epoch 0 - Training Losses:
08:56:08 [INFO]   Policy loss: 0.218396
08:56:08 [INFO]   Value loss: 18.825934
08:56:08 [INFO]   Entropy: -18.583265
08:56:08 [INFO]   Total loss: 9.817196
08:57:39 [INFO] Memory Gate Act: -0.001005
08:59:34 [INFO] ================================================================================
08:59:34 [INFO] Episode 132 - Aggregated Metrics Summary:
08:59:34 [INFO]   Reward: 93.5373 (recent avg: 89.6229, std: 4.6893)
08:59:34 [INFO]   Policy Loss: 0.139548 (recent avg: 0.159204)
08:59:34 [INFO]   Value Loss: 19.096588
08:59:34 [INFO]   Entropy: -18.584217
08:59:34 [INFO]   Memory Div Reward: -2.722229 (recent avg: -2.789853)
08:59:34 [INFO]   WM Uncertainty: 4.592975
08:59:34 [INFO]   Learning Rate: 1.00e-06
08:59:34 [INFO]   Total Steps: 6650
08:59:34 [INFO]   Log Std: mean=-1.9997, std=0.0002
08:59:34 [INFO] ================================================================================
08:59:34 [INFO] Episode 132: reward=93.54, p_loss=0.1395, v_loss=19.0966, entropy=-18.5842, mem_div=-2.7222, wm_unc=4.5930, lr=1.00e-06
Training Episodes:   7%|         | 133/2000 [14:59:10<205:40:36, 396.59s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
08:59:38 [INFO] [Step 0] First memory divergence measurement: 57.0863, setting reward=0
09:00:26 [INFO] Memory Gate WM: 0.001777
09:01:31 [INFO] [Episode 133] Completed - Detailed Statistics:
09:01:31 [INFO]   Total steps: 50
09:01:31 [INFO]   Episode reward: 88.5979
09:01:31 [INFO]   Avg memory div reward: -2.794994
09:01:31 [INFO]   Avg memory div abs: 57.035891
09:01:31 [INFO]   Avg WM uncertainty: 4.566952
09:01:31 [INFO]   Min/Max reward: 1.4588 / 4.6076
09:02:38 [INFO] [Episode 133] PPO Epoch 0 - Training Losses:
09:02:38 [INFO]   Policy loss: 0.082399
09:02:38 [INFO]   Value loss: 18.745132
09:02:38 [INFO]   Entropy: -18.583296
09:02:38 [INFO]   Total loss: 9.640798
09:04:04 [INFO] Memory Gate WM: 0.001783
09:04:51 [INFO] Memory Gate Act: -0.001011
09:04:55 [INFO] Memory Gate WM: 0.001788
09:06:03 [INFO] ================================================================================
09:06:03 [INFO] Episode 133 - Aggregated Metrics Summary:
09:06:03 [INFO]   Reward: 93.5017 (recent avg: 90.5172, std: 3.3703)
09:06:03 [INFO]   Policy Loss: 0.135618 (recent avg: 0.143337)
09:06:03 [INFO]   Value Loss: 19.075675
09:06:03 [INFO]   Entropy: -18.584135
09:06:03 [INFO]   Memory Div Reward: -2.724779 (recent avg: -2.790442)
09:06:03 [INFO]   WM Uncertainty: 4.594812
09:06:03 [INFO]   Learning Rate: 1.00e-06
09:06:03 [INFO]   Total Steps: 6700
09:06:03 [INFO]   Log Std: mean=-1.9997, std=0.0002
09:06:03 [INFO] ================================================================================
09:06:03 [INFO] Episode 133: reward=93.50, p_loss=0.1356, v_loss=19.0757, entropy=-18.5841, mem_div=-2.7248, wm_unc=4.5948, lr=1.00e-06
Training Episodes:   7%|         | 134/2000 [15:05:38<204:17:38, 394.14s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
09:06:06 [INFO] [Step 0] First memory divergence measurement: 57.2460, setting reward=0
09:07:59 [INFO] [Episode 134] Completed - Detailed Statistics:
09:07:59 [INFO]   Total steps: 50
09:07:59 [INFO]   Episode reward: 86.8313
09:07:59 [INFO]   Avg memory div reward: -2.802960
09:07:59 [INFO]   Avg memory div abs: 57.222297
09:07:59 [INFO]   Avg WM uncertainty: 4.539587
09:07:59 [INFO]   Min/Max reward: 1.4613 / 4.5641
09:09:05 [INFO] [Episode 134] PPO Epoch 0 - Training Losses:
09:09:05 [INFO]   Policy loss: 0.116622
09:09:05 [INFO]   Value loss: 18.138239
09:09:05 [INFO]   Entropy: -18.583172
09:09:05 [INFO]   Total loss: 9.371573
09:09:42 [INFO] Memory Gate Act: -0.001004
09:09:49 [INFO] Memory Gate WM: 0.001783
09:09:55 [INFO] Memory Gate WM: 0.001782
09:12:26 [INFO] ================================================================================
09:12:26 [INFO] Episode 134 - Aggregated Metrics Summary:
09:12:26 [INFO]   Reward: 93.4282 (recent avg: 89.9926, std: 3.4927)
09:12:26 [INFO]   Policy Loss: 0.126618 (recent avg: 0.103214)
09:12:26 [INFO]   Value Loss: 19.068992
09:12:26 [INFO]   Entropy: -18.584053
09:12:26 [INFO]   Memory Div Reward: -2.727250 (recent avg: -2.793962)
09:12:26 [INFO]   WM Uncertainty: 4.595813
09:12:26 [INFO]   Learning Rate: 1.00e-06
09:12:26 [INFO]   Total Steps: 6750
09:12:26 [INFO]   Log Std: mean=-1.9997, std=0.0002
09:12:26 [INFO] ================================================================================
09:12:26 [INFO] Episode 134: reward=93.43, p_loss=0.1266, v_loss=19.0690, entropy=-18.5841, mem_div=-2.7272, wm_unc=4.5958, lr=1.00e-06
Training Episodes:   7%|         | 135/2000 [15:12:02<202:29:40, 390.87s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
09:12:30 [INFO] [Step 0] First memory divergence measurement: 56.9586, setting reward=0
09:13:48 [INFO] Memory Gate Act: -0.001004
09:14:06 [INFO] Memory Gate Act: -0.001004
09:14:22 [INFO] [Episode 135] Completed - Detailed Statistics:
09:14:22 [INFO]   Total steps: 50
09:14:22 [INFO]   Episode reward: 96.9958
09:14:22 [INFO]   Avg memory div reward: -2.790292
09:14:22 [INFO]   Avg memory div abs: 56.921708
09:14:22 [INFO]   Avg WM uncertainty: 4.730208
09:14:22 [INFO]   Min/Max reward: 1.5211 / 4.8273
09:15:21 [INFO] Memory Gate WM: 0.001788
09:15:29 [INFO] [Episode 135] PPO Epoch 0 - Training Losses:
09:15:29 [INFO]   Policy loss: 0.240322
09:15:29 [INFO]   Value loss: 20.571958
09:15:29 [INFO]   Entropy: -18.583203
09:15:29 [INFO]   Total loss: 10.712133
09:15:35 [INFO] Memory Gate WM: 0.001789
09:15:56 [INFO] Memory Gate Act: -0.001002
09:16:36 [INFO] Memory Gate WM: 0.001791
09:18:47 [INFO] ================================================================================
09:18:47 [INFO] Episode 135 - Aggregated Metrics Summary:
09:18:47 [INFO]   Reward: 93.3123 (recent avg: 90.8644, std: 4.0062)
09:18:47 [INFO]   Policy Loss: 0.131404 (recent avg: 0.154664)
09:18:47 [INFO]   Value Loss: 19.184928
09:18:47 [INFO]   Entropy: -18.583979
09:18:47 [INFO]   Memory Div Reward: -2.729670 (recent avg: -2.794440)
09:18:47 [INFO]   WM Uncertainty: 4.595915
09:18:47 [INFO]   Learning Rate: 1.00e-06
09:18:47 [INFO]   Total Steps: 6800
09:18:47 [INFO]   Log Std: mean=-1.9997, std=0.0002
09:18:47 [INFO] ================================================================================
09:18:47 [INFO] Episode 135: reward=93.31, p_loss=0.1314, v_loss=19.1849, entropy=-18.5840, mem_div=-2.7297, wm_unc=4.5959, lr=1.00e-06
Training Episodes:   7%|         | 136/2000 [15:18:23<200:53:11, 387.98s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
09:18:50 [INFO] [Step 0] First memory divergence measurement: 56.1067, setting reward=0
09:19:44 [INFO] Memory Gate Act: -0.001000
09:20:44 [INFO] [Episode 136] Completed - Detailed Statistics:
09:20:44 [INFO]   Total steps: 50
09:20:44 [INFO]   Episode reward: 96.9349
09:20:44 [INFO]   Avg memory div reward: -2.801449
09:20:44 [INFO]   Avg memory div abs: 56.852066
09:20:44 [INFO]   Avg WM uncertainty: 4.740147
09:20:44 [INFO]   Min/Max reward: 1.4610 / 4.6311
09:21:01 [INFO] Memory Gate Act: -0.001001
09:21:52 [INFO] [Episode 136] PPO Epoch 0 - Training Losses:
09:21:52 [INFO]   Policy loss: 0.060733
09:21:52 [INFO]   Value loss: 20.432562
09:21:52 [INFO]   Entropy: -18.583099
09:21:52 [INFO]   Total loss: 10.462845
09:24:48 [INFO] Memory Gate WM: 0.001787
09:25:05 [INFO] Memory Gate WM: 0.001787
09:25:19 [INFO] ================================================================================
09:25:19 [INFO] Episode 136 - Aggregated Metrics Summary:
09:25:19 [INFO]   Reward: 93.2931 (recent avg: 91.8879, std: 4.1173)
09:25:19 [INFO]   Policy Loss: 0.130866 (recent avg: 0.156736)
09:25:19 [INFO]   Value Loss: 19.191786
09:25:19 [INFO]   Entropy: -18.583905
09:25:19 [INFO]   Memory Div Reward: -2.732144 (recent avg: -2.794966)
09:25:19 [INFO]   WM Uncertainty: 4.598005
09:25:19 [INFO]   Learning Rate: 1.00e-06
09:25:19 [INFO]   Total Steps: 6850
09:25:19 [INFO]   Log Std: mean=-1.9997, std=0.0002
09:25:19 [INFO] ================================================================================
09:25:19 [INFO] Episode 136: reward=93.29, p_loss=0.1309, v_loss=19.1918, entropy=-18.5839, mem_div=-2.7321, wm_unc=4.5980, lr=1.00e-06
Training Episodes:   7%|         | 137/2000 [15:24:54<201:19:30, 389.03s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
09:25:23 [INFO] [Step 0] First memory divergence measurement: 57.2512, setting reward=0
09:25:24 [INFO] Memory Gate Act: -0.001013
09:26:02 [INFO] Memory Gate Act: -0.001013
09:27:45 [INFO] Memory Gate Act: -0.001013
09:27:53 [INFO] [Episode 137] Completed - Detailed Statistics:
09:27:53 [INFO]   Total steps: 50
09:27:53 [INFO]   Episode reward: 92.5874
09:27:53 [INFO]   Avg memory div reward: -2.790751
09:27:53 [INFO]   Avg memory div abs: 57.125001
09:27:53 [INFO]   Avg WM uncertainty: 4.642500
09:27:53 [INFO]   Min/Max reward: 1.5525 / 4.7716
09:29:03 [INFO] [Episode 137] PPO Epoch 0 - Training Losses:
09:29:03 [INFO]   Policy loss: 0.098325
09:29:03 [INFO]   Value loss: 19.449713
09:29:03 [INFO]   Entropy: -18.583046
09:29:03 [INFO]   Total loss: 10.009012
09:29:08 [INFO] Memory Gate WM: 0.001789
09:29:43 [INFO] Memory Gate Act: -0.001013
09:32:33 [INFO] ================================================================================
09:32:33 [INFO] Episode 137 - Aggregated Metrics Summary:
09:32:33 [INFO]   Reward: 93.2529 (recent avg: 91.8834, std: 4.1165)
09:32:33 [INFO]   Policy Loss: 0.125956 (recent avg: 0.119582)
09:32:33 [INFO]   Value Loss: 19.195679
09:32:33 [INFO]   Entropy: -18.583831
09:32:33 [INFO]   Memory Div Reward: -2.734352 (recent avg: -2.795244)
09:32:33 [INFO]   WM Uncertainty: 4.599410
09:32:33 [INFO]   Learning Rate: 1.00e-06
09:32:33 [INFO]   Total Steps: 6900
09:32:33 [INFO]   Log Std: mean=-1.9997, std=0.0002
09:32:33 [INFO] ================================================================================
09:32:33 [INFO] Episode 137: reward=93.25, p_loss=0.1260, v_loss=19.1957, entropy=-18.5838, mem_div=-2.7344, wm_unc=4.5994, lr=1.00e-06
Training Episodes:   7%|         | 138/2000 [15:32:09<208:15:11, 402.64s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
09:32:39 [INFO] [Step 0] First memory divergence measurement: 56.8873, setting reward=0
09:33:01 [INFO] Memory Gate Act: -0.001011
09:35:13 [INFO] [Episode 138] Completed - Detailed Statistics:
09:35:13 [INFO]   Total steps: 50
09:35:13 [INFO]   Episode reward: 95.0594
09:35:13 [INFO]   Avg memory div reward: -2.783428
09:35:13 [INFO]   Avg memory div abs: 56.836935
09:35:13 [INFO]   Avg WM uncertainty: 4.684617
09:35:13 [INFO]   Min/Max reward: 1.2997 / 4.6822
09:35:53 [INFO] Memory Gate WM: 0.001786
09:36:20 [INFO] [Episode 138] PPO Epoch 0 - Training Losses:
09:36:20 [INFO]   Policy loss: 0.134985
09:36:20 [INFO]   Value loss: 20.002745
09:36:20 [INFO]   Entropy: -18.582937
09:36:20 [INFO]   Total loss: 10.322187
09:37:26 [INFO] Memory Gate Act: -0.001020
09:37:42 [INFO] Memory Gate Act: -0.001020
09:39:48 [INFO] ================================================================================
09:39:48 [INFO] Episode 138 - Aggregated Metrics Summary:
09:39:48 [INFO]   Reward: 93.0870 (recent avg: 91.6841, std: 3.9042)
09:39:48 [INFO]   Policy Loss: 0.126374 (recent avg: 0.116013)
09:39:48 [INFO]   Value Loss: 19.295028
09:39:48 [INFO]   Entropy: -18.583756
09:39:48 [INFO]   Memory Div Reward: -2.736597 (recent avg: -2.793713)
09:39:48 [INFO]   WM Uncertainty: 4.598337
09:39:48 [INFO]   Learning Rate: 1.00e-06
09:39:48 [INFO]   Total Steps: 6950
09:39:48 [INFO]   Log Std: mean=-1.9997, std=0.0002
09:39:48 [INFO] ================================================================================
09:39:48 [INFO] Episode 138: reward=93.09, p_loss=0.1264, v_loss=19.2950, entropy=-18.5838, mem_div=-2.7366, wm_unc=4.5983, lr=1.00e-06
Training Episodes:   7%|         | 139/2000 [15:39:23<213:05:38, 412.22s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
09:39:55 [INFO] [Step 0] First memory divergence measurement: 57.1994, setting reward=0
09:42:30 [INFO] [Episode 139] Completed - Detailed Statistics:
09:42:30 [INFO]   Total steps: 50
09:42:30 [INFO]   Episode reward: 102.2411
09:42:30 [INFO]   Avg memory div reward: -2.768563
09:42:30 [INFO]   Avg memory div abs: 56.820153
09:42:30 [INFO]   Avg WM uncertainty: 4.813384
09:42:30 [INFO]   Min/Max reward: 1.3026 / 5.0807
09:43:39 [INFO] [Episode 139] PPO Epoch 0 - Training Losses:
09:43:39 [INFO]   Policy loss: 0.147577
09:43:39 [INFO]   Value loss: 21.624500
09:43:39 [INFO]   Entropy: -18.582980
09:43:39 [INFO]   Total loss: 11.145657
09:43:46 [INFO] Memory Gate Act: -0.001031
09:45:30 [INFO] Memory Gate Act: -0.001039
09:47:06 [INFO] ================================================================================
09:47:06 [INFO] Episode 139 - Aggregated Metrics Summary:
09:47:06 [INFO]   Reward: 93.0966 (recent avg: 93.0378, std: 4.8649)
09:47:06 [INFO]   Policy Loss: 0.128399 (recent avg: 0.124213)
09:47:06 [INFO]   Value Loss: 19.416544
09:47:06 [INFO]   Entropy: -18.583682
09:47:06 [INFO]   Memory Div Reward: -2.738593 (recent avg: -2.791233)
09:47:06 [INFO]   WM Uncertainty: 4.600524
09:47:06 [INFO]   Learning Rate: 1.00e-06
09:47:06 [INFO]   Total Steps: 7000
09:47:06 [INFO]   Log Std: mean=-1.9997, std=0.0002
09:47:06 [INFO] ================================================================================
09:47:06 [INFO] Episode 139: reward=93.10, p_loss=0.1284, v_loss=19.4165, entropy=-18.5837, mem_div=-2.7386, wm_unc=4.6005, lr=1.00e-06
09:47:08 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000140.mp4
Training Episodes:   7%|         | 140/2000 [15:46:43<217:16:44, 420.54s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
09:47:13 [INFO] [Step 0] First memory divergence measurement: 56.7103, setting reward=0
09:49:14 [INFO] Memory Gate Act: -0.001043
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
moviepy is installed, but can't import moviepy.editor. Some packages could be missing [imageio, requests]
09:49:48 [INFO] [Episode 140] Completed - Detailed Statistics:
09:49:48 [INFO]   Total steps: 50
09:49:48 [INFO]   Episode reward: 89.1029
09:49:48 [INFO]   Avg memory div reward: -2.749017
09:49:48 [INFO]   Avg memory div abs: 56.469823
09:49:48 [INFO]   Avg WM uncertainty: 4.531074
09:49:48 [INFO]   Min/Max reward: 1.1650 / 4.5104
09:49:52 [INFO] Memory Gate Act: -0.001043
09:50:56 [INFO] [Episode 140] PPO Epoch 0 - Training Losses:
09:50:56 [INFO]   Policy loss: 0.131803
09:50:56 [INFO]   Value loss: 18.710986
09:50:56 [INFO]   Entropy: -18.582813
09:50:56 [INFO]   Total loss: 9.673125
09:54:18 [INFO] ================================================================================
09:54:18 [INFO] Episode 140 - Aggregated Metrics Summary:
09:54:18 [INFO]   Reward: 92.9478 (recent avg: 92.4353, std: 4.9412)
09:54:18 [INFO]   Policy Loss: 0.126573 (recent avg: 0.115197)
09:54:18 [INFO]   Value Loss: 19.390883
09:54:18 [INFO]   Entropy: -18.583603
09:54:18 [INFO]   Memory Div Reward: -2.740372 (recent avg: -2.786811)
09:54:18 [INFO]   WM Uncertainty: 4.599328
09:54:18 [INFO]   Learning Rate: 1.00e-06
09:54:18 [INFO]   Total Steps: 7050
09:54:18 [INFO]   Log Std: mean=-1.9996, std=0.0002
09:54:18 [INFO] ================================================================================
09:54:18 [INFO] Episode 140: reward=92.95, p_loss=0.1266, v_loss=19.3909, entropy=-18.5836, mem_div=-2.7404, wm_unc=4.5993, lr=1.00e-06
Training Episodes:   7%|         | 141/2000 [15:53:54<218:42:45, 423.54s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
09:54:25 [INFO] [Step 0] First memory divergence measurement: 56.9487, setting reward=0
09:56:13 [INFO] Memory Gate WM: 0.001817
09:57:01 [INFO] [Episode 141] Completed - Detailed Statistics:
09:57:01 [INFO]   Total steps: 50
09:57:01 [INFO]   Episode reward: 88.0431
09:57:01 [INFO]   Avg memory div reward: -2.805926
09:57:01 [INFO]   Avg memory div abs: 57.160204
09:57:01 [INFO]   Avg WM uncertainty: 4.566787
09:57:01 [INFO]   Min/Max reward: 1.3212 / 4.8749
09:58:00 [INFO] Memory Gate Act: -0.001057
09:58:10 [INFO] [Episode 141] PPO Epoch 0 - Training Losses:
09:58:10 [INFO]   Policy loss: 0.108696
09:58:10 [INFO]   Value loss: 18.307594
09:58:10 [INFO]   Entropy: -18.582686
09:58:10 [INFO]   Total loss: 9.448320
09:58:25 [INFO] Memory Gate Act: -0.001058
10:01:36 [INFO] ================================================================================
10:01:36 [INFO] Episode 141 - Aggregated Metrics Summary:
10:01:36 [INFO]   Reward: 92.8683 (recent avg: 92.5715, std: 4.7974)
10:01:36 [INFO]   Policy Loss: 0.126510 (recent avg: 0.113543)
10:01:36 [INFO]   Value Loss: 19.348199
10:01:36 [INFO]   Entropy: -18.583523
10:01:36 [INFO]   Memory Div Reward: -2.742862 (recent avg: -2.787282)
10:01:36 [INFO]   WM Uncertainty: 4.600227
10:01:36 [INFO]   Learning Rate: 1.00e-06
10:01:36 [INFO]   Total Steps: 7100
10:01:36 [INFO]   Log Std: mean=-1.9996, std=0.0002
10:01:36 [INFO] ================================================================================
10:01:36 [INFO] Episode 141: reward=92.87, p_loss=0.1265, v_loss=19.3482, entropy=-18.5835, mem_div=-2.7429, wm_unc=4.6002, lr=1.00e-06
Training Episodes:   7%|         | 142/2000 [16:01:12<220:49:08, 427.85s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
[PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
10:01:41 [INFO] [Step 0] First memory divergence measurement: 57.2463, setting reward=0
10:02:10 [INFO] Memory Gate Act: -0.001047
10:02:51 [INFO] Memory Gate Act: -0.001047
10:04:16 [INFO] [Episode 142] Completed - Detailed Statistics:
10:04:16 [INFO]   Total steps: 50
10:04:16 [INFO]   Episode reward: 87.0366
10:04:16 [INFO]   Avg memory div reward: -2.795235
10:04:16 [INFO]   Avg memory div abs: 57.080354
10:04:16 [INFO]   Avg WM uncertainty: 4.535967
10:04:16 [INFO]   Min/Max reward: 1.2562 / 4.5776
10:05:24 [INFO] [Episode 142] PPO Epoch 0 - Training Losses:
10:05:24 [INFO]   Policy loss: 0.050122
10:05:24 [INFO]   Value loss: 18.034394
10:05:24 [INFO]   Entropy: -18.582679
10:05:24 [INFO]   Total loss: 9.253146
10:08:34 [INFO] Memory Gate Act: -0.001047
10:08:47 [INFO] Memory Gate WM: 0.001841
10:08:52 [INFO] ================================================================================
10:08:52 [INFO] Episode 142 - Aggregated Metrics Summary:
10:08:52 [INFO]   Reward: 92.7309 (recent avg: 92.3430, std: 4.9969)
10:08:52 [INFO]   Policy Loss: 0.122103 (recent avg: 0.091099)
10:08:52 [INFO]   Value Loss: 19.325836
10:08:52 [INFO]   Entropy: -18.583445
10:08:52 [INFO]   Memory Div Reward: -2.745058 (recent avg: -2.788262)
10:08:52 [INFO]   WM Uncertainty: 4.599675
10:08:52 [INFO]   Learning Rate: 1.00e-06
10:08:52 [INFO]   Total Steps: 7150
10:08:52 [INFO]   Log Std: mean=-1.9996, std=0.0002
10:08:52 [INFO] ================================================================================
10:08:52 [INFO] Episode 142: reward=92.73, p_loss=0.1221, v_loss=19.3258, entropy=-18.5834, mem_div=-2.7451, wm_unc=4.5997, lr=1.00e-06
Training Episodes:   7%|         | 143/2000 [16:08:27<221:53:21, 430.16s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
10:08:58 [INFO] [Step 0] First memory divergence measurement: 57.3029, setting reward=0
10:08:59 [INFO] Memory Gate Act: -0.001048
10:11:15 [INFO] Memory Gate WM: 0.001841
10:11:34 [INFO] [Episode 143] Completed - Detailed Statistics:
10:11:34 [INFO]   Total steps: 50
10:11:34 [INFO]   Episode reward: 95.2343
10:11:34 [INFO]   Avg memory div reward: -2.802652
10:11:34 [INFO]   Avg memory div abs: 57.196309
10:11:34 [INFO]   Avg WM uncertainty: 4.707339
10:11:34 [INFO]   Min/Max reward: 1.4918 / 4.6747
10:11:42 [INFO] Memory Gate Act: -0.001048
10:12:42 [INFO] [Episode 143] PPO Epoch 0 - Training Losses:
10:12:42 [INFO]   Policy loss: 0.089990
10:12:42 [INFO]   Value loss: 20.196035
10:12:42 [INFO]   Entropy: -18.582654
10:12:42 [INFO]   Total loss: 10.373833
10:13:53 [INFO] Memory Gate Act: -0.001050
10:15:12 [INFO] Memory Gate WM: 0.001838
10:16:10 [INFO] ================================================================================
10:16:10 [INFO] Episode 143 - Aggregated Metrics Summary:
10:16:10 [INFO]   Reward: 92.6903 (recent avg: 93.0067, std: 4.8951)
10:16:10 [INFO]   Policy Loss: 0.118196 (recent avg: 0.076672)
10:16:10 [INFO]   Value Loss: 19.320335
10:16:10 [INFO]   Entropy: -18.583370
10:16:10 [INFO]   Memory Div Reward: -2.747285 (recent avg: -2.789027)
10:16:10 [INFO]   WM Uncertainty: 4.601090
10:16:10 [INFO]   Learning Rate: 1.00e-06
10:16:10 [INFO]   Total Steps: 7200
10:16:10 [INFO]   Log Std: mean=-1.9996, std=0.0002
10:16:10 [INFO] ================================================================================
10:16:10 [INFO] Episode 143: reward=92.69, p_loss=0.1182, v_loss=19.3203, entropy=-18.5834, mem_div=-2.7473, wm_unc=4.6011, lr=1.00e-06
Training Episodes:   7%|         | 144/2000 [16:15:45<223:00:43, 432.57s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
10:16:16 [INFO] [Step 0] First memory divergence measurement: 57.2663, setting reward=0
10:18:39 [INFO] Memory Gate WM: 0.001832
10:18:48 [INFO] [Episode 144] Completed - Detailed Statistics:
10:18:48 [INFO]   Total steps: 50
10:18:48 [INFO]   Episode reward: 89.3823
10:18:48 [INFO]   Avg memory div reward: -2.814246
10:18:48 [INFO]   Avg memory div abs: 57.413669
10:18:48 [INFO]   Avg WM uncertainty: 4.601892
10:18:48 [INFO]   Min/Max reward: 1.4541 / 4.6066
10:19:58 [INFO] [Episode 144] PPO Epoch 0 - Training Losses:
10:19:58 [INFO]   Policy loss: 0.197098
10:19:58 [INFO]   Value loss: 18.795709
10:19:58 [INFO]   Entropy: -18.582602
10:19:58 [INFO]   Total loss: 9.780779
10:20:07 [INFO] Memory Gate Act: -0.001035
10:20:12 [INFO] Memory Gate Act: -0.001035
10:20:23 [INFO] Memory Gate Act: -0.001034
10:23:25 [INFO] ================================================================================
10:23:25 [INFO] Episode 144 - Aggregated Metrics Summary:
10:23:25 [INFO]   Reward: 92.5954 (recent avg: 93.2618, std: 4.6257)
10:23:25 [INFO]   Policy Loss: 0.119941 (recent avg: 0.101707)
10:23:25 [INFO]   Value Loss: 19.301266
10:23:25 [INFO]   Entropy: -18.583299
10:23:25 [INFO]   Memory Div Reward: -2.749621 (recent avg: -2.790156)
10:23:25 [INFO]   WM Uncertainty: 4.601528
10:23:25 [INFO]   Learning Rate: 1.00e-06
10:23:25 [INFO]   Total Steps: 7250
10:23:25 [INFO]   Log Std: mean=-1.9996, std=0.0002
10:23:25 [INFO] ================================================================================
10:23:25 [INFO] Episode 144: reward=92.60, p_loss=0.1199, v_loss=19.3013, entropy=-18.5833, mem_div=-2.7496, wm_unc=4.6015, lr=1.00e-06
Training Episodes:   7%|         | 145/2000 [16:23:01<223:21:57, 433.49s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
10:23:32 [INFO] [Step 0] First memory divergence measurement: 57.1851, setting reward=0
10:26:07 [INFO] [Episode 145] Completed - Detailed Statistics:
10:26:07 [INFO]   Total steps: 50
10:26:07 [INFO]   Episode reward: 91.2078
10:26:07 [INFO]   Avg memory div reward: -2.797696
10:26:07 [INFO]   Avg memory div abs: 57.153091
10:26:07 [INFO]   Avg WM uncertainty: 4.621852
10:26:07 [INFO]   Min/Max reward: 1.4368 / 4.5556
10:27:24 [INFO] [Episode 145] PPO Epoch 0 - Training Losses:
10:27:24 [INFO]   Policy loss: 0.020614
10:27:24 [INFO]   Value loss: 19.035356
10:27:24 [INFO]   Entropy: -18.582546
10:27:24 [INFO]   Total loss: 9.724118
10:31:16 [INFO] ================================================================================
10:31:16 [INFO] Episode 145 - Aggregated Metrics Summary:
10:31:16 [INFO]   Reward: 92.5272 (recent avg: 92.6830, std: 4.4822)
10:31:16 [INFO]   Policy Loss: 0.115810 (recent avg: 0.082566)
10:31:16 [INFO]   Value Loss: 19.210278
10:31:16 [INFO]   Entropy: -18.583233
10:31:16 [INFO]   Memory Div Reward: -2.751634 (recent avg: -2.790896)
10:31:16 [INFO]   WM Uncertainty: 4.602178
10:31:16 [INFO]   Learning Rate: 1.00e-06
10:31:16 [INFO]   Total Steps: 7300
10:31:16 [INFO]   Log Std: mean=-1.9996, std=0.0002
10:31:16 [INFO] ================================================================================
10:31:16 [INFO] Episode 145: reward=92.53, p_loss=0.1158, v_loss=19.2103, entropy=-18.5832, mem_div=-2.7516, wm_unc=4.6022, lr=1.00e-06
Training Episodes:   7%|         | 146/2000 [16:30:51<228:56:03, 444.53s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
10:31:23 [INFO] [Step 0] First memory divergence measurement: 57.2863, setting reward=0
10:32:13 [INFO] Memory Gate Act: -0.001051
10:32:44 [INFO] Memory Gate Act: -0.001051
10:34:55 [INFO] [Episode 146] Completed - Detailed Statistics:
10:34:55 [INFO]   Total steps: 50
10:34:55 [INFO]   Episode reward: 76.9350
10:34:55 [INFO]   Avg memory div reward: -2.807834
10:34:55 [INFO]   Avg memory div abs: 57.311273
10:34:55 [INFO]   Avg WM uncertainty: 4.346533
10:34:55 [INFO]   Min/Max reward: 1.1232 / 4.4334
10:36:06 [INFO] [Episode 146] PPO Epoch 0 - Training Losses:
10:36:06 [INFO]   Policy loss: 0.084064
10:36:06 [INFO]   Value loss: 15.810033
10:36:06 [INFO]   Entropy: -18.582526
10:36:06 [INFO]   Total loss: 8.174906
10:39:45 [INFO] ================================================================================
10:39:45 [INFO] Episode 146 - Aggregated Metrics Summary:
10:39:45 [INFO]   Reward: 92.3251 (recent avg: 90.6830, std: 6.2516)
10:39:45 [INFO]   Policy Loss: 0.113597 (recent avg: 0.059109)
10:39:45 [INFO]   Value Loss: 19.041749
10:39:45 [INFO]   Entropy: -18.583172
10:39:45 [INFO]   Memory Div Reward: -2.753840 (recent avg: -2.791535)
10:39:45 [INFO]   WM Uncertainty: 4.600342
10:39:45 [INFO]   Learning Rate: 1.00e-06
10:39:45 [INFO]   Total Steps: 7350
10:39:45 [INFO]   Log Std: mean=-1.9996, std=0.0002
10:39:45 [INFO] ================================================================================
10:39:45 [INFO] Episode 146: reward=92.33, p_loss=0.1136, v_loss=19.0417, entropy=-18.5832, mem_div=-2.7538, wm_unc=4.6003, lr=1.00e-06
Training Episodes:   7%|         | 147/2000 [16:39:21<238:51:27, 464.05s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
10:39:51 [INFO] [Step 0] First memory divergence measurement: 57.1994, setting reward=0
10:39:52 [INFO] Memory Gate Act: -0.001037
10:41:46 [INFO] Memory Gate WM: 0.001804
10:42:26 [INFO] [Episode 147] Completed - Detailed Statistics:
10:42:26 [INFO]   Total steps: 50
10:42:26 [INFO]   Episode reward: 84.1957
10:42:26 [INFO]   Avg memory div reward: -2.793045
10:42:26 [INFO]   Avg memory div abs: 57.131338
10:42:26 [INFO]   Avg WM uncertainty: 4.476959
10:42:26 [INFO]   Min/Max reward: 1.1869 / 4.8925
10:43:36 [INFO] [Episode 147] PPO Epoch 0 - Training Losses:
10:43:36 [INFO]   Policy loss: 0.172940
10:43:36 [INFO]   Value loss: 17.487638
10:43:36 [INFO]   Entropy: -18.582677
10:43:36 [INFO]   Total loss: 9.102586
10:45:03 [INFO] Memory Gate Act: -0.001039
10:45:05 [INFO] Memory Gate WM: 0.001812
10:47:26 [INFO] ================================================================================
10:47:26 [INFO] Episode 147 - Aggregated Metrics Summary:
10:47:26 [INFO]   Reward: 92.1565 (recent avg: 89.8438, std: 6.4980)
10:47:26 [INFO]   Policy Loss: 0.111939 (recent avg: 0.074895)
10:47:26 [INFO]   Value Loss: 18.957637
10:47:26 [INFO]   Entropy: -18.583118
10:47:26 [INFO]   Memory Div Reward: -2.755827 (recent avg: -2.791764)
10:47:26 [INFO]   WM Uncertainty: 4.598957
10:47:26 [INFO]   Learning Rate: 1.00e-06
10:47:26 [INFO]   Total Steps: 7400
10:47:26 [INFO]   Log Std: mean=-1.9996, std=0.0002
10:47:26 [INFO] ================================================================================
10:47:26 [INFO] Episode 147: reward=92.16, p_loss=0.1119, v_loss=18.9576, entropy=-18.5831, mem_div=-2.7558, wm_unc=4.5990, lr=1.00e-06
Training Episodes:   7%|         | 148/2000 [16:47:02<238:11:44, 463.02s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
10:47:36 [INFO] [Step 0] First memory divergence measurement: 57.0819, setting reward=0
10:51:25 [INFO] Memory Gate WM: 0.001817
10:51:58 [INFO] [Episode 148] Completed - Detailed Statistics:
10:51:58 [INFO]   Total steps: 50
10:51:58 [INFO]   Episode reward: 93.8890
10:51:58 [INFO]   Avg memory div reward: -2.811749
10:51:58 [INFO]   Avg memory div abs: 57.245649
10:51:58 [INFO]   Avg WM uncertainty: 4.689528
10:51:58 [INFO]   Min/Max reward: 1.4385 / 4.7966
10:53:15 [INFO] [Episode 148] PPO Epoch 0 - Training Losses:
10:53:15 [INFO]   Policy loss: 0.026908
10:53:15 [INFO]   Value loss: 19.595947
10:53:15 [INFO]   Entropy: -18.582702
10:53:15 [INFO]   Total loss: 10.010709
10:55:41 [INFO] Memory Gate WM: 0.001834
10:56:52 [INFO] Memory Gate WM: 0.001840
10:57:07 [INFO] ================================================================================
10:57:07 [INFO] Episode 148 - Aggregated Metrics Summary:
10:57:07 [INFO]   Reward: 92.1253 (recent avg: 89.7268, std: 6.4130)
10:57:07 [INFO]   Policy Loss: 0.107239 (recent avg: 0.075528)
10:57:08 [INFO]   Value Loss: 19.087625
10:57:08 [INFO]   Entropy: -18.583064
10:57:08 [INFO]   Memory Div Reward: -2.757760 (recent avg: -2.794596)
10:57:08 [INFO]   WM Uncertainty: 4.600266
10:57:08 [INFO]   Learning Rate: 1.00e-06
10:57:08 [INFO]   Total Steps: 7450
10:57:08 [INFO]   Log Std: mean=-1.9996, std=0.0002
10:57:08 [INFO] ================================================================================
10:57:08 [INFO] Episode 148: reward=92.13, p_loss=0.1072, v_loss=19.0876, entropy=-18.5831, mem_div=-2.7578, wm_unc=4.6003, lr=1.00e-06
Training Episodes:   7%|         | 149/2000 [16:56:43<256:20:47, 498.57s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
10:57:16 [INFO] [Step 0] First memory divergence measurement: 57.3977, setting reward=0
11:01:26 [INFO] [Episode 149] Completed - Detailed Statistics:
11:01:26 [INFO]   Total steps: 50
11:01:26 [INFO]   Episode reward: 87.8438
11:01:26 [INFO]   Avg memory div reward: -2.810740
11:01:26 [INFO]   Avg memory div abs: 57.383705
11:01:26 [INFO]   Avg WM uncertainty: 4.567615
11:01:26 [INFO]   Min/Max reward: 1.3289 / 4.6093
11:02:26 [INFO] Memory Gate WM: 0.001843
11:02:37 [INFO] [Episode 149] PPO Epoch 0 - Training Losses:
11:02:37 [INFO]   Policy loss: 0.135155
11:02:37 [INFO]   Value loss: 18.401906
11:02:37 [INFO]   Entropy: -18.582505
11:02:37 [INFO]   Total loss: 9.521933
11:04:46 [INFO] Memory Gate WM: 0.001851
11:06:08 [INFO] ================================================================================
11:06:08 [INFO] Episode 149 - Aggregated Metrics Summary:
11:06:08 [INFO]   Reward: 92.0832 (recent avg: 88.2870, std: 4.8731)
11:06:08 [INFO]   Policy Loss: 0.110334 (recent avg: 0.094486)
11:06:08 [INFO]   Value Loss: 19.040556
11:06:08 [INFO]   Entropy: -18.583011
11:06:08 [INFO]   Memory Div Reward: -2.759679 (recent avg: -2.798814)
11:06:08 [INFO]   WM Uncertainty: 4.601342
11:06:08 [INFO]   Learning Rate: 1.00e-06
11:06:08 [INFO]   Total Steps: 7500
11:06:08 [INFO]   Log Std: mean=-1.9996, std=0.0002
11:06:08 [INFO] ================================================================================
11:06:08 [INFO] Episode 149: reward=92.08, p_loss=0.1103, v_loss=19.0406, entropy=-18.5830, mem_div=-2.7597, wm_unc=4.6013, lr=1.00e-06
11:06:23 [INFO] Saved checkpoint to outputs/student_rl/checkpoint-149
11:06:25 [INFO] [Video] Saved student episode video: outputs/student_rl/videos/episode_000150.mp4
Training Episodes:   8%|         | 150/2000 [17:06:01<265:17:22, 516.24s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
11:06:32 [INFO] [Step 0] First memory divergence measurement: 56.8900, setting reward=0
11:09:10 [INFO] [Episode 150] Completed - Detailed Statistics:
11:09:10 [INFO]   Total steps: 50
11:09:10 [INFO]   Episode reward: 89.9395
11:09:10 [INFO]   Avg memory div reward: -2.810224
11:09:10 [INFO]   Avg memory div abs: 57.156445
11:09:10 [INFO]   Avg WM uncertainty: 4.609014
11:09:10 [INFO]   Min/Max reward: 1.1834 / 4.6246
11:09:47 [INFO] Memory Gate WM: 0.001849
11:10:20 [INFO] [Episode 150] PPO Epoch 0 - Training Losses:
11:10:20 [INFO]   Policy loss: 0.042979
11:10:20 [INFO]   Value loss: 18.651673
11:10:20 [INFO]   Entropy: -18.582339
11:10:20 [INFO]   Total loss: 9.554639
11:13:45 [INFO] Memory Gate Act: -0.001057
11:13:49 [INFO] ================================================================================
11:13:49 [INFO] Episode 150 - Aggregated Metrics Summary:
11:13:49 [INFO]   Reward: 92.0711 (recent avg: 88.3707, std: 4.8935)
11:13:49 [INFO]   Policy Loss: 0.106167 (recent avg: 0.078986)
11:13:49 [INFO]   Value Loss: 19.052158
11:13:49 [INFO]   Entropy: -18.582958
11:13:49 [INFO]   Memory Div Reward: -2.761650 (recent avg: -2.804935)
11:13:49 [INFO]   WM Uncertainty: 4.603073
11:13:49 [INFO]   Learning Rate: 1.00e-06
11:13:49 [INFO]   Total Steps: 7550
11:13:49 [INFO]   Log Std: mean=-1.9996, std=0.0002
11:13:49 [INFO] ================================================================================
11:13:49 [INFO] Episode 150: reward=92.07, p_loss=0.1062, v_loss=19.0522, entropy=-18.5830, mem_div=-2.7617, wm_unc=4.6031, lr=1.00e-06
Training Episodes:   8%|         | 151/2000 [17:13:25<254:04:25, 494.68s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
11:13:56 [INFO] [Step 0] First memory divergence measurement: 57.2592, setting reward=0
11:16:33 [INFO] [Episode 151] Completed - Detailed Statistics:
11:16:33 [INFO]   Total steps: 50
11:16:33 [INFO]   Episode reward: 93.3550
11:16:33 [INFO]   Avg memory div reward: -2.806671
11:16:33 [INFO]   Avg memory div abs: 57.258292
11:16:33 [INFO]   Avg WM uncertainty: 4.673772
11:16:33 [INFO]   Min/Max reward: 1.4635 / 4.6091
11:17:43 [INFO] [Episode 151] PPO Epoch 0 - Training Losses:
11:17:43 [INFO]   Policy loss: 0.104667
11:17:43 [INFO]   Value loss: 19.935125
11:17:43 [INFO]   Entropy: -18.582367
11:17:43 [INFO]   Total loss: 10.258054
11:19:07 [INFO] Memory Gate WM: 0.001852
11:20:04 [INFO] Memory Gate WM: 0.001851
11:21:14 [INFO] ================================================================================
11:21:14 [INFO] Episode 151 - Aggregated Metrics Summary:
11:21:14 [INFO]   Reward: 92.0296 (recent avg: 88.9019, std: 5.1125)
11:21:14 [INFO]   Policy Loss: 0.105646 (recent avg: 0.084777)
11:21:14 [INFO]   Value Loss: 19.117502
11:21:14 [INFO]   Entropy: -18.582908
11:21:14 [INFO]   Memory Div Reward: -2.763467 (recent avg: -2.805009)
11:21:14 [INFO]   WM Uncertainty: 4.604059
11:21:14 [INFO]   Learning Rate: 1.00e-06
11:21:14 [INFO]   Total Steps: 7600
11:21:14 [INFO]   Log Std: mean=-1.9996, std=0.0002
11:21:14 [INFO] ================================================================================
11:21:14 [INFO] Episode 151: reward=92.03, p_loss=0.1056, v_loss=19.1175, entropy=-18.5829, mem_div=-2.7635, wm_unc=4.6041, lr=1.00e-06
Training Episodes:   8%|         | 152/2000 [17:20:49<246:12:03, 479.61s/ep][PID 986356] Created Scene with PhysxCpuSystem + RenderSystem(cuda:0)
11:21:23 [INFO] [Step 0] First memory divergence measurement: 57.4581, setting reward=0
11:24:00 [INFO] [Episode 152] Completed - Detailed Statistics:
11:24:00 [INFO]   Total steps: 50
11:24:00 [INFO]   Episode reward: 90.4574
11:24:00 [INFO]   Avg memory div reward: -2.791731
11:24:00 [INFO]   Avg memory div abs: 57.069953
11:24:00 [INFO]   Avg WM uncertainty: 4.600879
11:24:00 [INFO]   Min/Max reward: 1.3264 / 4.6992
